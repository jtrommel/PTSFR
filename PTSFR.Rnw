\documentclass{tufte-book}
\usepackage{graphicx}  % werken met figuren
\usepackage{gensymb} % werken met wetenschappelijke eenheden\usepackage{geometry}
\usepackage{changepage} % http://ctan.org/pkg/changepage
\usepackage[dutch,british]{babel} % instelling van de taal (woordsplitsing, spellingscontrole)
\usepackage[parfill]{parskip} % Paragrafen gescheiden door witte lijn en geen inspringing
\usepackage[font=small,skip=3pt]{caption} % Minder ruimte tussen figuur/table en ondertitel. Ondertitel klein
\usepackage{capt-of}
\usepackage{indentfirst}
\setlength{\parindent}{0.7cm}
\usepackage{enumitem} % Laat enumerate werken met letters
\usepackage{url}
\usepackage{lipsum}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
% Prints a trailing space in a smart way.
\usepackage{xspace}
\usepackage{hyperref}
\usepackage{amsmath}

\DeclareGraphicsExtensions{.pdf,.png,.jpg}

% Alter some LaTeX defaults for better treatment of figures:
% See p.105 of "TeX Unbound" for suggested values.
% See pp. 199-200 of Lamport's "LaTeX" book for details.
%   General parameters, for ALL pages:
    \renewcommand{\topfraction}{0.9}	% max fraction of floats at top
    \renewcommand{\bottomfraction}{0.9}	% max fraction of floats at bottom
%   Parameters for TEXT pages (not float pages):
    \setcounter{topnumber}{2}
    \setcounter{bottomnumber}{2}
    \setcounter{totalnumber}{4}     % 2 may work better
    \renewcommand{\textfraction}{0.1}	% allow minimal text w. figs
%   Parameters for FLOAT pages (not text pages):
    \renewcommand{\floatpagefraction}{0.8}	% require fuller float pages
% N.B.: floatpagefraction MUST be less than topfraction !!
\setcounter{secnumdepth}{3}

\newcommand{\tthdump}[1]{#1}

\newcommand{\openepigraph}[2]{
  \begin{fullwidth}
  \sffamily\large
    \begin{doublespace}
      \noindent\allcaps{#1}\\ % epigraph
      \noindent\allcaps{#2} % author
    \end{doublespace}
  \end{fullwidth}
}


\usepackage{makeidx}
\makeindex

\title{Practical Time Series Forecasting with R}
\author{Jan Trommelmans}

\begin{document}
\SweaveOpts{concordance=TRUE,prefix.string=PTSFR}
\setkeys{Gin}{width=1.1\marginparwidth} %% Sweave

<<echo=FALSE>>=
library(tidyverse)
library(lubridate)
library(broom)
library(funModeling)
library(forecast)
library(gridExtra)
library(writexl)
library(plotly)
library(ggfortify)
@

% Setting the ggplot theme:
<<echo=FALSE>>=
JT.theme <- theme(panel.border = element_rect(fill = NA, colour = "gray10"),
                  panel.background = element_blank(),
                  panel.grid.major = element_line(colour = "gray85"),
                  panel.grid.minor = element_line(colour = "gray85"),
                  panel.grid.major.x = element_line(colour = "gray85"),
                  axis.text = element_text(size = 8 , face = "bold"),
                  axis.title = element_text(size = 9 , face = "bold"),
                  plot.title = element_text(size = 12 , face = "bold"),
                  strip.text = element_text(size = 8 , face = "bold"),
                  strip.background = element_rect(colour = "black"),
                  legend.text = element_text(size = 8),
                  legend.title = element_text(size = 9 , face = "bold"),
                  legend.background = element_rect(fill = "white"),
                  legend.key = element_rect(fill = "white"))
@

% Functions
<<echo=FALSE>>=
TRJ.FFT <- function(signal.df) {
    # This function calculates the FFT for a time series stored in a data frame with as first 
    # column the time (or order of measurement) and as second column the vector of measurements.
    # The result is a list. 
    # The first element of the list is freqspec: the N frequencies plus for each frequency the 
    # amplitude and phase.
    # The second element of the list is resultaat: a data frame with those frequencies for which 
    # the amplitude  is at least 33% of the maximum amplitude. 
    # The data frame is sorted from highes amplitude to lowest. 
    # This data frame can be seen as containing the most influential frequencies.
    signal <- signal.df
    names(signal) <- c("t","x")
    N <- nrow(signal)
    Ts <- as.numeric(signal$t[2]-signal$t[1])
    Fs <- 1/Ts
    # Calculation of the double sided en single sided spectrum
    z <- fft(signal$x)
    P2 <- Mod(z/N)
    P1 <- P2[1:((N/2)+1)]
    P1[2:(length(P1)-1)] <- 2*P1[2:(length(P1)-1)]
    freq <- seq(0, (Fs/2)-(Fs/N), Fs/N)
    freqspec <- data.frame(freq=freq,amp=P1[1:(N/2)],arg=Arg(z[1:(N/2)]))
    # Finding the most important elements in the frequency spectrum
    grens <- ifelse(freqspec$freq[freqspec$amp==max(freqspec$amp)]==0,max(freqspec$amp[2:nrow(freqspec)])/3,max(freqspec$amp)/3)
    aantal <- length(freqspec$amp[freqspec$amp>grens])
    resultaat <- data.frame(freq=rep(0,aantal), amp=rep(0,aantal), fasehoek=rep(0,aantal))
    resultaat <- data.frame(freq=freqspec$freq[freqspec$amp>grens],
                            amp=freqspec$amp[freqspec$amp>grens],
                            fasehoek_pi=freqspec$arg[freqspec$amp>grens]/pi)
    resultaat <- resultaat[order(-resultaat$amp),]
    return(list("freqspec"=freqspec,"resultaat"=resultaat))
}
@


\frontmatter
\chapter*{Practical Time Series Forecasting with R}

\mainmatter

\chapter{Approaching Forecasting}
\section{Forecasting: Where?}
Everywhere

\section{Basic notation}
\begin{itemize}
 \item t=1, 2, 3 : index of time of measurement of the value
 \item $y_{1}, y_{2}, \ldots y_{i}, \ldots$ : value of the time series for t=i
 \item $F_{t}$ : forecasted value for time index t
 \item $F_{t+k}$ : k-step ahead forecast when the forecasting time is t (don't know what that means)
 \item $e_{t}$ : forecast error at time t =$y_{t}-F_{t}$
\end{itemize}

\section{Forecasting process}

\begin{enumerate}
	\item define goal (chapter 1)
	\item get data (chapter 2)
	\item explore and visualise the time series (chapter 2)
	\item pre-process data (chapter 2)
	\item partition the time series (chapter 4)
	\item apply forecasting methods (chapter 5 - 9)
	\item evaluate, compare performance (chapter 3)
	\item implement forecasting system (chapter 10)
\end{enumerate}

\section{Goal definition}

\begin{itemize}
	\item purpose of generating forecasts? \emph{time series analysis}\index{time series!analysis} is about \emph{descriptive modeling}\index{modeling!descriptive} the time series. \emph{time series forecasting}\index{time series!forecasting} is about \emph{predicting}\index{modeling!predictive} future values. Descriptive methods can use data ''from the future" (e.g. in moving averaging), while predictive methods can only rely on present data and past data.
	\item type of forecasts that are needed? \emph{Forecast horizon}\index{forecast!horizon}: how far in the future should we forecast? A forecast horizon $k$ is the number of time steps ahead we have to forecast $(F_{t+k})$. The value of $k$ depends on the purpose of the forecast. Here the element of data availability is important: if the most recent data are two months old, a forecast for next month requires a forecast horizon $k=3$.
	\item how will the forecasts be used? Do we want a numerical result or a binary one (''yes/no"). Who is the client: technocrats with a deep understanding of forecasting, or generalists who simply want to use the forecast?
	\item costs associated with forecast errors?
	\item data available in the future? \emph{Forecast updating}\index{forecast!updating}: forecasting for one point in time, or ongoing. If it is ongoing our forecasting method should be able to take new information into account. A forecast for december 2019 made in januari 2019 can be refreshed when data for februari, march ... 2019 become available.
	\item the level of automation. The level of automation increases when many time series have to be forecast, when it is an ongoing process, when extra data become available during the forecasting period and when less forecasting expertise is available.
\end{itemize}

\chapter{Time Series Data}

\section{Data Collection}

\subsection{Data quality}

\emph{Data quality}\index{data!quality} refers to:
\begin{itemize}
	\item accuracy of measurement
	\item missing values
	\item corrupted data
	\item data entry errors
\end{itemize}

Check if external data\index{data!external} can be more predictive then solely the historic sequence available in a time series.

\subsection{Temporal frequency}

The \emph{frequency}\index{frequency!of data collection} of data collection is not necessarily the right frequency for forecasting purposes. It depends on the forecasting goal. If we want monthly forecasts it is sensible to reduce daily measurements to monthly values. Fine grained data collection introduces \emph{noise}\index{noise} that is not relevant for longer term forecasts and should be filtered out.

Even if forecasts are wanted on at time base $T$ we can still aggregate the collected data on a time base $n*T$, make the forecast model and de-aggregate the forecast to time base $T$ (by a suitable interpolation method).

\subsection{Granularity of the time series}
\emph{Granularity}\index{granularity} refers to additional information present in the time series data that can be used to make subsets. These can be based on geographical information, sociological strata, age groups etc. A higher levels of detail can lead to subsets where the time series value equals zero. This could lead to changing the forecast method from numerical to binary (''yes/no").

\subsection{Domain expertise}

\index{domain expertise}Do we have the knowledge available to decide on the following topics:
\begin{itemize}
	\item which data are we going to collect?
	\item at what frequency?
	\item can we interpret the patterns in the data?
	\item can we identify extreme values?
	\item do the users of our forecast(s) have the knowledge to interpret the results?
\end{itemize}

\section{Time Series Components}

\emph{Time series}\index{time series} are a specific sort of data. In constrast with \emph{cross-secional data}\index{cross-sectional data}, which are multiple measurements taken at the same time, \emph{time series} consist of one measurement taken at different moments in time.

Time series can have the following characteristics:

\begin{enumerate}
  \item error: random fluctuation of values
  \item level: mean value of the series
  \item trend: gradual evolution without repetition
  \item seasonality: periodic behaviour with characteristic period $T_{i}$. Multiple periodicity is possible
\end{enumerate}

The use of \emph{level}\index{level} and \emph{trend}\index{trend} as separate characteristics is different from normal usage in model building where both are incorporated into the model equation. Thus if we model the general behaviour of $y$ as a function of $t$ by a polynomial:
\begin{equation}
y(t)=b_{0} + b_{1}t + b_{2}t^{2} + \ldots b_{i}t^{i} + \ldots = p(t)
\end{equation}

and we define the level as $\bar{y}$ then the \emph{trend} in the sense as defined above is given by:
\begin{equation}
trend = p(t) - \bar{y} = \left( b_{0} - \bar{y} \right) + b_{1}t + b_{2}t^{2} + \ldots b_{i}t^{i} + \ldots 
\end{equation}

The book\sidenote{Practical Time Series Forecasting with R - Galit Shmueli and Kenneth C. Lichtendahl Jr.} refers to examples of \emph{level} and \emph{trend} by Jim Flower (\url{http://techweb.bsu.edu/jcflowers1/rlo/trends.htm}). However, in his paragraph on ''Some Common Types of Trends" he says:

\textit{Trends are often shown graphically (as line graphs) with the level of a dependent variable on the y-axis and the time period on the x-axis. There are different types of trends, including the following:
\begin{itemize}
	\item constant
	\item linear
	\item exponential
	\item damped
\end{itemize}
}

In the example graphs given by Flower, he only uses the word ''trend", and he says nothing about ''level". This gives the impression that ''trend" here is used in the traditional modeling sense i.e. ''trend"=trend+level. In what follows I will only use the characteristic ''level" where needed and consider it to be equal to the average of the ''trend" in the modeling sense of the word. 

Time series can be constructed by \emph{adding} or \emph{multiplying} or \emph{combinations of adding and multiplying} these basic characteristics. \emph{Multiplication} is used when e.g. the amplitude of an seasonal characteristic is linked to the trend. Multiplying a trend with a sine function (seasonality) will increase the amplitude of the sine function when the trend is rising, and decrease the amplitude when the trend is falling. The same can be said for multiplying the trend and the error term: bigger errors when the trend rises, smaller when it falls.

This gives a lot of possible combinations. If we limit ourselves to a maximum of two seasonal elements, and either a completely additive or a completely multiplicative type we get $2^5 = 32$ combinations:

\begin{tabular}{r | c | c}
characteristic & 0 = not present & 1 = present \\
\hline
error & 0 & 1 \\
trend & 0 & 1 \\
seasonality 1 & 0 & 1 \\
seasonality 2 & 0 & 1 \\
type & A & M
\end{tabular}

However, some of these 32 possibilities are not interesting or self evident:
\begin{itemize}
  \item all multiplicative constructed time series where one of the characteristics is 0, have the same end result i.e. zero everywhere
  \item some models without an error term. Where we have only trend the graph is given by the polynomial $p(t)$ (or another function) and  forecasting is a, tentative, extrapolation
\end{itemize}

The interesting ones are:
\begin{enumerate}
  \item additively created time series:
    \begin{itemize}
      \item without error
        \begin{itemize}
          \item trend + seasonality 1
          \item trend + seasonality 1 + seasonality 2
        \end{itemize}
      \item with error
        \begin{itemize}
          \item error
          \item error + trend
          \item error + trend + seasonality 1
          \item error + trend + seasonality 1 + seasonality 2
        \end{itemize}
      \end{itemize}
  \item multiplicatively created time series:
    \begin{itemize}
      \item without error
        \begin{itemize}
          \item trend*seasonality 1
          \item trend*seasonality 1*seasonality 2
        \end{itemize}
      \item with error
        \begin{itemize}
          \item error*trend
          \item error*trend*seasonality 1
          \item error*trend*seasonality 1*seasonality 2
        \end{itemize}
    \end{itemize}
\end{enumerate}

\section{Constructed data sets}
\label{sec:constructed}
\begin{itemize}
  \item number of elements per time series: N=1000
  \item time step unit: 1
  \item error: random from normal distributiion with $\mu=0$, $\sigma=3$. Random number generator seed: 2019
  \item trend: polynomial. We restrict ourselves to models $y=p(x)$ with second degree polynomials (or first degree when $b_{2}=0$) with co\"{e}ffici\"{e}nts $b_{0}=0$, $b_{1}=0.01$ and $b_{2}=0.00005$. 
  \item seasonality 1: sine with amplitude=0 or $3\sigma$, period=$\frac{N}{9}$, phase=0. N not exact multiple of T1
  \item seasonality 2: sine with amplitude=0 or $\sigma$, period=$\frac{N}{39}$, phase=$\frac{\pi}{3}$. N not exaxt multiple of T2
  \item type: additive (''A") or multiplicative (''M")
\end{itemize}

<<echo=FALSE>>=
# Number of elements in the data set
N <- 1000
# Random number generator seed
set.seed(2019)
# error parameters
mu <- 0
sigma <- 3
# trend model parameters
b0 <- 0
b1 <- 0.01
b2 <- 0.00005
# level
level <- b0 + (b1/2)*N + (b2/3)*N^2
# seasonality 1
amp1 <- 3*sigma
teta1 <- 0
T1 <- N/9
# seasonality 2
amp2 <- sigma
teta2 <- pi/3
T2 <- N/39
# constant for combined additive/multiplicative time series
c <- 0.05
@

\newpage
\subsection{Additive constructed time series}
<<echo=FALSE>>=
#
# creating the additive constructed set
#
construct <- data.frame(t = seq(1:N),
                        error = rnorm(N, mean = mu, sd = sigma))
construct %>% mutate(trend = (b0 + b1*t +b2*t^2)) -> construct
construct %>% mutate(season1 = amp1*sin(2*pi*t/T1 + teta1)) -> construct
construct %>% mutate(season2 = amp2*sin(2*pi*t/T2 + teta2)) -> construct
@

<<label=additive, fig=TRUE, include=FALSE, echo=FALSE>>=
p1 <- ggplot(data = construct, aes(x = t)) +
  geom_line(aes(y = error), size = 1, color="red") +
  geom_hline(yintercept = mu, linetype = 2) +
  scale_y_continuous(limits= c(-25, 75)) +
  JT.theme
p2 <- ggplot(data = construct, aes(x = t)) +
  geom_line(aes(y = error + trend), size = 1, color="red") +
  geom_hline(yintercept = level, linetype = 2) +
  scale_y_continuous(limits= c(-25, 75)) +
  JT.theme
p3 <- ggplot(data = construct, aes(x = t)) +
  geom_line(aes(y = error + trend + season1), size = 1, color="red") +
  geom_hline(yintercept = level, linetype = 2) +
  scale_y_continuous(limits= c(-25, 75)) +
  JT.theme
p4 <- ggplot(data = construct, aes(x = t)) +
  geom_line(aes(y = error + trend + season1 + season2), size = 1, color="red") +
  geom_hline(yintercept = level, linetype = 2) +
  scale_y_continuous(limits= c(-25, 75)) +
  JT.theme
grid.arrange(p1, p2, p3, p4, nrow=2)
construct %>% mutate(addcon = error + trend + season1 + season2) -> construct
write_xlsx(data.frame(t = construct$t,addcon = construct$addcon), "Data/addcon.xlsx")
@

\begin{figure}
\includegraphics[width=0.7\textwidth]{PTSFR-additive}
\caption{Additive constructed time series}
\label{fig:additive}
\setfloatalignment{b}
\end{figure}

This time series is stored in the data frame \textit{construct} in column \textit{addcon}. It is also stored as an Excel-spreadsheet (\textit{addcon.xlsx}).

\subsection{Multiplicative constructed time series}

The definition of a multiplicative time series is:
\begin{equation}
y(t)=error(t)*trend(t)*seasonality(t)
\end{equation}

<<label=trend, fig=TRUE, include=FALSE, echo=FALSE>>=
p1 <- ggplot(data = construct, aes(x = t)) +
  geom_line(aes(y = trend*season1), size=1, color="red") +
  labs(title="") +
  JT.theme
p2 <- ggplot(data = construct, aes(x = t)) +
  geom_line(aes(y = trend*(season1 + season2), size=1, color="red")) +
  labs(title="") +
  JT.theme
grid.arrange(p1, p2, nrow=1)
@

<<label=error, fig=TRUE, include=FALSE, echo=FALSE>>=
p1 <- ggplot(data = construct, aes(x = t)) +
  geom_line(aes(y = error*trend), size=1, color="red") +
  labs(title="") +
  JT.theme
p2 <- ggplot(data = construct, aes(x = t)) +
  geom_line(aes(y = error*trend*season1, size=1, color="red")) +
  labs(title="") +
  JT.theme
grid.arrange(p1, p2, nrow=1)
@

But the multiplication means that $y(t)$ will always be equal to zero whenever one of the factors is zero\sidenote[][-1cm]{\url{https://fs.blog/2016/08/multiplicative-systems/}}. Let's look at some combinations of characteristics:
\begin{itemize}
  \item trend*season: this changes the amplitude of the seasonal component but it loses the overall trend (Figure~\ref{fig:trend})
    \begin{marginfigure}[0cm]
      \includegraphics[width=1\textwidth]{PTSFR-trend}
      \caption{}
      \label{fig:trend}
      \setfloatalignment{b}
    \end{marginfigure}
  \item error*trend and error*trend*season (Figure~\ref{fig:error})
  \begin{marginfigure}[0cm]
      \includegraphics[width=1\textwidth]{PTSFR-error}
      \caption{}
      \label{fig:error}
      \setfloatalignment{b}
    \end{marginfigure}
\end{itemize}

These time series constructions with multiplication do not give the desirede result: the trend only changes the amplitude of the seasonal component and/or the error term, but the trend itself is lost.

\newpage
\subsection{Combined additive and multiplicative constructed time series}
In the end to obtain the goal that both the amplitude of the seasonal terms and the amount of error are influenced by trend we have to combine additive and multiplicative elements in
\begin{equation}
y(t) = trend*(1 + c*(error + season1 + season2)) \quad c=0.05
\end{equation}
resulting in Figure~\ref{fig:multiplicative}.

<<label=multiplicative, fig=TRUE, include=FALSE, echo=FALSE>>=
ggplot(data = construct, aes(x = t)) +
  geom_line(aes(y = trend*(1 + c*(error + season1 + season2))), size=1, color="red") +
  labs(title="") +
  JT.theme
construct %>% mutate(combicon = trend*(1 + c*(error + season1 + season2))) -> construct
write_xlsx(data.frame(t = construct$t,combicon = construct$combicon), "Data/combicon.xlsx")
@

\begin{figure}
\includegraphics[width=0.5\textwidth]{PTSFR-multiplicative}
\caption{multiplicative constructed time series}
\label{fig:multiplicative}
\setfloatalignment{b}
\end{figure}

This time series is stored in the data frame \textit{construct} in column \textit{combicon}.

\section{Visualising Time Series}

A visualisation of the time series will give us a first, visual, indication of the nature of the series. We can look for \emph{missing} or \emph{extreme values}, \emph{unequal spacing} and \emph{patterns}.

A first and obvious plot is a line chart of the series as a function of time. \textit{ggplot} does this very well.

\newthought{Example: Ridership on Amtrak Trains}

<<label=Amdata,fig=TRUE,include=FALSE, echo=FALSE>>=
Amtrak.data <- read.csv("Data/Amtrak data.csv", sep=";", stringsAsFactors = FALSE)
# transforming the ''Month" column into a proper date variable
Amtrak.data %>%  mutate(day = myd(paste0(Month, "-01"))) -> Amtrak.data
Amtrak.data$t <- c(1:nrow(Amtrak.data))
Amtrak.baseplot <- ggplot(data=Amtrak.data) +
                          geom_line(aes(x=t, y=Ridership), size=1, color="red") +
                          labs(title="Amtrak Ridership data") +
                          JT.theme
Amtrak.baseplot
@

\begin{marginfigure}
\includegraphics[width=1\textwidth]{PTSFR-Amdata}
\caption{}
\label{fig:Amdata}
\setfloatalignment{b}
\end{marginfigure}


\newpage
Other operations are:
\subsection{Zooming}\index{visualization!zooming}
\label{subsec:zoomingin}
\newthought{Example} 
We use the time series of the number of vehicles in the Baregg Tunnel.

<<label=Baregg,fig=TRUE,include=FALSE, echo=FALSE>>=
Baregg.data <- read.csv("Data/BareggTunnelTraffic.csv", sep=",", stringsAsFactors = FALSE)
Baregg.data$t <- c(1:nrow(Baregg.data))
Baregg.data$day <- dmy(Baregg.data$day)
Baregg.baseplot <- ggplot(data=Baregg.data) +
  geom_line(aes(x=day, y=number), size=1, color="red") +
  labs(title="Number of vehicles in Baregg Tunnel") +
  JT.theme
p2 <- ggplot(data=Baregg.data) +
  geom_line(aes(x=day, y=number), size=1, color="red") +
  scale_x_date(limits = dmy(c("01-02-2004","31-05-2004")), breaks = dmy(c("01-02-2004", "01-03-2004", "01-04-2004", "01-05-2004", "31-05-2004")))  +
  labs(title="Number of vehicles in Baregg Tunnel\nzooming from 1-Feb-2004 to 31-May-2004") +
  JT.theme
p3 <- ggplot(data=Baregg.data) +
  geom_line(aes(x=day, y=number), size=1, color="red") +
  scale_x_date(limits = dmy(c("01-02-2004","08-02-2004")), breaks = dmy(c("01-02-2004", "02-02-2004", "03-02-2004", "04-02-2004", "05-02-2004", "06-02-2004", "07-02-2004", "08-02-2004")))  +
  labs(title="Number of vehicles in Baregg Tunnel\nzooming week of 1-Feb-2004") +
  JT.theme
grid.arrange(Baregg.baseplot, p2, p3, nrow=3)
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-Baregg}
\caption{}
\label{fig:Baregg}
\setfloatalignment{b}
\end{figure}

\newthought{addcon time series}
<<label=addconviz,fig=TRUE,include=FALSE, echo=FALSE>>=
p1 <- ggplot(data=construct, aes(x=t)) +
  geom_line(aes(y=addcon), size=1, color="red") +
  labs(title="Additive constructed model\nfull range") +
  JT.theme
p2 <- ggplot(data=construct, aes(x=t)) +
  geom_line(aes(y=addcon), size=1, color="red") +
  scale_x_continuous(limits = c(250,500), breaks = seq(250, 500, by=10))  +
  scale_y_continuous(limits = c(-10,40), breaks = seq(-10, 40, by=5))  +
  labs(title="Additive constructed model\nrange = 250 - 500") +
  JT.theme
p3 <- ggplot(data=construct, aes(x=t)) +
  geom_line(aes(y=addcon), size=1, color="red") +
  scale_x_continuous(limits = c(275,285), breaks = seq(275, 285, by=1))  +
  scale_y_continuous(limits = c(-5,15), breaks = seq(-5, 15, by=1))  +
  labs(title="Additive constructed model\nrange = 275 - 285") +
  JT.theme
grid.arrange(p1, p2, p3, nrow=3)
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-addconviz}
\caption{Various levels of detail}
\label{fig:addconviz}
\setfloatalignment{b}
\end{figure}

First we plot the whole range. Then we determine the range where we can see a few periods of a possible seasonal. Then we go in for some more detail(Figure~\ref{fig:addconviz}). Looking at the top graph I can see a seasonal pattern of approximatively 9 periods over a range from 0 to 1000. A first estimate for the seasonal component is that it has a period of 111 (not bad given that it is N/9). The other, more detailed graphs, do not improve on this.

\newthought{combicon time series}
<<label=combiconviz,fig=TRUE,include=FALSE, echo=FALSE>>=
p1 <- ggplot(data=construct, aes(x=t)) +
  geom_line(aes(y=combicon), size=1, color="red") +
  labs(title="Combined constructed model\nfull range") +
  JT.theme
p2 <- ggplot(data=construct, aes(x=t)) +
  geom_line(aes(y=combicon), size=1, color="red") +
  scale_x_continuous(limits = c(250,500), breaks = seq(250, 500, by=10))  +
  scale_y_continuous(limits = c(0,35), breaks = seq(0, 35, by=5))  +
  labs(title="Combined constructed model\nrange = 250 - 500") +
  JT.theme
p3 <- ggplot(data=construct, aes(x=t)) +
  geom_line(aes(y=combicon), size=1, color="red") +
  scale_x_continuous(limits = c(350,375), breaks = seq(350, 375, by=1))  +
  scale_y_continuous(limits = c(7.5,17.5), breaks = seq(7.5, 17.5, by=2.5))  +
  labs(title="Combined constructed model\nrange = 350 - 375") +
  JT.theme
grid.arrange(p1, p2, p3, nrow=3)
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-combiconviz}
\caption{Various levels of detail}
\label{fig:combiconviz}
\setfloatalignment{b}
\end{figure}

First we plot the whole range. Then we determine the range where we can see a few periods of a possible seasonal. Then we go in for some more detail (Figure~\ref{fig:combiconviz}). The top graph shows a seasonal pattern that starts around t=125 and ends at t=1000 after 8 periods. A first estimate for the seasonal component is that it has a period of (1000 - 125)/8=109,4 (again not bad given that it is N/9). The other, more detailed graphs, do not improve on this.

\newthought{Using plotly}

We can turn a \textit{ggplot} into a more flexible \textit{plotly}-object by using the \textbf{plotly}-package and the command \textit{ggplotly(p)} where p is the \textit{ggplot}-object. The full range combined constructed model gets a dynamic zoom window in this way. The result is not a pdf-file, so it cannot be loaded into the Sweave file. But it is a quick tool to do some zooming.

<<echo=FALSE>>=
ggplotly(p1)
@

\newthought{Time Series Objects in R and the parameter frequency}
We can do the visualising perfectly with \textit{ggplot}, which means that az yet we don't need to force the data into a time series object\index{time series object}. Doing so requires you to select a value for the parameter ''frequency"\index{time series!frequency}. First of all: it is \textbf{NOT} a frequency, but a period! Not Hz but sec (or other time scale). While this parameter can be chosen reasonably in certain situations (e.f. the Amtrak data where values are recorded monthly and we can assume a seasonality of 1 year = 12 months), it is (at present) not so clear for the Belragg data or the constructed time series (addcon and combicon). Rob Hyndman\sidenote{\url{https://otexts.org/fpp2/}} gives a few examples:
\begin{itemize}
	\item if you have monthly records, set the frequency to 12. The assumption is that the seasonality has a period of one year
	\item if you have weekly records, set the frequency to 52. Again the assumption is that the seasonality has a period of one year. However: a year is not exactly 52 weeks, but 52,14... However: most time series objects cannot have a non-integer frequency parameter
	\item if you have daily records, set the frequency to 7. Here we assume that many variables will show a weekly pattern (working days on the one hand, weekend on the other). So for the Belragg data is could be sensible to set the parameter frequency to 7.
\end{itemize}

\newthought{Amtrak Riderschip as a time series object}
<<label=Amtrakts, fig=TRUE, include=FALSE, echo=FALSE>>=
ridership.ts <- ts(Amtrak.data$Ridership, start = c(1991,1), end = c(2004,3), frequency = 12)
plot(ridership.ts, xlab = "Time", ylab = "Ridership", ylim = c(1300, 2300), bty = "l")
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-Amtrakts}
\caption{Amtrak Riderschip as a time series object}
\label{fig:Amtrakts}
\setfloatalignment{b}
\end{figure}

\newthought{Example: Baregg Number of Vehicles as a time series object}
The data are collected daily, starting on 2003-11-01. That is the start of week 44 in the year 2003. When we group them  in a weekly pattern (frequency=7) we can create the following time series object:

<<label=Bareggts, fig=TRUE, include=FALSE, echo=FALSE>>=
number.ts <- ts(Baregg.data$number, start = c(44,1), frequency = 7)
plot(number.ts, xlab = "Time", ylab = "Number of vehicles in Baregg tunnel", ylim = c(50000, 150000), bty = "l")
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-Bareggts}
\caption{Baregg Number of Vehicles as a time series object}
\label{fig:Bareggts}
\setfloatalignment{b}
\end{figure}

\newpage
\subsection{Change the scale}
Meant is: try a transformation. If a trend is exponential, a log-transformation\sidenote{all y-values have to be positive for such a transformation!} will lead to a linear relation, which is much easier to spot. Let's try this for the addcon-time series:

<<label=addconscale, fig=TRUE, include=FALSE, echo=FALSE>>=
ggplot(data=construct, aes(x=t)) +
  geom_line(aes(y=log(addcon)), size=1, color="red") +
  labs(title="Log-transformed addcom time series") +
  JT.theme
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-addconscale}
\caption{}
\label{fig:addconscale}
\setfloatalignment{b}
\end{figure}

\subsection{Add a trend line}
The advice here is to do this preliminary work in Excel (or Numbers). The function "add trendline" allows for a quick an dirty check.

In Figure~\ref{fig:addconnumberstrend}) and Figure~\ref{fig:combiconnumberstrend} we can see the trendlines with the highest $R^{2}$ values. For the ''addcon" time series this a second degree polynomial (surprise!), for the ''combicon" time series it is a power law approximation.

\begin{marginfigure}[-2cm]
\includegraphics[width=0.85\textwidth]{PTSFR-addconnumberstrend}
\caption{Trend with Numbers for addcon}
\label{fig:addconnumberstrend}
\setfloatalignment{b}
\end{marginfigure}

\begin{marginfigure}[0cm]
\includegraphics[width=0.85\textwidth]{PTSFR-combiconnumberstrend}
\caption{Trend with Numbers for combicon}
\label{fig:combiconnumberstrend}
\setfloatalignment{b}
\end{marginfigure}

\newpage
\subsection{Suppressing seasonality}

It is often easier to see trends in the data when seasonality is suppressed. We can use:

<<label=Amtrakagg, fig=TRUE, include=FALSE, echo=FALSE>>=
    Amtrak.data %>% group_by(year(day), quarter(day)) %>% summarise(mean(Ridership)) -> Amtrak.year
    names(Amtrak.year) <- c("year","quarter", "mean")
    Amtrak.year$yrqt <- paste(as.character(Amtrak.year$year), "-", Amtrak.year$quarter)
    Amtrak.year <- as.data.frame(Amtrak.year)
    ggplot(data = Amtrak.year) +
      geom_line(aes(x=yrqt, y= mean, group=1), size=1, color="red") +
      scale_y_continuous(limits= c(1000, 2500)) +
      theme(axis.text.x=element_text(angle=-45, hjust=0.001)) +
      labs(title="Quarterly average of riderschip") +
      JT.theme
@

<<label=Bareggagg, fig=TRUE, include=FALSE, echo=FALSE>>=
    Baregg.data %>% group_by(year(day), month(day)) %>% summarise(mean(number)) -> Baregg.month
    names(Baregg.month) <- c("year", "month","mean")
    Baregg.month$yrmt <- paste(as.character(Baregg.month$year), "-", Baregg.month$month)
    Baregg.month <- as.data.frame(Baregg.month)
    ggplot(data = Baregg.month) +
      geom_line(aes(x=yrmt, y= mean, group=1), size=1, color="red") +
      scale_y_continuous(limits= c(0, 125000)) +
      theme(axis.text.x=element_text(angle=-45, hjust=0.001)) +
      labs(title="Monthly average of numbers in Baregg tunnel") +
      JT.theme
@ 

\begin{itemize}
	\item a larger time scale: aggregating daily data into weeks or months, monthly data into years.
	\begin{enumerate}
	  \item \newthought{Amtrak data}: grouped per quarter (a year is too long)
      \begin{marginfigure}[0cm]
        \includegraphics[width=1\textwidth]{PTSFR-Amtrakagg}
        \caption{Amtrak}
        \label{fig:Amtrakagg}
        \setfloatalignment{b}
      \end{marginfigure}
    \item \newthought{Baregg data}
      \begin{marginfigure}
        \includegraphics[width=1\textwidth]{PTSFR-Bareggagg}
        \caption{Baregg tunnel}
        \label{fig:Bareggagg}
        \setfloatalignment{b}
      \end{marginfigure}
    \end{enumerate}
\end{itemize}

Looking at these results, this seems an awkward way of finding the trend. A lot depends on the choice of the level of aggregation. This does not seem to improve on finding a first idea of the trend by using the inbuilt functions of Excel (of Numbers).

\subsection{Interactive visualization}

Specific software (e.g. Tableau) offer the possibility of visualizing\index{visualization} data dynamically so that you can do a number of the above mentioned techniques (zooming, trending, suppressing seasonality) quickly. As mentioned in \ref{subsec:zoomingin} zooming\index{visualization!zooming} can quickly be done on an existing \textit{ggplot}-object by using the \textbf{plotly}-library. It is not ''Tableau" but it's quick and free.

The book offers more examples of visualization packages that allow you to zoom and filter across multiple data sets. Could be done with \textit{ggplot} and \textit{plotly} but with a great deal of effort!

\section{Data pre-processing}\index{data!quality}

\subsection{Exploratory Data Analysis}\index{EDA = Exploratory Data Analysis}
See EDA.pdf

Using the libraries \textit{broom} and \textit{funModelling} we can get a quick overview of the characteristics of the data frame:

\newthought{Number of rows, variables and head of the first row}
<<>>=
glimpse(Amtrak.data)
@

\newpage
\newthought{Metrics on data types, zeros, infinite numbers, missing values}\index{data!zeros}\index{data!missing values}\index{data!infinite numbers}
<<>>=
df_status(Amtrak.data)
@

\newthought{Distribution of the numerical value}
<<label=dist, fig=TRUE, include=FALSE, echo=FALSE>>=
plot_num(data=Amtrak.data)
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-dist}
\caption{distribution of numerical data}
\label{fig:dist}
\setfloatalignment{b}
\end{figure}

\newthought{Checking for extreme values}\index{data!extreme values}
<<>>=
describe(Amtrak.data$Ridership)
@

\subsection{Missing values}\index{data!missing values}

Missing values should turn up in the Exploratory Data Analysis and/or the visualization process. But they can be subtle. So we can detect them by finding the number of values there should be between the start and the end of the time series, and the number of values in the data:

<<echo=TRUE>>=
(interval(ymd(Amtrak.data$day[1]), ymd(Amtrak.data$day[nrow(Amtrak.data)])) %/% months(1) + 1) -
      nrow(Amtrak.data)
(interval(ymd(Baregg.data$day[1]),ymd(Baregg.data$day[nrow(Baregg.data)])) %/% days(1) + 1) -
      nrow(Baregg.data)
@

To check: let's construct a faulty time series: two weeks of data with the weekends missing:
<<>>=
faulty <- data.frame(day = seq(ymd('2012-04-09'), ymd('2012-04-22'), by = '1 day'),
                     y = seq(0:13)) 
faulty$weekday <- wday(faulty$day, label=TRUE)
faulty %>% dplyr::filter(as.character(weekday) != "Sun" &
                         as.character(weekday) !="Sat") -> faulty
(interval(ymd(faulty$day[1]),ymd(faulty$day[nrow(faulty)])) %/% days(1) + 1) - nrow(faulty)
@

Many time-series forecasting methods (such as ARIMA) cannot stand time-series with ''holes". Usually they are filled by \emph{imputation}\index{data!imputing missing values}

Other methods (e.g. linear or logistic regression models) are not influenced when there are missing values.

\subsection{Unequally spaced series}

Spacing can be checked with the \textit{diff}-command:

<<>>=
diff(Amtrak.data$day)
diff(Baregg.data$day)
@

\subsection{Extreme values}\index{data!extreme values}
Extreme values can be discarded if they are unique (e.g. the result of an earthquake) and unlikely to repeat itself in the forecasting period. When in doubt: do two forecasts: one with and one without the extreme values.

\subsection{Choice of time span}\index{forecast!horizon}
How far back in the past should we consider data to be relevant? Older data may have resulted from a different context and environment which are not relevant to the situation in the forecasting period. The rule is the \emph{we extend our time series backwards for only those periods where the environment is assumed to be similar to the forecast horizon}.

\chapter{Performance evaluation}

Using the same data to develop the forecasting model \textbf{and} to assess iets performance, we introduce \emph{bias}\index{bias}\index{performance!bias}. Our model will be fine tuned to this particular set of data. It will not only be adjusted to the systematic component of the data, but also to the noise. The metaphor of the tailor making a perfectly fitting suit for a customer is apt. The suit will not be useful when the customer gains or loses some weight. From this observation follows the idea of partitioning of the data set.

\section{Data partitioning}\index{partition}\index{data!partition}

\emph{Partitioning} is the splitting of the data set into two parts: forecasting models are built using the data in part 1. Using these models we make a forecast of part 2. The differences between the forecasted values and the true values of part 2 are used to measure the performance of different forecasting models.

\subsection{Partitioning of cross-sectional data}

Cross-sectional data\index{data!cross sectional} have values for different variables at one specific moment in time. Then we create three partitions: the \emph{training set}\index{training set}, a \emph{validation set}\index{validation set} and a \emph{test set}\index{test set}. The partitioning is done \emph{at random}: we (the computer) picks at random which data elements go into each set.

\subsection{Temporal partitioning}
When our goal is forecasting, we want to predict the future. Therefore there is no \emph{test set} available. Partitioning is \emph{random} because this would create \emph{missing values}\index{missing values} into both the \emph{training set} and the \emph{validation set}, and it would miss the logical temporal sequence of past and future. Visualization of actual an predicted series for both sets is interesting, because it could indicate overfitting when the predicted and actual series are close to each other in the training period, but not in the validation period.

\subsection{Joining partitions for forecasting}
Before attempting to predict the, unknown, future values we will use our forecasting methods on the whole time series: the joined training and validation sets. This has the following advantages:
\begin{itemize}
	\item the validation period is the most recent and therefore more in tune with the future we wish to forecast
	\item more data can lead to better estimation of model parameters
	\item using only the training set will require that the forecasting must be done further in the future
\end{itemize}

\subsection{Choosing the validation period}
The length of the validation period is determined by the forecast horizon\index{forecast!horizon}. A shorter validation period does not give good information on the quality of our forecasting model at the end of the forecasting period. A longer validation period reduces the amount of data in the training set, and specifically the most recent data.

If we were to make a forecast on the Amtrak data for the next 3 years (=36 months) we would set the validation period nValid to 36. Consequently the training set (nTrain) would be limited to the whole range of Amtrak.data (=nrow(Amtrak.data)) minus nValid. We will be using the times series object that was created before. We include a polynomial regression for the trend. This gives us the following code (you need the library \textbf{forecast}):

<<label=Validationperiod, fig=TRUE, include=FALSE, echo=FALSE>>=
nValid <- 36
nTrain <- length(ridership.ts) - nValid
train.ts <- window(ridership.ts, 
                   start = c(1991, 1), 
                   end = c(1991, nTrain))
valid.ts <- window(ridership.ts, 
                   start = c(1991, nTrain + 1), 
                   end = c(1991, nTrain + nValid))
ridership.lm <- tslm(train.ts ~ trend + I(trend^2))
ridership.lm.pred <- forecast(ridership.lm, h = nValid, level = 0)
plot(ridership.lm.pred, 
     ylim = c(1300, 2600), 
     ylab = "Ridership", 
     xlab = "Time", 
     bty = "l",
     xaxt = "n",
     xlim = c(1991, 2006.25),
     main = "",
     lty = 2)
axis(1, at = seq(1991, 2006, 1), labels = format(seq(1991, 2006,1)))
lines(ridership.lm$fitted, lwd = 2)
lines(valid.ts)
abline(v = 1991 , col="blue")
abline(v = 1991 + (nTrain)/12 , col="blue")
abline(v = 1991 + (nTrain + nValid)/12, col="blue")
text(x = 1996, y = 2400, labels="Training")
arrows(x0 = 1991, y0 = 2300, 
       x1 = 1991 + nTrain/12, y1 = 2300, 
       code = 3, length = 0.1)
text(x = 2002.6, y = 2400, labels="Validation")
arrows(x0 = 1991 + (nTrain)/12, y0 = 2300, 
       x1 = 1991 + (nTrain + nValid)/12, y1 = 2300, 
       code = 3, length = 0.1)
text(x = 2005.5, y = 2400, labels="Future")
arrows(x0 = 1991 + (nTrain + nValid + 1)/12, y0 = 2300, 
       x1 = 1991 + (nTrain + nValid + 36)/12, y1 = 2300, 
       code = 2, length = 0.1)
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-Validationperiod}
\caption{Amtrak Riderschip validation period}
\label{fig:Validationperiod}
\setfloatalignment{b}
\end{figure}

\section{Naive forecasts}\index{forecast!naive}

\emph{Naive forecasts} have the merit that they are simple, easy to understand and, in some cases, are sufficient to achieve the forecasting goal. If not, their performance can by used as a benchmark for more complicated model.

\begin{itemize}
	\item For a time series that does not show substantial seasonality, or if the seasonality has a period that is much greater than the forecast horizon, the \emph{naive} forecast is \emph{the most recent value} of the time series. The k-step-ahead forecast is:
    \begin{equation}
    	F_{t_{latest}+k} = y_{t_{latest}} = constant
    \end{equation}
        Graphically it means that the forecasted values are on a horizontal line for the whole forecasting horizon.
  \item For a time series with seasonality with known period T that extends into the forecasting horizon the \emph{naive} forecast is \emph{the value - 1 period} of the time series. The k-step-ahead forecast is:
    \begin{equation}
    	F_{t_{latest} + 1} = y_{t_{latest} + 1 - T}
    \end{equation}
        Graphically it means that the forecasted values are a repeat of the last known period
\end{itemize}

It is the equivalent of the principle of \emph{persistency} that is used as a naive forecast in weather forecasting.

\section{Measuring predictive accuray}\index{accuracy}\index{forecast!accuracy}

\emph{Predictive accuracy} is not the same as \emph{Goodness of Fit}. The latter measure how well the model fits the data. In forecasting however we are interested in the question of how wel the model, based on the \emph{training set}, predicts the values in the \emph{validation set}.

\subsection{Metrics for prediction accurary}\index{forecast!accuracy metrics}

The \emph{forecast error}\index{forecast!error}\index{forecast!accuracy} or \emph{residual}\index{residual} is the difference between the actual value $y_{t}$ and the forecast value $F_{t}$:
\begin{equation}
	e_{t}=y_{t} - F_{t}
\end{equation}

The validation set has time indexes $t_{start + n_{Train}} + i$, with $i=1 \ldots n_{Valid}$, than we can define the following metrics:
\begin{itemize}
	\item Mean Absolute Error (or Deviation) (MAE of MAD)
	  \begin{equation}
	    MAE = \frac{1}{n_{Valid}}\sum_{1}^{n_{Valid}} |e_{i}|
    \end{equation}
  \item Average Error: idem but without the absolute values. Gives an idea of under- or overprediction
     \begin{equation}
	    MAE = \frac{1}{n_{Valid}}\sum_{1}^{n_{Valid}} e_{i}
    \end{equation}
  \item Mean Absolute Percentage Error: relative absolute error. Used when comparing performance across time series with different scales
     \begin{equation}
	    MAPE = \frac{1}{n_{Valid}}\sum_{1}^{n_{Valid}}| \frac{e_{i}}{y_{i}}*100 | 
    \end{equation}
  \item Root Mean Square Error
    \begin{equation}
	    RMSE = \sqrt{ \frac{1}{n_{Valid}}\sum_{1}^{n_{Valid}} e_{i}^{2} }
    \end{equation}
\end{itemize}

Applying these principles and definitons to the Amtrak.data time series we get:

<<>>=
forecast.error <- valid.ts - ridership.lm.pred$mean
MAE <- (1/nValid)*sum(abs(forecast.error))
MAE
AvEr <- (1/nValid)*sum(forecast.error)
AvEr
MAPE <- (1/nValid)*sum(abs(forecast.error*100/valid.ts))
MAPE
RMSE <- sqrt((1/nValid)*sum(forecast.error^2))
RMSE
@

or we immediately use the \textit{accuracy}-function\index{accuracy}\index{forecast!accuracy}:
<<>>=
accuracy(ridership.lm.pred$mean, valid.ts)
@

\subsection{Zero counts}
Calculating MAPE is impossible when $y_{i}=0$. When you cannot reasonably exclude these values, another metric is used: the MASE = Mean Absolute Scaled Error. It devides the model MAE by the MAE of the naive forecast \emph{on the training set}:

\begin{equation}
	MASE = \frac{validation MAE}{training MAE of naive forecast} = \frac{\frac{1}{n_{Valid}}\sum_{1}^{n_{Valid}} |e_{i}|}{\frac{1}{n_{Train}-1}\sum_{1}^{n_{Train}} |y_{t-1} - y_{t}|}
\end{equation}

Values of MASE lower higher than 1 indicate a poorer performance than the naive forecast.

\subsection{Forecast accuracy vs. profitability}\index{forecast!accuracy}\index{forecast!cost}
Some metrics (e.g. MAPE and RMSE) inflate the effect of large errors. But the effects, or costs, of the error should also come into consideration. Sometimes the cost of a forecast error is big for positive errors, but not for negative ones. Sometimes the cost is proportional to the error, but the relation can be non-linear: it can be that the cost is the same once a certain error treshold is crossed.A measure of profitability could be very usefull when choosing the forecast method.

\section{Evaluating forecast uncertainty}

\subsection{Distribution of forecast errors}

We should go beyond the clustered metrics like MAPE or RMSE. It is interesting to examine the distribution of forecasting errors: are there extremely low or high errors. For example: we have constructed a second degree polynomial trend model (ridership.lm) based on the training set. Using this model we can predict the ridership data both for the training set and for the validation set (ridership.lm.pred).

<<label=Residuals, fig=TRUE, include=FALSE, echo=FALSE>>=
par(mfrow = c(2,1))
plot(ridership.lm.pred$residuals, 
     ylim = c(-400, 400), 
     ylab = "Residuals", 
     xlab = "Time", 
     bty = "l",
     xaxt = "n",
     xlim = c(1991, 2006.25),
     main = "",
     lty = 1)
axis(1, at = seq(1991, 2006, 1), labels = format(seq(1991, 2006,1)))
lines(valid.ts - ridership.lm.pred$mean)
abline(v = 1991 , col="blue")
abline(v = 1991 + (nTrain)/12 , col="blue")
abline(v = 1991 + (nTrain + nValid)/12, col="blue")
text(x = 1996, y = 350, labels="Training")
arrows(x0 = 1991, y0 = 300, 
       x1 = 1991 + nTrain/12, y1 = 300, 
       code = 3, length = 0.1)
text(x = 2002.6, y = 350, labels="Validation")
arrows(x0 = 1991 + (nTrain)/12, y0 = 300, 
       x1 = 1991 + (nTrain + nValid)/12, y1 = 300, 
       code = 3, length = 0.1)
text(x = 2005.5, y = 350, labels="Future")
arrows(x0 = 1991 + (nTrain + nValid + 1)/12, y0 = 300, 
       x1 = 1991 + (nTrain + nValid + 36)/12, y1 = 300, 
       code = 2, length = 0.1)
hist(ridership.lm.pred$residuals, 
     ylab = "Frequency", 
     xlab = "Residuals", 
     bty = "l", 
     main = "")
par(mfrow=c(1,1))
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-Residuals}
\caption{Amtrak Riderschip Residuals}
\label{fig:Residuals}
\setfloatalignment{b}
\end{figure}

\subsection{Prediction intervals}
A \emph{prediction intervel}\index{prediction interval} gives a confidence interval\index{confidence interval} of the forecast. If the forecast error is normally distributed we can use standard techniques to calculate the confidence intervals. For forecast models where the error does not have a normal distribution (e.g. the Amtrak second order polynomial model with residuals given in Figure\ref{fig:Residuals}) we can use the 5th and 95th quantile from the residuals to construct a confidence interval.

\subsection{Prediction cones}
Certain forecast models will have confidence intervals that become larger when our forecasts are made for points more and more into the future.

\section{Advanced Data Partitioning: roll-forward validation}\index{roll-forward validation}
When we partition the time series into one, fixed, training set and one, fixed, validation set then our forecasting method will give us only one prediction trajectory into the future.

When using \emph{roll-forward validation} we have a number of partitions where we start from one training/validation combination, and make new training/validation sets by extending the training set with one period, and consequently reduce the validation set with the same period. For example: with the Amtrak time series we started with a validation set that had 36 months. With \emph{roll-forward validation} we create 36 partitionings where the validation set has 36, 35, 34 ... 1 months of data, and the training set grows each time with one month. When we keep the forecast horizon equal to the length of the validation set, we will get forecasts for 36 months from the first partition, 35 months from the second, ending with 1 month forecast from the last partition. This means that in the end we will have 36 forecasts for month 1 into the future, 35 forecasts for mont 2 into the future and 1 forecast for month 36 into the future. Roll-forward partitioning is also a good choice when our time series is dynamic and we get new information on a steady basis. Our time series grows in time, and we can enlarge the training set and reduce the validation set if the forecast date(s) remain the same.

For example: if we use a very simple forecast model (naive, only trend), then for the fixed partition our model for the next 36 months is a constant equal to the last value of $y_{i}$ in the training set.

<<>>=
forecast.fixed <- rep(last(train.ts), nValid)
forecast.fixed.ts <- ts(forecast.fixed, 
                        start = c(2001,4), 
                        end = c(2004, 3), 
                        freq = 12)
@

In a \emph{roll-forward validation} we 

\begin{itemize}
	\item start with a 36 period validation set, which gives 36 forecasts for the future equal to the last value in the training set
	\item continue with a 35 period validation set, resulting in 35 forecast equal to the last value of the training set \textbf{+ 1}. This is the first value in the fixed validation set
	\item a 34 period validation set, with 34 forecasts equal to the second value in the fixed validation set
	\item ...
	\item a 1 period validation set, with 1 forecast equal to the last value \textbf{but one} in the fixed validation set
\end{itemize}

The forecasts are thus given by:
<<>>=
forecast.roll.fwd <- append(last(train.ts), valid.ts[1:(length(valid.ts)-1)])
forecast.roll.fwd.ts <- ts(forecast.roll.fwd, 
                           start = c(2001,4), 
                           end = c(2004, 3), 
                           freq = 12)
@

<<label=rollforward, fig=TRUE, include=FALSE, echo=FALSE>>=
plot(ridership.lm.pred, 
     ylim = c(1300, 2600), 
     ylab = "Ridership", 
     xlab = "Time", 
     bty = "l",
     xaxt = "n",
     xlim = c(1991, 2006.25),
     main = "",
     lty = 1)
axis(1, at = seq(1991, 2006, 1), labels = format(seq(1991, 2006,1)))
lines(ridership.lm$fitted, lwd = 2)
lines(valid.ts, lty = 1)
lines(forecast.fixed.ts, lty = 3, col="red")
lines(forecast.roll.fwd.ts, lty = 2, col="red")
abline(v = 1991 , col="blue")
abline(v = 1991 + (nTrain)/12 , col="blue")
abline(v = 1991 + (nTrain + nValid)/12, col="blue")
text(x = 1996, y = 2400, labels="Training")
arrows(x0 = 1991, y0 = 2300, 
       x1 = 1991 + nTrain/12, y1 = 2300, 
       code = 3, length = 0.1)
text(x = 2002.6, y = 2400, labels="Validation")
arrows(x0 = 1991 + (nTrain)/12, y0 = 2300, 
       x1 = 1991 + (nTrain + nValid)/12, y1 = 2300, 
       code = 3, length = 0.1)
text(x = 2005.5, y = 2400, labels="Future")
arrows(x0 = 1991 + (nTrain + nValid + 1)/12, y0 = 2300, 
       x1 = 1991 + (nTrain + nValid + 36)/12, y1 = 2300, 
       code = 2, length = 0.1)
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-Rollforward}
\caption{Amtrak Riderschip roll-forward validation}
\label{fig:Rollforward}
\setfloatalignment{b}
\end{figure}

However, this is not a good example! Of course our forecast in the validation period is better because we use the actual data from that period! We could have made it a perfect fit by choosing the actual last data point instead of the data point minus 1. A better example would be to determine the second degree polynomial model based on the dynamic training set and calculate the forecasting metrics using the dynamic validation set.

\section{Example: comparing two models}

Using the Amtrak ridership data we compare two forecast methods: naive and seasonal naive. For both we use fixed partitioning and roll-forward partitioning. \textsf{R} packages like \textit{forecast} have functions \textit{naive} and \textit{snaive} that generate the forecast models.

\subsection{Fixed partitioning}

<<>>=
fixed.nValid <- 36
fixed.nTrain <- length(ridership.ts) - fixed.nValid
train.ts <- window(ridership.ts, 
                   start = c(1991, 1),
                   end = c(1991, fixed.nTrain))
valid.ts <- window(ridership.ts, 
                   start = c(1991, fixed.nTrain + 1), 
                   end = c(1991, fixed.nTrain + fixed.nValid))
naive.pred <- naive(train.ts, h = fixed.nValid)
snaive.pred <- snaive(train.ts, h = fixed.nValid)
accuracy(naive.pred, valid.ts)
accuracy(snaive.pred, valid.ts)
@

\subsection{Roll-forward partitioning}

<<>>=
fixed.nValid <- 36
fixed.nTrain <- length(ridership.ts) - fixed.nValid
stepsAhead <- 1
error.naive <- rep(0, fixed.nValid - stepsAhead + 1)
percent.error.naive <- rep(0, fixed.nValid - stepsAhead + 1)
error.snaive <- rep(0, fixed.nValid - stepsAhead + 1)
percent.error.snaive <- rep(0, fixed.nValid - stepsAhead + 1)
for  (j in fixed.nTrain:(fixed.nTrain + fixed.nValid - stepsAhead)) {
  train.ts <- window(ridership.ts, 
                    start = c(1991, 1),
                    end = c(1991, j))
  valid.ts <- window(ridership.ts, 
                   start = c(1991, j + stepsAhead), 
                   end = c(1991, fixed.nTrain + fixed.nValid))
  naive.pred <- naive(train.ts, h = stepsAhead)
  snaive.pred <- snaive(train.ts, h = stepsAhead)
  error.naive[j - fixed.nTrain + 1] <- valid.ts - naive.pred$mean[stepsAhead]
  percent.error.naive[j - fixed.nTrain + 1] <- error.naive[j - fixed.nTrain + 1]/valid.ts
  error.snaive[j - fixed.nTrain + 1] <- valid.ts - snaive.pred$mean[stepsAhead]
  percent.error.snaive[j - fixed.nTrain + 1] <- error.snaive[j - fixed.nTrain + 1]/valid.ts
}
print("naive")
mean(abs(error.naive))
sqrt(mean(error.naive^2))
mean(abs(percent.error.naive))
print("snaive")
mean(abs(error.snaive))
sqrt(mean(error.snaive^2))
mean(abs(percent.error.snaive))
@

\chapter{Forecasting methods: overview}

\section{Model-based vs. Data-driven methods}\index{forecast!model-based methods}\index{forecast!data-driven methods}\index{model-based methods}\index{data-driven methods}\index{methods!model-based}\index{methods!data-driven}

\emph{Model-based methods} use statistical, mathematical or other scientific models to approximate a time series. E.g. based on some prior knowledge of the subject we can reasonably assume a linear relation in time: $y(t)=b_{0} + b_{1}t$. The training data and a statistical method (linear regression) are used to determine the parameters $b_{0}$ and $b_{1}$ and the confidence interval on the predicted (forecasted) values. Model-based methods are very usefull when the data set is small: we do not need a lot of data points when we ''know" the type of relationship. These methods are better when the pattern is \emph{global}.

\emph{Data-driven methods} find a pattern within the data, without presuming the know the relation of the data with time. A typical example is finding the trend using a \emph{moving average} or other smoothing technique. Naive methods are also only based on the data. Data-driven methods usually need more data points, but they have the advantage of being less user dependent and they adapt automatically when additional data become available. These methods adapt themselves to \emph{local} patterns.

\section{Extrapolation methods, Econometric methods, External information}
\emph{Extrapolation methods}\index{extrapolation methods} such as forecasting learn from history and forecast the future. Even when the forecast is done for multiple time series, each serie is forecast based on its own history.

\emph{Econometric methods}\index{econometric methods} take into account the cross-correlation between series from a causal standpoint. Information from one or more other series are used as input for the forecast of the time series under study.\emph{Multivariate time series}\index{multivariate time series}\index{time series!multivariate} directly model the cross-correlations between sets of series.

Using \emph{external information} to improve a forecast is a third method. The golder rule in this case is to \emph{only use values that are available at the time of prediction}.

\section{Manual vs. Automated Forecasting}\index{forecast!manual}\index{forecast!automatic}

\emph{Model-based methods}\index{methods!model-based} are usually built on assumptions and therefore they require constant attention to make sure that the assumptions are still valid. They fall within the domain of \emph{manual forecasting}. \emph{Data-driven methods}\index{methods!data-driven} learn from the data, and are more easily automated.

\section{Combining methods and Ensembles}

\emph{Combining}\index{methods!combining} the results of different forecasting methods often lead to better results than relying on only one method. A two step action where method one makes a forecast based on the original time series, and method two uses the forecast errors to predict future forecast errors and in this way ''corrects" the first method.

\emph{Ensembles}\index{ensemble}\index{methods!ensembles} is the name for multiple forecasts generated by different methods starting from the same original time series. A weighed average of the forecast results is the final forecast. Different forecasting methods capture different aspects of the time series, and a combination of the results is often better than relying on only one method.

\emph{Using different time series} that measure the variable of interest can also lead to better forecasts.

\emph{Combining methods} and \emph{Ensembles} have a number of disadvantages:
\begin{itemize}
	\item costly
	\item require knowledge of the methods used
	\item good procedures! Agree beforehand how the different elements will be weighed.
\end{itemize}

\chapter{Smoothing methods}

\emph{Smoothing methods}\index{smoothing}\index{methods!smoothing} are \emph{data-driven} and therefore mostly \emph{automated}. However, care must be taken in the choice of the \emph{smoothing constants}, and this is \emph{manual} work.

\section{Introducton}
\emph{Smoothing methods}\index{methods!smoothing} estimate time series components directly from the data, \emph{without predetermined structure} e.g. a linear evolution in time. When the time series components (e.g. the trend) changes in time, a smoothing method will adapt to this change. \emph{Smoothing methods} filter out the noise\index{noise}, and doing so can reveal the underlying pattern (trend and/or seasonality).

\section{Moving Average}

The \emph{moving average smoother}\index{moving average} comes in two flavors: \emph{centered}\index{smoothing!moving average!centered}\index{CMA} and \emph{trailing}\index{smoothing!moving average!trailing}\index{TMA}. In both cases we look at the time series through a \emph{window} with width $w$ = the number of consecutive values we take into account.

\subsection{Centered moving average}\index{moving average!centered}

\begin{equation}
	MA_{t} = \left( y_{t-(w-1)/2} + \ldots + y_{t-1} + y_{t} + y_{t+1} + \ldots  + y_{t+(w-1)/2} \right)/w
\end{equation}

\textbf{The width of the window $w$ is chosen equal to the period of the time series}. However: in engineering and science this period (and not the frequency!!!) is usually not known. And idealy $w$ should be uneven, which is not always the case! The \textit{filter}-function is useful. \sidenote{If the window $w$ is even, the function description says: 'more of the filter is forward in time than backward". This means that for $w=5$ the filter will give ''NA" for the first two calculations. For the third calculation it will use values 1, 2, ... 5, which are two values backward (1 and 2), two values forward (4 and 5) and the value at the data point itself (3). For $w=6$ it will give ''NA" for the first two calculations. For the third calculation it will use values 1, 2, ... 6, which are two values backward (1 and 2), three values forward (4, 5 and 6) and the value at the data point itself (3).} The function $f_{w}$ is a weighing function. For a uniform distribution $f_{w}=\frac{1}{w}$ and the result will be the average of the $w$ data points. Experimenting with the value of $w$ can get us close to eliminating the seasonal effect.

<<label=centeredsmooth, fig=TRUE, include=FALSE, echo=FALSE>>=
w <- 12
fw <- rep(1/w, w)
p1 <- ggplot(data=Amtrak.data) +
      geom_line(aes(x = t, y = Ridership), size = 0.5, color ="red") +
      geom_line(aes(x = t, y = stats::filter(Ridership, fw, sides=2)), color= "blue", size = 1) +
      labs(title="Amtrak Ridership data\ncentered smooth") +
      JT.theme
w1 <- 10
fw1 <- rep(1/w1, w1)
w2 <- 50
fw2 <- rep(1/w2, w2)
w3 <- 100
fw3 <- rep(1/w3, w3)
p2 <- ggplot(data=construct) +
      geom_line(aes(x = t, y = addcon), size = 0.5, color ="red") +
      geom_line(aes(x = t, y = stats::filter(addcon, fw1, sides=2)), color= "lightblue", alpha = 0.5, size = 1) +
      annotate("text", x= 10, y = 75, label = "w=10", color = "lightblue", hjust = 0) +
      geom_line(aes(x = t, y = stats::filter(addcon, fw2, sides=2)), color= "blue", alpha = 0.5, size = 1) +
      annotate("text", x= 10, y = 65, label = "w=50", color = "blue", hjust = 0) +
      geom_line(aes(x = t, y = stats::filter(addcon, fw3, sides=2)), color= "darkblue", alpha = 0.5, size = 1) +
      annotate("text", x= 10, y = 55, label = "w=100", color = "darkblue", hjust = 0) +
      labs(title="addcon\ncentered smooth") +
      JT.theme
p3 <- ggplot(data=construct) +
      geom_line(aes(x = t, y = combicon), size = 0.5, color ="red") +
      geom_line(aes(x = t, y = stats::filter(combicon, fw1, sides=2)), color= "lightblue", alpha = 0.5, size = 1) +
      annotate("text", x= 10, y = 75, label = "w=10", color = "lightblue", hjust = 0) +
      geom_line(aes(x = t, y = stats::filter(combicon, fw2, sides=2)), color= "blue", alpha = 0.5, size = 1) +
      annotate("text", x= 10, y = 65, label = "w=50", color = "blue", hjust = 0) +
      geom_line(aes(x = t, y = stats::filter(combicon, fw3, sides=2)), color= "darkblue", alpha = 0.5, size = 1) +
      annotate("text", x= 10, y = 55, label = "w=100", color = "darkblue", hjust = 0) +
      labs(title="combicon\ncentered smooth") +
      JT.theme
grid.arrange(p1, p2, p3, nrow=1)
@

\begin{figure*}
\includegraphics[width=0.85\textwidth]{PTSFR-centeredsmooth}
\caption{Centered moving average}
\label{fig:centeredsmooth}
\setfloatalignment{b}
\end{figure*}

The graphs obtained by Centered Moving Average (Figure~\ref{fig:centeredsmooth}) give a good idea of the trend. However they lose information at the start, but more crucially for forecasting, also at the end.

\subsection{Trailing Moving Average}
When wanting to forecast we cannot use ''future" values, because they to not exist. In that case we use \emph{Trailing Moving Average}\index(moving average!trailing):
\begin{equation}
	F_{t+k} = (y_{t} + y_{t-1} + \ldots + y_{t-w+1})/w
\end{equation}

In the \textit{filter}-function we only have to set the parameter \emph{sides} equal to 1.

<<label=trailingsmooth, fig=TRUE, include=FALSE, echo=FALSE>>=
w <- 12
fw <- rep(1/w, w)
p1 <- ggplot(data=Amtrak.data) +
      geom_line(aes(x = t, y = Ridership), size = 0.5, color ="red") +
      geom_line(aes(x = t, y = stats::filter(Ridership, fw, sides=1)), color= "blue", size = 1) +
      labs(title="Amtrak Ridership data\ntrailing smooth") +
      JT.theme
w1 <- 10
fw1 <- rep(1/w1, w1)
w2 <- 50
fw2 <- rep(1/w2, w2)
w3 <- 100
fw3 <- rep(1/w3, w3)
p2 <- ggplot(data=construct) +
      geom_line(aes(x = t, y = addcon), size = 0.5, color ="red") +
      geom_line(aes(x = t, y = stats::filter(addcon, fw1, sides=1)), color= "lightblue", alpha = 0.5, size = 1) +
      annotate("text", x= 10, y = 75, label = "w=10", color = "lightblue", hjust = 0) +
      geom_line(aes(x = t, y = stats::filter(addcon, fw2, sides=1)), color= "blue", alpha = 0.5, size = 1) +
      annotate("text", x= 10, y = 65, label = "w=50", color = "blue", hjust = 0) +
      geom_line(aes(x = t, y = stats::filter(addcon, fw3, sides=1)), color= "darkblue", alpha = 0.5, size = 1) +
      annotate("text", x= 10, y = 55, label = "w=100", color = "darkblue", hjust = 0) +
      labs(title="addcon\ntrailing smooth") +
      JT.theme
p3 <- ggplot(data=construct) +
      geom_line(aes(x = t, y = combicon), size = 0.5, color ="red") +
      geom_line(aes(x = t, y = stats::filter(combicon, fw1, sides=1)), color= "lightblue", alpha = 0.5, size = 1) +
      annotate("text", x= 10, y = 75, label = "w=10", color = "lightblue", hjust = 0) +
      geom_line(aes(x = t, y = stats::filter(combicon, fw2, sides=1)), color= "blue", alpha = 0.5, size = 1) +
      annotate("text", x= 10, y = 65, label = "w=50", color = "blue", hjust = 0) +
      geom_line(aes(x = t, y = stats::filter(combicon, fw3, sides=1)), color= "darkblue", alpha = 0.5, size = 1) +
      annotate("text", x= 10, y = 55, label = "w=100", color = "darkblue", hjust = 0) +
      labs(title="combicon\ntrailing smooth") +
      JT.theme
grid.arrange(p1, p2, p3, nrow=1)
@

\begin{figure*}
\includegraphics[width=1\textwidth]{PTSFR-trailingsmooth}
\caption{}
\label{fig:trailingsmooth}
\setfloatalignment{b}
\end{figure*}

The accuracy of a naive forecast of the Amtrak ridership data based on the smoothed data in the training set is given by
<<>>=
accuracy(naive(stats::filter(train.ts, fw, sides=1), h = nValid), valid.ts)
@

<<label=naivetrailingsmooth, fig=TRUE, include=FALSE, echo=FALSE>>=
nValid <- 36
nTrain <- length(ridership.ts) - nValid
train.ts <- window(ridership.ts, 
                   start = c(1991, 1), 
                   end = c(1991, nTrain))
valid.ts <- window(ridership.ts, 
                   start = c(1991, nTrain + 1), 
                   end = c(1991, nTrain + nValid))
w <- 12
fw <- rep(1/w, w)
ridership.trailing <- stats::filter(train.ts, fw, sides=1)
naive.pred.trailing <- naive(ridership.trailing, h = nValid)
plot(train.ts, 
     ylim = c(1300, 2200), 
     ylab = "Ridership", 
     xlab = "Time", 
     bty = "l",
     xaxt = "n",
     xlim = c(1991, 2006.25),
     main = "",
     lty = 1)
axis(1, at = seq(1991, 2006, 1), labels = format(seq(1991, 2006,1)))
lines(ridership.trailing, col = "red", lty = 2, lwd = 2)
lines(valid.ts, lty = 1)
lines(naive.pred.trailing$mean, col = "red", lty = 2, lwd = 2)
abline(v = 1991 , col="blue")
abline(v = 1991 + (nTrain)/12 , col="blue")
abline(v = 1991 + (nTrain + nValid)/12, col="blue")
text(x = 1996, y = 2190, labels="Training")
arrows(x0 = 1991, y0 = 2170, 
       x1 = 1991 + nTrain/12, y1 = 2170, 
       code = 3, length = 0.1)
text(x = 2002.6, y = 2190, labels="Validation")
arrows(x0 = 1991 + (nTrain)/12, y0 = 2170, 
       x1 = 1991 + (nTrain + nValid)/12, y1 = 2170, 
       code = 3, length = 0.1)
text(x = 2005.5, y = 2190, labels="Future")
arrows(x0 = 1991 + (nTrain + nValid + 1)/12, y0 = 2170, 
       x1 = 1991 + (nTrain + nValid + 36)/12, y1 = 2170, 
       code = 2, length = 0.1)
@

\begin{figure}
\includegraphics[width=1\textwidth]{PTSFR-naivetrailingsmooth}
\caption{Amtrak Ridership with naive forecast based on trailing moving average}
\label{fig:naivetrailingsmooth}
\setfloatalignment{b}
\end{figure}

From Figure~\ref{fig:naivetrailingsmooth} we can see that this model does not predict the actual values very well in the validation period. The accuracy of this naive forecast of the Amtrak ridership data based on the smoothed data in the training set is given by
<<>>=
accuracy(naive(stats::filter(train.ts, fw, sides=1), h = nValid), valid.ts)
@

\subsection{Choosing window width w}

The choice of the window width $w$ is a balancing act between \emph{under-smoothing}\index{smoothing!under-smoothing} and \emph{over-smoothing}\index{smoothing!over-smoothing}. Narrow windows will show local trends, broader windows will show the general trend. Domain knowledge should give some indication.

\subsection{What does averaging do to trend, seasonal and error components?}

\newthought{Trend}\index{averaging!effect on trend}

The average of a linear trend, is the trend itself. Centered averaging will give you the same line, trailing averaging results in a time shift.

<<label=influenceaverageontrend, fig=TRUE, include=FALSE, echo=FALSE>>=
inf.trend <- data.frame(t = seq(0, 100, by = 1), trend = 0, smoothcent = 0, smoothtrail = 0)
w <- 10
fw <- rep(1/w, w)
inf.trend$trend <- 1 + 5*inf.trend$t
inf.trend$smoothcent <- stats::filter(inf.trend$trend, fw, sides = 2)
inf.trend$smoothtrail <- stats::filter(inf.trend$trend, fw, sides = 1)
ggplot(data=inf.trend, aes(x=t)) +
  geom_line(aes(y = inf.trend$trend)) +
  geom_line(aes(y = inf.trend$smoothcent), color = "red", lty = 2) +
  geom_line(aes(y = inf.trend$smoothtrail), color = "blue", lty = 2) +
  JT.theme
@

\begin{marginfigure}
\includegraphics[width=1\textwidth]{PTSFR-influenceaverageontrend}
\caption{Averaging a linear trend. Centered (no influence), Trailing (time shift)}
\label{fig:influenceaverageontrend}
\setfloatalignment{b}
\end{marginfigure}

The average of a polynomial trend follows the polynomial trend, with an extra constant (Figure~\ref{fig:influenceaverageontrendpoly}). Centered averaging will give you approximatively the same line, trailing averaging results in a time shift equal to $w/2$.

If
\begin{equation}
	y = b_{0} + b_{1}t + b_{2}t^{2}
\end{equation}

then
\begin{equation}
	\bar{y} = \frac{1}{w} \int_{t-\frac{w}{2}}^{t + \frac{w}{2}} (b_{0} + b_{1}t + b_{2}t^2) =b_{0} + b_{1}t + b_{2}t^{2} + \frac{b_{2}w^{2}}{12} = y(t) + const
\end{equation}

<<label=influenceaverageontrendpoly, fig=TRUE, include=FALSE, echo=FALSE>>=
inf.trend <- data.frame(t = seq(0, 100, by = 1), trend = 0, smoothcent = 0, smoothtrail = 0)
w <- 10
fw <- rep(1/w, w)
inf.trend$trend <- 1 + 5*inf.trend$t + 0.1*inf.trend$t^2
inf.trend$smoothcent <- stats::filter(inf.trend$trend, fw, sides = 2)
inf.trend$smoothtrail <- stats::filter(inf.trend$trend, fw, sides = 1)
ggplot(data=inf.trend, aes(x=t)) +
  geom_line(aes(y = inf.trend$trend)) +
  geom_line(aes(y = inf.trend$smoothcent), color = "red", lty = 2) +
  geom_line(aes(y = inf.trend$smoothtrail), color = "blue", lty = 2) +
  JT.theme
@

\begin{marginfigure}
\includegraphics[width=1\textwidth]{PTSFR-influenceaverageontrendpoly}
\caption{Averaging a polynomial trend. Centered (no influence), Trailing (time shift)}
\label{fig:influenceaverageontrendpoly}
\setfloatalignment{b}
\end{marginfigure}

\newthought{Seasonal}\index{averaging!effect on seasonal component}

The moving average of a seasonal component (if it is a pure sine), is a sine with the same frequency but a lower amplitude. If the width equals the period of the sine, the amplitude is zero. Centered averaging will give a sine in phase, trailing average will give a phase shift. 

<<label=influenceaverageonseason, fig=TRUE, include=FALSE, echo=FALSE>>=
inf.season <- data.frame(t = seq(0, 100, by = 1), sine = 0, smoothcent = 0, smoothtrail = 0)
T <- 25
w <- 7
fw <- rep(1/w, w)
inf.season$sine <- sin(2*pi*inf.season$t/T)
inf.season$smoothcent <- stats::filter(inf.season$sine, fw, sides = 2)
inf.season$smoothtrail <- stats::filter(inf.season$sine, fw, sides = 1)
ggplot(data=inf.season, aes(x=t)) +
  geom_line(aes(y = inf.season$sine)) +
  geom_line(aes(y = inf.season$smoothcent), color = "red", lty = 2) +
  geom_line(aes(y = inf.season$smoothtrail), color = "blue", lty = 2) +
  JT.theme
@

\begin{marginfigure}
\includegraphics[width=1\textwidth]{PTSFR-influenceaverageonseason}
\caption{Averaging a seasonal component. Centered (no influence), Trailing (time shift)}
\label{fig:influenceaverageonseason}
\setfloatalignment{b}
\end{marginfigure}

\newthought{The error term}\index{averaging!effect on error}

If the error (noise) can be considered as coming from a normal distribution with mean=0 and a variance =$\sigma^{2}$, then averaging of width $w$ will lead to a new error distribution which will be normal, with mean=0 but with a reduced variance=$var=\frac{\sigma^{2}}{w}$.
\newpage
\section{Differencing}

Removing a trend and/or a seasonal pattern can be done by \emph{differencing}\index(differencing)\index{trend!removing of}\index{seasonality!removing of}. A \emph{lag-1} difference\index{lag}\index{differencing!lag} takes the difference between every two consecutive values in the series. In general, a \emph{lag-k} difference substracts the value from k time units back:
\begin{equation}
	lag_{k}(t) = y_{t} - y_{t-k}
\end{equation}

In R the \textit{diff}-function provides the differenced series.

\subsection{Removing a trend}\index{trend:removing}

A \emph{lag-1} difference can remove a trend. While the ridership time series (upper section in Figure~\ref{fig:Amtraklag1}) shows a U-shaped trend, no trend is visible in the lag1-series. For highter order trends, another round of lag-1 differencing might by necessary. In this case it seems that one lag-1 difference is sufficient to remove the trend. Look at the difference in the scale of the y-axis! The differenced time series should hover around the horizontal line when the trend is removed. In the detrended time series the seasonal component should now be detectable.

\newthought{linear trend}\index{differencing!linear trend}

If
\begin{equation}
	y_{t} = b_{0} + b_{1}t
\end{equation}

then
\begin{equation}
	lagk_{t} = y_{t} - y_{t-k}=\left( b_{0} + b_{1}t \right) - \left( b_{0} + b_{1}(t-k) \right) = b_{1}k
\end{equation}

The linear trend is removed and we only have a level equal to $b_{1}k$. For $k=1$ this reduces to $lag1_{t}=b_{1}$.

\newthought{quadratic trend}\index{differencing!quadratic trend}

If
\begin{equation}
	y_{t} = b_{0} + b_{1}t + b_{2}t^{2}
\end{equation}

then
\begin{equation}
	lagk_{t} = y_{t} - y_{t-1}=\left( b_{0} + b_{1}t + b_{2}t^{2}\right) - \left( b_{0} + b_{1}(t-k) + b_{2}(t-k)^{2} \right) = (b_{1} - b_{2}k^{2}) + 2b_{2}kt
\end{equation}

The quadratic trend is reduced to a linear trend. A second lag-1 differencing will reduce this to a constant. For $k=1$ this reduces to $lag1_{t}=(b_{1} - b_{2}) + 2b_{2}t$.

\newthought{polynomial trend}\index{differencing!polynomial trend}

If
\begin{equation}
	y_{t} = p(t)
\end{equation}

A lag-1 differencing will reduce the degree of the polynomial trend by 1.

<<label=Amtraklag1, fig=TRUE, include=FALSE, echo=FALSE>>=
p1 <- autoplot(ridership.ts) + 
  labs(title="ridership.ts") +
  xlab("time") +
  ylab("ridership") +
  JT.theme
p2 <- autoplot(diff(ridership.ts, lag = 1)) + 
  labs(title="ridership.lag1.ts") +
  xlab("time") +
  ylab("ridership.lag1.ts") +
  JT.theme
p3 <- autoplot(diff(diff(ridership.ts, lag = 1),1)) + 
  labs(title="ridership.double.lag1.ts") +
  xlab("time") +
  ylab("ridership.double.lag1.ts") +
  JT.theme
grid.arrange(p1, p2, p3, nrow=3)
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-Amtraklag1}
\caption{Amtrak Ridership original series and lag1-series}
\label{fig:Amtraklag1}
\setfloatalignment{b}
\end{figure}

Let's do this for the addcon- and combicon-series (Figure~\ref{fig:Constructlag1}). In both cases the trend is removed after a lag-1 differencing.

<<label=Constructlag1, fig=TRUE, include=FALSE, echo=FALSE>>=
addcon.ts <- ts(construct$addcon, start = 1)
combicon.ts <- ts(construct$combicon, start = 1)
p1 <- autoplot(addcon.ts) + 
  labs(title="addcon.ts") +
  xlab("time") +
  ylab("addcon") +
  JT.theme
p2 <- autoplot(combicon.ts) + 
  labs(title="combicon.ts") +
  xlab("time") +
  ylab("combicon") +
  JT.theme
p3 <- autoplot(diff(addcon.ts, lag = 1)) + 
  labs(title="addcon.lag1.ts") +
  xlab("time") +
  ylab("addcon.lag1.ts") +
  JT.theme
p4 <- autoplot(diff(combicon.ts, lag = 1)) + 
  labs(title="combicon.lag1.ts") +
  xlab("time") +
  ylab("combicon.lag1.ts") +
  JT.theme
grid.arrange(p1, p2, p3, p4,  nrow=2)
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-Constructlag1}
\caption{addcon and combicon series: original and lag1}
\label{fig:Constructlag1}
\setfloatalignment{b}
\end{figure}

\subsection{Removing Seasonality}\index{seasonality!removing}\index{deseasonalizing}\index{seasonal adjustment}

\newthought{Seasonality as a sine-function}\index{differencing!seasonality!sine function}

If
\begin{equation}
	y_{t} = \hat{a}sin(\frac{2\pi t}{T} + \alpha)
\end{equation}

then

\begin{equation}
	lag_k(t)=y_{t}-y_{t-k}=\hat{a}sin(\frac{2\pi t}{T} + \alpha) - \hat{a}sin(\frac{2\pi (t-k)}{T} + \alpha) = \hat{a}sin(\frac{2\pi t}{T} + \alpha) - \hat{a}sin(\frac{2\pi t}{T} + \alpha - \frac{2\pi k}{T})
	\label{eq:seasonlag}
\end{equation}

The difference between two sine-functions with the same period T (and thus the same frequency f) is again a sine function with the same period T. \emph{Differencing} seasonal elements within the signal will therefore not change the frequency of the intial seasonal component. It will change the amplitude because the phase-angle of the second term in equation \ref{eq:seasonlag} has changed by $\frac{2\pi k}{T}$. In the special case where $k=T$ the change in phase-angle will be $2\pi$ and the difference between the two sine-functions will be zero. This means that choosing the lag equal to the period of a seasonal component will make this component disappear: a \emph{lag-T}-differencing will completely remove (a true sine) seasonality of the same period.

Do \emph{lag-T}-differencing, where $T$ is the period (or suspected period) of the season. For the Amtrak time series $T=12$. 

<<label=Amtraklag12, fig=TRUE, include=FALSE, echo=FALSE>>=
p1 <- autoplot(ridership.ts) + 
  labs(title="ridership.ts") +
  xlab("time") +
  ylab("ridership") +
  JT.theme
p2 <- autoplot(diff(ridership.ts, lag = 12)) + 
  labs(title="ridership.lag12.ts") +
  xlab("time") +
  ylab("ridership.lag12.ts") +
  JT.theme
grid.arrange(p1, p2, nrow=2)
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-Amtraklag12}
\caption{Amtrak Ridership original series and lag12-series}
\label{fig:Amtraklag12}
\setfloatalignment{b}
\end{figure}

Figure~\ref{fig:Amtraklag12}: the monthly pattern has gone, but there is a hint of a 6 to 8 month pattern.

For the addcon time series we have guessed before that it is around $T=1000/9=111$.

<<label=Constructlag111, fig=TRUE, include=FALSE, echo=FALSE>>=
p1 <- autoplot(addcon.ts) + 
  labs(title="addcon.ts") +
  xlab("time") +
  ylab("addcon") +
  JT.theme
p2 <- autoplot(combicon.ts) + 
  labs(title="combicon.ts") +
  xlab("time") +
  ylab("combicon") +
  JT.theme
p3 <- autoplot(diff(addcon.ts, lag = 111)) + 
  labs(title="addcon.lag111.ts") +
  xlab("time") +
  ylab("addcon.lag111.ts") +
  JT.theme
p4 <- autoplot(diff(combicon.ts, lag = 111)) + 
  labs(title="combicon.lag111.ts") +
  xlab("time") +
  ylab("combicon.lag111.ts") +
  JT.theme
grid.arrange(p1, p2, p3, p4,  nrow=2)
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-Constructlag111}
\caption{addcon and combicon series: original and lag111}
\label{fig:Constructlag111}
\setfloatalignment{b}
\end{figure}

Figure~\ref{fig:Constructlag111}: the most obvious pattern has gone, but some trend has remained and there is a sign of a shorter periodicity which we can determine by doing a \textit{ggplotly} on graphs p3 and p4. Some measurements on these plots suggest a shorter period of about 27. Doing a second differencing with the shorter period give Figure~\ref{fig:Constructdoublelag}.

<<label=Constructdoublelag, fig=TRUE, include=FALSE, echo=FALSE>>=
p1 <- autoplot(addcon.ts) + 
  labs(title="addcon.ts") +
  xlab("time") +
  ylab("addcon") +
  JT.theme
p2 <- autoplot(combicon.ts) + 
  labs(title="combicon.ts") +
  xlab("time") +
  ylab("combicon") +
  JT.theme
p3 <- autoplot(diff(diff(addcon.ts, lag = 111),27)) + 
  labs(title="addcon.lag111/27.ts") +
  xlab("time") +
  ylab("addcon.lag111/27.ts") +
  JT.theme
p4 <- autoplot(diff(diff(combicon.ts, lag = 111),27)) + 
  labs(title="combicon.lag111/27.ts") +
  xlab("time") +
  ylab("combicon.lag111/27.ts") +
  JT.theme
grid.arrange(p1, p2, p3, p4,  nrow=2)
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-Constructdoublelag}
\caption{addcon and combicon series: double lag: first T=111, then T=27}
\label{fig:Constructdoublelag}
\setfloatalignment{b}
\end{figure}

\subsection{Removing trend and seasonality}
Do a double lag: first for seasonality (lag-T) and on the lagged time series a (lag-1).

<<label=Amtrakdouble, fig=TRUE, include=FALSE, echo=FALSE>>=
p1 <- autoplot(ridership.ts) + 
  labs(title="ridership.ts") +
  xlab("time") +
  ylab("ridership") +
  JT.theme
p2 <- autoplot(diff(diff(ridership.ts, lag = 12),1)) + 
  labs(title="ridership.lag12/1.ts") +
  xlab("time") +
  ylab("ridership.lag12/1.ts") +
  JT.theme
grid.arrange(p1, p2, nrow=2)
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-Amtrakdouble}
\caption{Amtrak Ridership original series and lag12-lag1 series}
\label{fig:Amtrakdouble}
\setfloatalignment{b}
\end{figure}

<<label=Constructdouble, fig=TRUE, include=FALSE, echo=FALSE>>=
p1 <- autoplot(addcon.ts) + 
  labs(title="addcon.ts") +
  xlab("time") +
  ylab("addcon") +
  JT.theme
p2 <- autoplot(combicon.ts) + 
  labs(title="combicon.ts") +
  xlab("time") +
  ylab("combicon") +
  JT.theme
p3 <- autoplot(diff(diff(addcon.ts, lag = 111),1)) + 
  labs(title="addcon.lag111/1.ts") +
  xlab("time") +
  ylab("addcon.lag111/1.ts") +
  JT.theme
p4 <- autoplot(diff(diff(combicon.ts, lag = 111),1)) + 
  labs(title="combicon.lag111/1.ts") +
  xlab("time") +
  ylab("combicon.lag111/1.ts") +
  JT.theme
grid.arrange(p1, p2, p3, p4,  nrow=2)
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-Constructdouble}
\caption{addcon and combicon series: double lag: first T=111, then T=1}
\label{fig:Constructdouble}
\setfloatalignment{b}
\end{figure}

\subsection{Effect of differencing on the error term}\index{differencing!effect on error}

If the error (noise) can be considered as coming from a normal distribution with mean=0 and a variance =$\sigma^{2}$, then a differencing operation (of any lag) will lead to a new error distribution which will be normal, with mean=0 but with variance=$var=\sigma^{2} + \sigma^{2}=2\sigma^{2}$. Further lag-differencing operations will steadily increase the variance of the error term.

\section{Simple exponential smoothing}\index{simple exponential smoothing}\index{SEM}\index{smoothing!simple exponential}

It is a trailing moving average method where we take into account \emph{all} previous values, but they are not weighted uniformly: the weighing factor decreases exponentially. This give more weight to recent information. The sum of the weighing factors must of course be equal to 1. The choice for the weighing factor is $\alpha (1 - \alpha)^{i}$, because
\begin{equation}
	\sum_{i=0}^{i=+\infty} \alpha (1 -\alpha)^{i} = 1 \quad for \quad 0 \leq \alpha \leq 1
\label{eq:smoothingconstant}
\end{equation}

This generates a forecast at $t=t+1$:
\begin{equation}
	F_{t+1}= \alpha y_{t} + \alpha (1-\alpha) y_{t-1} + \ldots + \alpha (1-\alpha)^{i}y_{t-i} + \ldots
\label{eq:forecastexpsmooth}
\end{equation}

Because
\begin{equation}
	F_{t}= \alpha y_{t-1} + \alpha (1-\alpha) y_{t-2} + \ldots + \alpha (1-\alpha)^{i}y_{t-i-1} + \ldots
\end{equation}

it follows that:
\begin{equation}
	F_{t+1}  = \alpha y_{t} + (1-\alpha) F_{t} = F_{t} + \alpha (y_{t} - F_{t}) = F_{t} + \alpha e_{t}
\end{equation}

This formulation shows that \emph{simple exponential smoothing} is a \emph{learning} process: it takes the former forecast and adjusts it based on the difference with the actual value. If the former forecast was too high ($e_{t} <0$) then it will adjust the next forecast downward. If the former was too low ($e_{t}>0$) it will adjust upwards. The ''learning" is exemplified by the \emph{smoothing constant}\index{smoothing constant}\index{smoothing!constant} $\alpha$.

From the formula we can see that \emph{simple exponential smoothing} will only generate one prediction for the future. The second prediction cannot be calculated as we do not know the acutal value of $y_{t+1}$. Usually this means that for forecasts from $F_{t+2}$ and higher we keep the forecast constant and equal to $F_{t+1}$. This is a small improvement on a naive forecast, but not much. As such, \emph{simple exponential smoothing} is not a usefull forecasting method for time series that have trend and/or seasonality.

\subsection{Choosing the smoothing constant $\alpha$}\index{smoothing constant}\index{simple exponential smoothing!smoothing constant}

The value of the smoothing constant has to be between 0 and 1 (included) (see equation~\ref{eq:smoothingconstant}). $\alpha=1$ results in $F_{t+1}=y_{t}$, which is the naive forecast. $\alpha$ values close to 0 will take into account a lot of the former data values. A rule of thumb for $\alpha$ is the range $[0.1 - 0.2]$, but we can optimize $\alpha$ by looking at the predictive quality metrics (MAPE, RMSE ...) of the validation period for different choices of $\alpha$.

\newthought{Example: Amtrak data, twice differenced}

The example given in the book is the time series that we get from the Amtrak time series after double differencing: a lag12 differencing to remove the seasonality, and on the result a lag1 differencing to remove the trend. The result of these operations should be the error, but not the original noise on the signal but an ''enhanced" version because the differencing operations each time double the variance. The goal is, according to the book, to work with a time series that has no trend and no seasonality. I think that means a random signal with average 0. Forecasting such a signal seems to be easy: it is zero for the whole validation period and for the future. I see no reason to make this very complicated. Working with the assumption that this is a ''deformed" error signal with 4 times (due to two differencing operations) the variance of the original signal, we could even say more: the forecast for the error signal is a random value taken from a normal distribution with mean=0 and standard deviation equal to $\frac{\sqrt{var signal}}{2}$. From this we could calculate a 95\% confidence interval.

However, the book uses the \textit{ets}-function from the \textbf{forecast}-package. The letters ''ets" stand for: \emph{error}, \emph{trend} and \emph{seasonality}. The simple exponential smoothing model is called using the letter combination ''ANN" where ''A" stands for \emph{additive error}, ''N" for \emph{no trend} en the second ''N" for \emph{no seasonality}. The results of the model with $\alpha=0.2$ are given in Figure~\ref{fig:Amtraksimpleexpo} (the blue line). You can let the \textit{ets}-function determine the optimum value of $\alpha$ by calling the \textit{ets}-function without specifying the value of $\alpha$ (the red line).

<<label=Amtraksimpleexpo, fig=TRUE, include=FALSE, echo=FALSE>>=
diff.twice.ts <- diff(diff(ridership.ts, lag = 12), lag = 1)
train.ts <- window(diff.twice.ts, start = c(1992, 2), end = c(1992, nTrain + 1))
valid.ts <- window(diff.twice.ts, start = c(1992, nTrain + 2), end = c(1992, nTrain + 1 + nValid))
ses <- ets(train.ts, model = "ANN", alpha = 0.2)
ses.opt <- ets(train.ts, model = "ANN")
ses.pred <- forecast(ses, h = nValid, level = 0) # level=0 means no confidence interval calculation
ses.opt.pred <- forecast(ses.opt, h = nValid, level = 0) # level=0 means no confidence interval calculation
plot(ses.pred,
     ylim = c(-250, 300),
     ylab = "Ridership (Twice-Differenced)",
     xlab = "Time",
     bty = "l",
     xaxt = "n",
     xlim = c(1991, 2006.25),
     main = "",
     lty = 2)
axis(1 , at = seq(1991, 2006, 1), labels = format(seq(1991, 2006, 1)))
lines(ses.pred$fitted, lwd = 2, col = "blue")
lines(ses.opt.pred$fitted, lwd = 2, col = "red")
lines(ses.opt.pred$mean, lwd = 2, col = "red")
lines(valid.ts)
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-Amtraksimpleexpo}
\caption{Simple exponential smoothing on Amtrak data twice differenced lag12 and lag1}
\label{fig:Amtraksimpleexpo}
\setfloatalignment{b}
\end{figure}

The model forecasts a constant value equal to \Sexpr{round(tail(ses.pred$fitted, n = 1),3)} when we set $\alpha=0.2$. For the optimum model the constant is \Sexpr{round(tail(ses.opt.pred$fitted, n = 1),5)}. When we look at the details of the optimal model, we find that the optimum value of $\alpha=$ \Sexpr{round(ses.opt$par[1],4)}. Because every term in equation~\ref{eq:forecastexpsmooth} is multiplied with $\alpha$ this comes down to a value of about zero.

\newthought{Example: addcon data, twice differenced T=111 and T=1}

<<label=addconsimpleexpo, fig=TRUE, include=FALSE, echo=FALSE>>=
diff.twice.ts <- diff(diff(addcon.ts, lag = 111), lag = 1)
nValid <- 100
nTrain <- length(addcon.ts) - nValid
train.ts <- window(diff.twice.ts, start = 1, end = nTrain)
valid.ts <- window(diff.twice.ts, start = nTrain + 1, end = nTrain + nValid)
ses <- ets(train.ts, model = "ANN", alpha = 0.2)
ses.opt <- ets(train.ts, model = "ANN")
ses.pred <- forecast(ses, h = nValid, level = 0) # level=0 means no confidence interval calculation
ses.opt.pred <- forecast(ses.opt, h = nValid, level = 0) # level=0 means no confidence interval calculation
plot(ses.pred,
     ylim = c(-25, 25),
     ylab = "addcon (Twice-Differenced)",
     xlab = "t",
     bty = "l",
     xaxt = "n",
     xlim = c(0, 1000),
     main = "",
     lty = 2)
axis(1 , at = seq(0, 1000, 100))
lines(ses.pred$fitted, lwd = 2, col = "blue")
lines(ses.opt.pred$fitted, lwd = 2, col = "red")
lines(ses.opt.pred$mean, lwd = 2, col = "red")
lines(valid.ts)
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-addconsimpleexpo}
\caption{Simple exponential smoothing on addcon data twice differenced lag111 and lag1}
\label{fig:addconsimpleexpo}
\setfloatalignment{b}
\end{figure}

The model forecasts a constant value equal to \Sexpr{round(tail(ses.pred$fitted, n = 1),3)} when we set $\alpha=0.2$. For the optimum model the constant is \Sexpr{round(tail(ses.opt.pred$fitted, n = 1),5)}. When we look at the details of the optimal model, we find that the optimum value of $\alpha=$ \Sexpr{round(ses.opt$par[1],4)}. Because every term in equation~\ref{eq:forecastexpsmooth} is multiplied with $\alpha$ this comes down to a value of about zero.

What I expected was that this represents the error term in the construction of the addcon time-series. The error was created by choosing random from a N(0,$\sigma=3$) distribution. However, as noted, the differencing will increase the variance. Twice differencing will create a variance that is 4 times the initial variance. Our time-series in the training set has the following characteristics:

<<>>=
mean(train.ts)
var(train.ts)
sqrt(var(train.ts))/2 - sigma
# looks good
@

Is addcon twice differenced normally distributed?

<<label=histaddcon2diff, fig=TRUE, include=FALSE, echo=FALSE>>=
ggplot(data = as.data.frame(diff.twice.ts)) + 
  geom_histogram(aes(x), bins = 50) +
  labs(title="histogram of addcon twice differenced") +
  xlab("time") +
  ylab("addcon lag111 and lag1") +
  JT.theme
@

\begin{figure}
\includegraphics[width=0.6\textwidth]{PTSFR-histaddcon2diff}
\caption{Histogram of addcon data twice differenced lag111 and lag1}
\label{fig:histaddcon2diff}
\setfloatalignment{b}
\end{figure}

\newthought{!!! Important point !!!}

My irritation with the seemingly pointless exercise of modelling a time series without a trend or seasonality is the consequence of ''automatice engineering thinking". The automatic assumption is that there is an underlying trend which can be found as a polynomial (or other function) model. Seasonality can be reduced to finding the Fourier components of the periodic element within the time series. The rest is noise: an error component presumed to be modelled by a normal distribution N(0,$\sigma$). This is clearly a \emph{model based} forecasting method. The forecasting element resides in extrapolating the trend (=assuming that the function holds outside the data range), assuming that the seasonality holds, and that we can estimate the error.

Methods like \emph{simple exponential smoothing} and more sofisticated versions (see \ref{sec:advanced exponential smoothing} \emph{advanced exponential smooting}) are \emph{data driven} methods. For example: in the modelling of a time series without trend or seasonality, it does not assume that the error is N(0,$\sigma$)-distributed. It prefers agnosticism and bases itself on the data. The \emph{simple exponential smoothing} ''learns" from all previous data points. The \emph{simple exponential smoothing} will only give you one forecast $F_{t+1}$ based on all available data points $y_{i} \quad for \quad i=1 \ldots t$.  

\subsection{Relation between $w$ (moving average smoothing) and $\alpha$ (simple exponential smoothing)}
\begin{equation}
	w = \frac{2}{\alpha}-1
\end{equation}

\newpage
\section{Advanced Exponential Smoothing}\index{advanced exponential smoothing}\index{smoothing!advanced exponential}
\label{sec:advanced exponential smoothing}

We can eliminate trend and seasonality by differencing. But these actions also have an impact on each other and on the error component. An alternative is the use of special functions that allow for trend and/or seasonality.

\subsection{Time series with a trend: additive or multiplicative}

\newthought{Additive trend}

The main idea is that we ''rebuild" the time series learning from previous data points. This is another way of looking at \emph{trend}\index{trend}\index{trend!additive}. From a modelling perspective we can propose that the trend is given by (e.g.) a linear equation:

\begin{equation}
	y_{t} = b_{0} + b_{1}t
\end{equation}

But we could also define it by specifying \emph{locally} how the next data point can be found from the last:

\begin{eqnarray}
  y_{0} &=& b_{0}\\
  y_{t+1} &=& y_{t} + b_{1}
\end{eqnarray}

This is a \emph{local} generating formula, because it is based on the last local position and it adds to that a, local, change. The \emph{trend}\index{trend} here is interpreted as an added (positive or negative) \emph{local} term. In this case this trend is a constant ($b_{1}$), but this is not necessary. In general, when there is a trend, we can write that the next element can be found from two terms: the (local) \emph{level} $L_{t}$ and the (local) \emph{trend} $T_{t}$. For an \emph{additive trend}\index{trend!additive} the forecast one time step ahead will be:

\begin{equation}
	F_{t+1} = L_{t} + T_{t}
\end{equation}

It is also the beginning of a forecast further into the future. Assuming the trend holds we can write:

\begin{equation}
	F_{t+k} = L_{t} + k.T_{t}
	\label{eq:AESadded}
\end{equation}

The values for the local level $L_{t}$ and the local trend $T_{t}$ are calculated by using a form of \emph{simple exponential smoothing}\emph{smoothing!simple exponential}:

\begin{eqnarray}
  L_{t} &=& \alpha y_{t} + (1-\alpha)(L_{t-1} + T_{t-1}) = \alpha y_{t} + (1-\alpha)F_{t}  \\
  T_{t} &=& \beta (L_{t} - L_{t-1}) + (1-\beta)T_{t-1}
\end{eqnarray}

The smoothing constants\index{smoothing!constants} $\alpha$ and $\beta$ are constants between 0 and 1. Once we have calculated the values of $L_{t}$ and $T_{t}$ the forecast with an additive trend, given by \ref{eq:AESadded} is \emph{linear}. It is clear that the level $L_{t}$ and the trend $T_{t}$ have the same units as the time-series value $y_{t}$.

There is of course also an error term present. It can be additive
\begin{equation}
	y_{t+1} = L_{t} + T_{t} + e_{t+1} = F_{t+1}  + e_{t+1}
\end{equation}

or multiplicative
\begin{equation}
	y_{t+1} = (L_{t} + T_{t}).(1 + e_{t+1}) = F_{t+1}.(1 + e_{t+1})
\end{equation}

\newthought{Multiplicative trend}\index{trend!multiplicative}

The disadvantage of a model with \emph{added trend} is that the forecast will be linear in time. When we have the impression (from visualizing the data, or from other information) that the relation between the time-series value $y_{t}$ and time $t$ is non-linear we can incorporate this into the forecasting model by making the term we add to the level \emph{dependent} upon the present value of the time-series. If the difference between the present value and the next value depends upon the present value we can write this locally as:
\begin{equation}
	y_{t+1} - y_{t} = y_{t}.p
\end{equation}

From this it follows that
\begin{equation}
	y_{t+1} = y_{t} + y_{t}.p = y_{t}(1 + p)
\end{equation}

and, assuming that $p$ remains the same,
\begin{equation}
	y_{t+2} = y_{t+1}(1 + p) = y_{t}(1+p)^{2}
\end{equation}

and in general
\begin{equation}
	y_{t+k} = y_{t}(1+p)^{k} = y_{t}(1 + kp + \frac{1}{2}k(k-1)p^{2} + \ldots + \frac{1}{i!}k(k-1)..(k-i+1)p^{i} + \ldots)
\end{equation}

which makes $y_{t+k}$ non-linear in the time step $k$ (Figure~\ref{fig:nonlinearp}).

<<label=nonlinearp, fig=TRUE, include=FALSE, echo=FALSE>>=
nonlin <- data.frame(k = seq(0, 5, by=0.1), pos = 0, neg = 0)
ppos <- 0.8
pneg <- -0.8
nonlin$pos <- 5*(1+ppos)^(nonlin$k)
nonlin$neg <- 100*(1+pneg)^(nonlin$k)
p1 <- ggplot(data = nonlin, aes(x = k)) +
  geom_line(aes(y = pos), color= "red") +
  ggtitle("positive percentage growth") +
  ylab("p = +0.8") +
  ylim (low = 0, high = 100) +
  JT.theme
p2 <- ggplot(data = nonlin, aes(x = k)) +
  geom_line(aes(y = neg), color = "red") +
  ggtitle("negative percentage growth") +
  ylab("p = -0.8") +
  JT.theme
grid.arrange(p1, p2, nrow = 1)
@

\begin{marginfigure}
\includegraphics[width=0.85\textwidth]{PTSFR-nonlinearp}
\caption{Non-linear aspect of multiplicative model}
\label{fig:nonlinearp}
\setfloatalignment{b}
\end{marginfigure}

Our forecast for $t+k$ will now be:
\begin{equation}
	F_{t+k} = L_{t}.T_{t}^{k} \quad with \quad T_{t}=1+p
\end{equation}

From this equation it follows that the level $L_{t}$ has the same units as the time-series value $y_{t}$, but that the trend $T_{t}$ is \emph{dimensionless} and depends on the fraction $\frac{L_{t}}{L_{t-1}}$.

The values for the local level $L_{t}$ and the local trend $T_{t}$ are again calculated using a form of \emph{simple exponential smoothing}\index{simple exponential smoothing}\index{smoothing!simple exponential}. For the trend we must use the fraction $\frac{L_{t}}{L_{t-1}}$ :
\begin{eqnarray}
  L_{t} &=& \alpha y_{t} + (1-\alpha)(L_{t-1} . T_{t-1}) = \alpha y_{t} + (1-\alpha)F_{t}  \\
  T_{t} &=& \beta (\frac{L_{t}}{L_{t-1}}) + (1-\beta)T_{t-1}
\end{eqnarray}

The error component can be additive\index{error!additive}
\begin{equation}
	y_{t+1} = L_{t} . T_{t} + e_{t+1} = F_{t+1} + e_{t+1}
\end{equation}

or multiplicative\index{error!multiplicative}
\begin{equation}
	y_{t+1} = (L_{t} . T_{t}).(1 + e_{t+1}) = F_{t+1}.(1 + e_{t+1})
\end{equation}

\subsection{Time series with a trend and seasonality: Holt-Winters exponential smoothing}\index{Holt-Winter}\index{double exponential smoothing}

\newthought{Additive model}

An extra term $S$ is added to capture the seasonality. Assuming all components are additive, the forecast $k$ time steps ahead is now:
\begin{equation}
	F_{t+k} = L_{t} + k.T{t} + S_{t+k-M} \quad with \quad M=period
\end{equation}

We can see what happens in this example. Here we create a time-series by adding a linear function ($0.005t$) and a sine-function ($sin(2 \pi t/M)$).

<<label=exampletrendandseason, fig=TRUE, include=FALSE, echo=FALSE>>=
M <- 24 # lengte van de periode
n <- 8 # aantal periodes
data.df <- data.frame(t=seq(0,n*M,by=1), y = 0, L = 0, T = 0, S = 0, F= 0)
alpha <- 0.1
beta <- 0.1
gamma <- 0.9
data.df$y <- (1/(M*n))*data.df$t + sin(2*pi*data.df$t/M)
data.df$L[1] <- data.df$y[1]
for (i in seq(2, M, by = 1)) {
  data.df$L[i] <- alpha*data.df$y[i] + (1 - alpha)*(data.df$L[i-1] + data.df$T[i-1])
  data.df$T[i] <- beta*(data.df$L[i] - data.df$L[i-1]) + (1 - beta)*data.df$T[i-1]
  data.df$S[i] <- gamma*(data.df$y[i] - data.df$L[i])
  data.df$F[i] <- data.df$L[i-1] + data.df$T[i-1]
}
for (i in seq(M + 1, n*M, by = 1)) {
  data.df$L[i] <- alpha*(data.df$y[i] - data.df$S[i-M]) + (1 - alpha)*(data.df$L[i-1] + data.df$T[i-1])
  data.df$T[i] <- beta*(data.df$L[i] - data.df$L[i-1]) + (1 - beta)*data.df$T[i-1]
  data.df$S[i] <- gamma*(data.df$y[i] - data.df$L[i]) + (1 - gamma)*data.df$S[i-M]
  data.df$F[i] <- data.df$L[i-1] + data.df$T[i-1] + data.df$S[i-M]
}
ggplot(data = data.df, aes(x = t)) +
  geom_line(aes(y = y), color = "red") +
  geom_line(aes(y = L), color = "blue") +
  geom_line(aes(y = T), color = "cyan") +
  geom_line(aes(y = S), color = "green") +
  geom_line(aes(y = F), color = "black") +
  labs(title="Example Holt-Winters on linear + sine") +
  JT.theme
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-exampletrendandseason}
\caption{Evolution of components for linear + sine example}
\label{fig:exampletrendandseason}
\setfloatalignment{b}
\end{figure}

The interesting line is the trend (cyan): gradually it becomes a constant, and at first sight it is zero. Checking with the data we find that the final value is \Sexpr{round(data.df$T[nrow(data.df) - 1], 4)}. This is a small value, but it is almost equal to the slope of the linear function (0.005). And because $L_{t+1} \approx (L_{t-1} + T_{t-1})$ the level $L$ (blue) builds up to the original linear function. But we can see that the true value $y_{t}$ (red) switches regurarly above and below the blue line. The difference $y_{t} - L_{t}$ is a measure for the seasonal component. On the other hand $y_{t} - S_{t}$ is a measure for the level $L_{t}$. However: we do not know $S_{t}$. But because we know that $S_{t}$ is periodic with period $M$, we can use $S_{t-M}$ as a substitude.

This gives us the following exponential smoothing equations:
\begin{eqnarray}
  L_{t} &=& \alpha (y_{t} - S_{t-M}) + (1-\alpha)(L_{t-1} + T_{t-1})  \\
  T_{t} &=& \beta (L_{t} - L_{t-1}) + (1-\beta)T_{t-1} \\
  S_{t} &=& \gamma (y_{t} - L_{t}) + (1 - \gamma)S_{t-M} \\
  F_{t + 1} &=& L_{t} + T_{t} + S_{t + 1 -M}
\end{eqnarray}

We can see that the forecasted values $F_{t+1}$ (black) gradually get closer and closer to the true values $y_{t}$ (red). But it is clear that we need at least y-values for one complete period M, and preferably for many of them. In this example we get good convergence after 5 periods.

\newthought{Multiplicative and combined models}

The \textbf{forecast}-package has the function \textit{ets}\index{ets-function} which can handle all kinds of additive, multiplicative and combined models. The choice of the model is communicated to the function by a three letter combination ''ets". The ''e" stands for the error (additive=A, multiplicative=M), the ''t" stands for the trend (additive=A, multiplicative=M, no trend=N), the ''s" stands for the seasonal component (additive=A, multiplicative=M, no season=N). This gives us 18 possible combinations. For example: the combination AMN stands for additive error, mulitplicative trend and no seasonality. If we model the Amtrak data with a multiplicative error, an additive trend, and an additive seasonal we get:

<<label=HWAmtrakMAA, fig=TRUE, include=FALSE, echo=FALSE>>=
fixed.nValid <- 36
fixed.nTrain <- length(ridership.ts) - fixed.nValid
train.ts <- window(ridership.ts, 
                   start = c(1991, 1),
                   end = c(1991, fixed.nTrain))
valid.ts <- window(ridership.ts, 
                   start = c(1991, fixed.nTrain + 1), 
                   end = c(1991, fixed.nTrain + fixed.nValid))
hwin <- ets(train.ts, model = "MAA")
hwin.pred <- forecast(hwin, h = nValid, level = 0)
plot(hwin.pred,
     ylim = c(1300, 2600),
     ylab = "Ridership",
     xlab = "Time",
     bty = "l",
     xaxt = "n",
     xlim = c(1991, 2006.25),
     main = "",
     lty = 2)
axis(1, at = seq(1991, 2006, 1),
     labels = format(seq(1991, 2006, 1)))
lines(hwin.pred$fitted, lwd = 2, col = "green")
lines(valid.ts, col = "red", lty = 2)
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-HWAmtrakMAA}
\caption{Holt-Winters MAA model of Amtrak Ridership}
\label{fig:HWAmtrakMAA}
\setfloatalignment{b}
\end{figure}

The details of the model are given by:
<<>>=
hwin
@

It gives the values for $\alpha=0.5543$ (local smoothing), $\beta=1e-04$ (global smoothing) and $\gamma=1e-04$ (global smooting). We can also find the starting conditions for the level $L_{0} = l = 1838.8778$, for the trend $T_{0} = b = 0.7871$ and the $12 = M$ seasonal components $S_{0}=[28.3319 , -11.2337, ..., -207.0408]$. In his book ''Forecasting with exponential smoothing" Rob Hyndman explains how the initial states are calculated\sidenote[][-7cm]{... a much better procedure is based on a decomposition. This is what was recommended in my 2008 Springer book and is implemented in the HoltWinters and ets functions in R.

First fit a (2xM) moving average smoother to the first 2 or 3 years of data (HoltWinters uses 2 years, ets uses 3 years). Then subtract (for additive HW) or divide (for multiplicative HW) the smooth trend from the original data to get de-trended data. The initial seasonal values are then obtained from the averaged de-trended data. For example, the initial seasonal value for January is the average of the de-trended Januaries.Next subtract (for additive HW) or divide (for multiplicative HW) the seasonal values from the original data to get seasonally adjusted data.

Fit a linear trend to the seasonally adjusted data to get the initial level $l_{0}$ (the intercept) and the initial slope $b_{0}$.} . We can find all subsequent states of the level, trend and seasonal component by \textit{hwin\$states}. We generate here only the initial states, and the final states:

<<>>=
hwin$states[1, ]
hwin$states[nrow(hwin$states), ]
@

\newthought{addcon time-series}
When you try to apply the Holt-Winter's method to the constructed time-series addcon and combicon, you get error-messages such as ''frequency too high". It comes from forcing the time-series into a time-series object using the \textit{ts}-function. The main stumbling block seems to be the definition of the ''frequency"-parameter. You can put in a decimal number, but in further calculations it seems that the functions only accept integer frequencies, and you get error messages.


\section{Summary of exponential smoothing using the \textit{ets}-function}\index{ets-function!parameters}
Two extra choices have been added to the \emph{trend}-component: apart from additive (A), multiplicative (M) and no trend (N), we can also choose for a \emph{damped additive} (Ad) or \emph{damped multiplicative} (Md) trend. This gives us 2 choices for the error component, 5 for the trend and 3 for the seasonal component: in total 30 possible combinations.

However: some combinations are potentially numerically instable, and the \textit{ets}-function will not execute them unless we specifically force it to do so, by setting the parameter \textit{restrict = FALSE}. When doing this the results should be examined carefully!

\section{Finding the best setting: automated model selection}\index{ets: automated model selection}

With 30 possible settings of the parameters for \textit{ets} it is quite daunting to try to find the optimal choice.  We can leave it to \textit{ets} to find it for us, by setting the parameters to ''(Z, Z, Z)". You can also restrict the search by picking one or more parameters yourself. If you want to find the optimal modal with an additive error, you set the parameters to ''(A, Z, Z)". \textit{ets} uses the Akaike Information Criterion (AIC) as the metric for determining the ''best" model.

\newthought{Amtrak time series}

The optimal model for the Amtrak data can be found (without the restricted elements) from:
<<>>=
hwinbest <- ets(train.ts, model = "ZZZ")
hwinbest
@

\textit{ets} suggests that an (ANA)-model is the best with an $AIC=1615,673$. Our previous model was (MAA) with an $AIC=1617,460$. The optimal model is lightly better, but a AIC-difference of less than 2 is not considered to be relevant.

Automating the selection process is handy, but it takes away the personal input. It is still better to work with different models and try them out on the validation set.

<<>>=
accuracy(hwin.pred, valid.ts)
hwinbest.pred <- forecast(hwinbest, h = nValid, level=0)
accuracy(hwinbest.pred, valid.ts)
@

We can see that the previous (MAA)-model has a better accuracy scores for the validation set than the optimal (ANA)-model.

\section{Extensions of exponential smoothing}

\subsection{Multiple seasonal cycles}

The \textbf{forecast}-package has a function \textit{dshw} (double-seasonal Holt-Winter)\index{Holt-Winter!double-seasonal} that can handle this, \textbf{on condition that the larger period is an integer multiple of the smaller period}. This gives problems with the combination (year-week) and (year-day) because a year has 52.18 weeks, and 365.25 days. The alternatives are the \textit{tbats}-function (\textbf{forecast}-package) or the \textit{stlm}-function (\textbf{stats}-package). See the vignettes.

\newthought{addcon time series}
The \textit{ets}-function does not seem to work with the self constructed addcon time-series. However, the \textit{msts}-function (multiple seasonal time series)\index{time series!multiple seasonal} combined with the \textit{tabs}-function works well. Of course this implies that we have a good idea of the different seasonal periods. Supposing that we can determine those accurately (Fourier transform?) we will find $T_{1}=25.641$ and $T_{2}=111.111$ for the addcon time-series. Than we get the following forecast:

<<label=tbatsaddcon, fig=TRUE, include=FALSE, echo=FALSE>>=
addcon.msts <- msts(construct$addcon, seasonal.periods = c(25.641, 111.111))
addcon.fit <- tbats(addcon.msts)
plot(forecast(addcon.fit))
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-tbatsaddcon}
\caption{Multiple seasonal time series tbats model of addcon time-series}
\label{fig:tbatsaddcon}
\setfloatalignment{b}
\end{figure}

\subsection{Adaptive smoothing constants}\index{smoothing constant!adaptive}
A method where the smoothing constants $\alpha$, $\beta$ and $\gamma$ can change along the time axis.

\chapter{Regression-based models: capturing trend and seasonality}\index{model!regression based}

This model tries to find patterns (trend, seasonality) by using linear regression models (linear in the sense of the coefficients! It can be exponential, polynomial ...). These are \emph{global} methods\index{model!global} that we use based on the data in the training set. Then we use the validation set to see if the patterns are indeed global and can be extended into the future. Because that is ultimately what we will be doing when forecasting.

\section{Model with trend}\index{trend!regression model}\index{regression model!trend}

\subsection{Linear trend}
We can fit a linear model to the data using the \textit{tslm}-function. Instead of working with the actual time as the independent variable, we use a variable ''trend" which starts at 0 and increments with 1.

\newthought{Amtrak time-series}

<<label=Amtraklinreglin, fig=TRUE, include=FALSE, echo=FALSE>>=
nValid <- 36
nTrain <- length(ridership.ts) - nValid
train.ts <- window(ridership.ts, 
                   start = c(1991, 1),
                   end = c(1991, nTrain))
valid.ts <- window(ridership.ts, 
                   start = c(1991, nTrain + 1), 
                   end = c(1991, nTrain + nValid))
train.lm <- tslm(train.ts ~ trend)
train.lm.pred <- forecast(train.lm, h = nValid, level = 0)
plot(train.lm.pred,
     ylim = c(1300, 2600),
     ylab = "Ridership",
     xlab = "Time",
     bty = "l",
     xaxt = "n",
     xlim = c(1991, 2006.25),
     main = "",
     lty = 2)
axis(1, at = seq(1991, 2006, 1),
     labels = format(seq(1991, 2006, 1)))
lines(train.lm$fitted, lwd = 2, col = "green")
lines(valid.ts, col = "red", lty = 2)
abline(v = 1991 , col="blue")
abline(v = 1991 + (nTrain)/12 , col="blue")
abline(v = 1991 + (nTrain + nValid)/12, col="blue")
text(x = 1996, y = 2210, labels="Training")
arrows(x0 = 1991, y0 = 2170, 
       x1 = 1991 + nTrain/12, y1 = 2170, 
       code = 3, length = 0.1)
text(x = 2002.6, y = 2210, labels="Validation")
arrows(x0 = 1991 + (nTrain)/12, y0 = 2170, 
       x1 = 1991 + (nTrain + nValid)/12, y1 = 2170, 
       code = 3, length = 0.1)
text(x = 2005.5, y = 2210, labels="Future")
arrows(x0 = 1991 + (nTrain + nValid + 1)/12, y0 = 2170, 
       x1 = 1991 + (nTrain + nValid + 36)/12, y1 = 2170, 
       code = 2, length = 0.1)
@

\begin{marginfigure}
\includegraphics[width=0.85\textwidth]{PTSFR-Amtraklinreglin}
\caption{Linear regression of Amtrak Ridership}
\label{fig:Amtraklinreglin}
\setfloatalignment{b}
\end{marginfigure}

We can see in Figure~\ref{fig:Amtraklinreglin} that this does not seem to lead to good forecasting of the validation period. Furthermore, the statistical summary of the model is not very encouraging:

<<>>=
summary(train.lm)
@
\medskip
The p-value for the slope (''trend") is very high. However, this should not surprise us because we have taken into account all the data, seasonality included, to determine this linear relation. At least we should remove the seasonal component from the data in order to get a better idea of the linear trend.

We can do this by looking at the differenced tims-seris, where we take the lag equal to the seasonal period. For the Amtrak time-series this is lag12. This gives the following result. The p-value for the slope is now clearly very significant. The differencing will however influence the result. If
\begin{equation}
	y_{t}=b_{0} + b_{1}t +b_{2}t^{2}
\end{equation}

then
\begin{equation}
	lagy_{T} = y_{t}-y_{t-T}=b_{0}+b_{1}t+b_{2}t^{2} - [b_{0}+b_{1}(t-T)+b_{2}(t-T)^{2}] =(b_{1}T + b_{2}T^{2}) -2b_{2}Tt
\end{equation}

It is a linear equation in $t$ and from the intercept and the slope we can calcualte $b_{1}$ and $b_{2}$:

\begin{eqnarray}
  b_{1} &=& \frac{intercept}{T} + \frac{slope}{2} \\
  b_{2} &=& -\frac{slope}{2T}
\label{eq:derivedcnst}
\end{eqnarray}

<<label=Amtraklag12reglin, fig=TRUE, include=FALSE, echo=FALSE>>=
Amtrak.lag12.ts <- diff(ridership.ts, lag = 12)
nValid <- 36
nTrain <- length(Amtrak.lag12.ts) - nValid
train.lag12.ts <- window(Amtrak.lag12.ts, 
                   start = c(1991, 1),
                   end = c(1991, nTrain))
valid.lag12.ts <- window(Amtrak.lag12.ts, 
                   start = c(1991, nTrain + 1), 
                   end = c(1991, nTrain + nValid))
train.lag12.lm <- tslm(train.lag12.ts ~ trend)
train.lag12.lm.pred <- forecast(train.lag12.lm, h = nValid, level = 0)
plot(train.lag12.lm.pred,
     ylim = c(-250, 250),
     ylab = "Amtrak Ridership",
     xlab = "Time",
     bty = "l",
     xaxt = "n",
     xlim = c(1991, 2006.25),
     main = "",
     lty = 2)
axis(1, at = seq(1991, 2006, 1),
     labels = format(seq(1991, 2006, 1)))
lines(train.lag12.lm$fitted, lwd = 2, col = "green")
lines(valid.lag12.ts, col = "red", lty = 2)
abline(v = 1991 , col="blue")
abline(v = 1991 + (nTrain)/12 , col="blue")
abline(v = 1991 + (nTrain + nValid)/12, col="blue")
text(x = 1996, y = 225, labels="Training")
arrows(x0 = 1991, y0 = 210, 
       x1 = 1991 + nTrain/12, y1 = 210, 
       code = 3, length = 0.1)
text(x = 2001.6, y = 225, labels="Validation")
arrows(x0 = 1991 + (nTrain)/12, y0 = 210, 
       x1 = 1991 + (nTrain + nValid)/12, y1 = 210, 
       code = 3, length = 0.1)
text(x = 2005.5, y = 225, labels="Future")
arrows(x0 = 1991 + (nTrain + nValid + 1)/12, y0 = 210, 
       x1 = 1991 + (nTrain + nValid + 36)/12, y1 = 210, 
       code = 2, length = 0.1)
@

\begin{marginfigure}[5cm]
\includegraphics[width=0.85\textwidth]{PTSFR-Amtraklag12reglin}
\caption{Linear regression of Amtrak Ridership lag12 differenced}
\label{fig:Amtraklag12reglin}
\setfloatalignment{b}
\end{marginfigure}


<<>>=
summary(train.lag12.lm)
@
\medskip
When we calculate the values of $b_{1}$ and $b_{2}$ using equation~\ref{eq:derivedcnst}, we get $b_{1}=$\Sexpr{round(train.lag12.lm$coefficients[1]/12 + train.lag12.lm$coefficients[2]/2, 4)} and $b_{2}=$\Sexpr{round(- train.lag12.lm$coefficients[2]/24, 4)}, which is not far from the co\"{e}fficients for the linear term ($b_{1}=-4.4488$) and the quadratic term ($b_{2}=0.0381$) when we use Excel to find a quadratic trend through the Amtrak data. Also take into account that we are working here on data that lack the first period, due to the differencing.

\subsection{Exponential trend}

Take the log and do a linear regression.

\subsection{Polynomial trend}

\newthought{Amtrak time-series}

<<label=Amtraklinregpoly, fig=TRUE, include=FALSE, echo=FALSE>>=
nValid <- 36
nTrain <- length(ridership.ts) - nValid
train.ts <- window(ridership.ts, 
                   start = c(1991, 1),
                   end = c(1991, nTrain))
valid.ts <- window(ridership.ts, 
                   start = c(1991, nTrain + 1), 
                   end = c(1991, nTrain + nValid))
train.lm <- tslm(train.ts ~ trend + I(trend^2))
train.lm.pred <- forecast(train.lm, h = nValid, level = 0)
plot(train.lm.pred,
     ylim = c(1300, 2600),
     ylab = "Ridership",
     xlab = "Time",
     bty = "l",
     xaxt = "n",
     xlim = c(1991, 2006.25),
     main = "",
     lty = 2)
axis(1, at = seq(1991, 2006, 1),
     labels = format(seq(1991, 2006, 1)))
lines(train.lm$fitted, lwd = 2, col = "green")
lines(valid.ts, col = "red", lty = 2)
abline(v = 1991 , col="blue")
abline(v = 1991 + (nTrain)/12 , col="blue")
abline(v = 1991 + (nTrain + nValid)/12, col="blue")
text(x = 1996, y = 2210, labels="Training")
arrows(x0 = 1991, y0 = 2170, 
       x1 = 1991 + nTrain/12, y1 = 2170, 
       code = 3, length = 0.1)
text(x = 2002.6, y = 2210, labels="Validation")
arrows(x0 = 1991 + (nTrain)/12, y0 = 2170, 
       x1 = 1991 + (nTrain + nValid)/12, y1 = 2170, 
       code = 3, length = 0.1)
text(x = 2005.5, y = 2210, labels="Future")
arrows(x0 = 1991 + (nTrain + nValid + 1)/12, y0 = 2170, 
       x1 = 1991 + (nTrain + nValid + 36)/12, y1 = 2170, 
       code = 2, length = 0.1)
@

\begin{marginfigure}
\includegraphics[width=0.85\textwidth]{PTSFR-Amtraklinregpoly}
\caption{Polynomial (quadratic) regression of Amtrak Ridership}
\label{fig:Amtraklinregpoly}
\setfloatalignment{b}
\end{marginfigure}

<<>>=
summary(train.lm)
@
\medskip
The p-value for all co\"{e}fficients is very low and a quadratic trend seems relevant even if we  are taking into account all the data, seasonality included. The co\"{e}fficients and the $R^{2}$-value that R give are not the same as those in Excel: the R results come from the training set, the Excel values from training + evaluation. Using the whole time-series in R we get the same results as in Excel:

<<>>=
summary(tslm(ridership.ts ~ trend + I(trend^2)))
@

\section{Model with seasonality}\index{seasonality!regression model}\index{regression model!seasonality}

The most common way to capture \emph{seasonality}\index{seasonality} is by creating a new categorial variable with levels that equal the subdivisions of the season's period. In the Amtrak time-series the period $T=12$ where every month should be repetitive: Januari in one year should be the same as Januari in any other year, provided the trend is excluded. The same goes for Februari, and any other month. So we add a new variable ''Season" to the Amtrak time-series, with levels ''Jan", ''Feb" etc.
The linear regression is a regression the categorial variable ''Season". The base value is the value for ''Jan" and then we get the shifts for every month. The time series already has this decomposition of the period into 12 parts. We can force the \textit{tslm}-function to consider these as categorical variables by stating in the formula for the linear regression that it should be done with the parameter ''season".

<<label=Amtrakllinregseason, fig=TRUE, include=FALSE, echo=FALSE>>=
train.seas.lm <- tslm(train.ts ~ season)
summary(train.seas.lm)
train.seas.lm.pred <- forecast(train.seas.lm, h = nValid, level = 0)
plot(train.seas.lm.pred,
     ylim = c(1300, 2600),
     ylab = "Ridership",
     xlab = "Time",
     bty = "l",
     xaxt = "n",
     xlim = c(1991, 2006.25),
     main = "",
     lty = 2)
axis(1, at = seq(1991, 2006, 1),
     labels = format(seq(1991, 2006, 1)))
lines(train.seas.lm$fitted, lwd = 2, col = "green")
lines(valid.ts, col = "red", lty = 2)
abline(v = 1991 , col="blue")
abline(v = 1991 + (nTrain)/12 , col="blue")
abline(v = 1991 + (nTrain + nValid)/12, col="blue")
text(x = 1996, y = 2210, labels="Training")
arrows(x0 = 1991, y0 = 2170, 
       x1 = 1991 + nTrain/12, y1 = 2170, 
       code = 3, length = 0.1)
text(x = 2002.6, y = 2210, labels="Validation")
arrows(x0 = 1991 + (nTrain)/12, y0 = 2170, 
       x1 = 1991 + (nTrain + nValid)/12, y1 = 2170, 
       code = 3, length = 0.1)
text(x = 2005.5, y = 2210, labels="Future")
arrows(x0 = 1991 + (nTrain + nValid + 1)/12, y0 = 2170, 
       x1 = 1991 + (nTrain + nValid + 36)/12, y1 = 2170, 
       code = 2, length = 0.1)
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-Amtrakllinregseason}
\caption{Linear regression of Amtrak Ridership on factor Season}
\label{fig:Amtrakllinregseason}
\setfloatalignment{b}
\end{figure}

\newpage
\section{Model with trend and seasonality}\index{trend and seasonality!regression model}\index{regression model!trend and seasonality}

We can combine the whole analysis of trend and seasonality in one step:

<<label=Amtrakllinregtrendseason, fig=TRUE, include=FALSE, echo=FALSE>>=
train.trend.seas.lm <- tslm(train.ts ~ trend + I(trend^2) + season)
summary(train.trend.seas.lm)
train.trend.seas.lm.pred <- forecast(train.trend.seas.lm, h = nValid, level = 0)
plot(train.trend.seas.lm.pred,
     ylim = c(1300, 2600),
     ylab = "Ridership",
     xlab = "Time",
     bty = "l",
     xaxt = "n",
     xlim = c(1991, 2006.25),
     main = "",
     lty = 2)
axis(1, at = seq(1991, 2006, 1),
     labels = format(seq(1991, 2006, 1)))
lines(train.trend.seas.lm$fitted, lwd = 2, col = "green")
lines(valid.ts, col = "red", lty = 2)
abline(v = 1991 , col="blue")
abline(v = 1991 + (nTrain)/12 , col="blue")
abline(v = 1991 + (nTrain + nValid)/12, col="blue")
text(x = 1996, y = 2500, labels="Training")
arrows(x0 = 1991, y0 = 2450, 
       x1 = 1991 + nTrain/12, y1 = 2450, 
       code = 3, length = 0.1)
text(x = 2002.6, y = 2500, labels="Validation")
arrows(x0 = 1991 + (nTrain)/12, y0 = 2450, 
       x1 = 1991 + (nTrain + nValid)/12, y1 = 2450, 
       code = 3, length = 0.1)
text(x = 2005.5, y = 2500, labels="Future")
arrows(x0 = 1991 + (nTrain + nValid + 1)/12, y0 = 2450, 
       x1 = 1991 + (nTrain + nValid + 36)/12, y1 = 2450, 
       code = 2, length = 0.1)
@

\begin{marginfigure}[-12cm]
\includegraphics[width=0.85\textwidth]{PTSFR-Amtrakllinregtrendseason}
\caption{Linear regression of Amtrak Ridership: trend = quadratic, season = factor}
\label{fig:Amtrakllinregtrendseason}
\setfloatalignment{b}
\end{marginfigure}

You can add other functions to the formula that discribes the linear regression to be executed. For example \textit{$tslm(train.ts \approx trend + I(trend^{2}) + sin(2*pi*trend/50) + sin(2*pi*trend/100))$} will add two continuous sine-functions to the regression.

\section{Creating forecasts from the global model}
First combine the test and the validation set, refit the chosen model and then start forecasting!

\chapter{Regression base models: autocorrelation and external information}\index{model!regression based}\index{autocorrelation}

Global models use linear regression to capture trend and seasonality into functions (trend, sinusoidal seasonal behaviour) or stepwise adjustments (constructing a typical season). One of the basic hypothesis in linear regression is that the data are independent. \emph{Autocorrelation} tries to find out if this assumption is correct. If not, the knowledge that some data points are correlated, can help us to improve our forecast model beyond trend and seasonality.

The method can also be used to include information about \emph{special events}\index{special events} or to integrate \emph{external informaton}\index{external information}.

\section{Autocorrelation}\index{autocorrelation}

In many time-series values in neighbouring time slots tend to be correlated. This correlation between values belonging to the same time-series is called \emph{autocorrelation}.

\subsection{Computing autocorrelation}\index{autocorrelation!computing}

To compute autocorrelation we calculate the correlation between a time-series and a lagged version of the series. The lag1-version will of course have one data point less (the first), a lagk-version will have k data points less. For the Amtrak time-series the first 24 data points for the original series, the lag1- and the lag2-version are:

<<echo = FALSE>>=
Amtrak.lag <- data.frame(original = Amtrak.data$Ridership[1:24], lag1 = 0, lag2 = 0 )
Amtrak.lag$lag1[1] <- NA
Amtrak.lag$lag1[2:24] <- Amtrak.lag$original[1:23]
Amtrak.lag$lag2[1:2] <- NA
Amtrak.lag$lag2[3:24] <- Amtrak.lag$original[1:22]
@
\medskip
<<>>=
Amtrak.lag
@
\medskip
We can calculate the correlation between the original time-series and lag1, ad between the original time-series and lag2 with the R-function \textit{cor} (use parameter ''use = 'complete.obs'").

<<>>=
cor(Amtrak.lag$original, Amtrak.lag$lag1, use = "complete.obs")
cor(Amtrak.lag$original, Amtrak.lag$lag2, use = "complete.obs")
@

The \textbf{forecast}-package has the \textit{Acf}-function (Autocorrelation function). For the first 24 datapoints in the data-series we get for lag1 and lag2

<<>>=
ridership.24.ts <- window(ridership.ts, start = c(1991, 1), end = c(1991, 24))
Acf(ridership.24.ts, lag.max = 2, plot=FALSE)
@
\medskip

Close, but no sigar. There is something odd in the output of the \textit{Afc}-function. 

Let's start from Pearson's correlation formula:
\begin{equation}
	cor(x,y) = \frac{\sum\limits_{j=1}^{j=n} \left( x_{j}-\bar{x} \right) \left( y_{j}-\bar{y} \right)}{\sqrt{\sum\limits_{j=1}^{i=n} \left( x_{j}-\bar{x} \right)^{2}} \sqrt{\sum\limits_{j=1}^{i=n} \left( y_{j}-\bar{y} \right)^{2}}}
\end{equation}

When we start from the original time-series $Orig$ with N elements, and we want to study the correlation with $lag=k$ we define:
\begin{eqnarray}
  x_{j} &=& [Orig_{1}, Orig_{2}, \ldots , Orig_{N-k}] \quad  n=(N-k) \\
  y_{j} &=& [Orig_{k+1}, Orig_{k+2}, \ldots, Orig_{N}] \quad n=(N-k)
\end{eqnarray}

the autocorrelation with $lag=k$ becomes:
\begin{equation}
	cor(x,y) = \frac{\sum\limits_{j=1}^{j=n} \left( x_{j}-\bar{x} \right) \left( y_{j}-\bar{y} \right)}{\sqrt{\sum\limits_{j=1}^{i=n} \left( x_{j}-\bar{x} \right)^{2}} \sqrt{\sum\limits_{j=1}^{i=n} \left( y_{j}-\bar{y} \right)^{2}}}
\end{equation}

The definition of the output of the \textit{Acf}-function is given in\sidenote{''Forecasting: Principles and Practice", Rob J Hyndman and George Athanasopoulos, Monash University, Australia; paragraph 2.8}:

\begin{equation}
	r_{k} = \frac{\sum\limits_{t=k+1}^{T}(y_{t}-\bar{y})(y_{t-k} - \bar{y})}{\sum\limits_{t=1}^{T}(y_{t}-\bar{y})^{2}} 
\end{equation}

Rearranging the nominator factors, call the original time-series $Orig$, the index $i$ and the number of values in the original time series $N$,
\begin{equation}
	r_{k} = \frac{\sum\limits_{i=k+1}^{N}(Orig_{i-k} - \overline{Orig})(Orig_{i}-\overline{Orig})}{\sum\limits_{i=1}^{N}(Orig_{i}-\overline{Orig})^{2}}  = \frac{\sum\limits_{i=k+1}^{N}(Orig_{i-k} - \overline{Orig})(Orig_{i}-\overline{Orig})}{\sum\limits_{i=1}^{N}(0_{i}-\overline{Orig})^{2}} = \frac{\sum\limits_{j=1}^{N}(x_{j} - \overline{Orig})(y_{j}-\overline{Orig})}{\sum\limits_{i=1}^{N}(Orig_{i}-\overline{Orig})^{2}}
\end{equation}

The $\overline{Orig}$ in the first and second factor of the nominator are the same and refer to average value of the original series. In Pearson's formula these should be $mean(x_{j})$ for $j=1 \ldots (N-k)$ in the first factor, and $mean(y_{j})$ for $j=1 \ldots (N-k)$ in the second factor. The difference with $\overline{Orig}$ will probably not be important on condition that the reduced series $x$ and $y$ are not too different from the original series $Orig$. In other words: when $n \approx N$ or when $\frac{k}{N}$ is small. 

In the denominator we expect to find the product of the square roots of $(n-1)$\sidenote{There is a suggestion in Hyndman that for time series we should use $n$ times the variance} times the variance of the reduced series $x$ and $y$. In the formula for $r_{k}$ we have the product of the square roots of $(N-1)$ times the variance of the original time series. Because both factors are the same, the square root disappears. Again, this is probably OK when $n \approx N$ or when $\frac{k}{N}$ is small.

The question is: why these approximations? Why not use the ''proper" Pearson correlation formula. Let's calculate both Pearson, rHyndman and Acf for the Arima data for lags 0 ... 5:

<<echo=FALSE>>=
JT.Hyndman <- function(k, Orig) {
  # Orig.df is original time-series
  N <- length(Orig)
  barOrig <- mean(Orig)
  if (k == 0) {
    x <- Orig
    } else {
      x <- Orig[-((N - k + 1):N)] 
    }
  barx <- mean(x)
  if (k == 0) {
    y <- Orig
  } else {
    y <- Orig[-(1:k)] 
  }
  bary <- mean(y)
  Pearson <- cor(x, y)
  rHyndman <- sum((x - barOrig)*(y - barOrig))/(sum((Orig - barOrig)^2))
  Autoc <- Acf(Orig, lag.max = k, plot=FALSE)[k]
  return(c(k, Pearson, rHyndman, Autoc$acf[1,1,1]))
}
@

<<>>=
res <- data.frame(k = 0, Pearson = 0, rHyndman = 0, Acf = 0)
for (k in (0:5)) {
  res[(k+1),] <- JT.Hyndman(k, Amtrak.data$Ridership[1:24])
}
res
@

We can see that the formula given in \sidenote{''Forecasting: Principles and Practice", Rob J Hyndman and George Athanasopoulos, Monash University, Australia; paragraph 2.8} does indeed give the output from the \textit{Acf}-function. Both these results are not very different from the ''correct" Pearson correlation.

But for short and/or special time-series the situation is different. Take a simple linear time-series with 10 values ($seq(1,10)$) (Figure~\ref{fig:CorandAcf10}):

<<>>=
res <- data.frame(k = 0, Pearson = 0, rHyndman = 0, Acf = 0)
for (k in (0:5)) {
  res[(k+1),] <- JT.Hyndman(k, seq(1,10))
}
res
@

<<label=CorandAcf10, fig=TRUE, include=FALSE, echo=FALSE>>=
ggplot(data = res) +
  geom_segment(aes(x = k - 0.05, y = 0, xend = k - 0.05, yend = Pearson), size = 2, color="red") +
  geom_text(x = 2.4, y = 0.95, color = "red", label = "Pearson") +
  geom_segment(aes(x = k + 0.05, y = 0, xend = k + 0.051, yend = rHyndman), size = 2, color="blue") +
  geom_text(x = 2.5, y = 0.45, color = "blue", label = "rHyndman") +
  ggtitle("Autoregression of [1, 2, ..., 10] with cor and Afc") +
  xlab("k") +
  geom_hline(yintercept = 0, size = 1, color = "black") +
  scale_y_continuous(limits= c(-0.5, 1)) +
  JT.theme
@

\begin{marginfigure}[-7cm]
\includegraphics[width=0.85\textwidth]{PTSFR-CorandAcf10}
\caption{Autoregression of [1, 2, ..., 10] with cor and Afc}
\label{fig:CorandAcf10}
\setfloatalignment{b}
\end{marginfigure}

The same simple linear time-series but now with 100 values ($seq(1,100)$) (Figure~\ref{fig:CorandAcf100})

<<label=CorandAcf100, fig=TRUE, include=FALSE, echo=FALSE>>=
res <- data.frame(k = 0, Pearson = 0, rHyndman = 0, Acf = 0)
for (k in (0:25)) {
  res[(k+1),] <- JT.Hyndman(k, seq(1,100))
}
ggplot(data = res) +
  geom_segment(aes(x = k - 0.1, y = 0, xend = k - 0.1, yend = Pearson), size = 1, color="red") +
  geom_text(x = 17.4, y = 0.95, color = "red", label = "Pearson") +
  geom_segment(aes(x = k + 0.1, y = 0, xend = k + 0.1, yend = rHyndman), size = 1, color="blue") +
  geom_text(x = 17.7, y = 0.55, color = "blue", label = "rHyndman") +
  ggtitle("Autoregression of [1, 2, ..., 10O] with cor and Afc") +
  xlab("k") +
  geom_hline(yintercept = 0, size = 1, color = "black") +
  scale_y_continuous(limits= c(-0.5, 1)) +
  JT.theme
@

\begin{marginfigure}[0cm]
\includegraphics[width=0.85\textwidth]{PTSFR-CorandAcf100}
\caption{Autoregression of [1, 2, ..., 100] with cor and Afc}
\label{fig:CorandAcf100}
\setfloatalignment{b}
\end{marginfigure}

The Hyndman formula seems to ''punish" lag-series with fewer terms. But why?

Let's try a sine-function with period $T=20$ and the same sine-function with an additive $N(0,\sigma=0.5)$ random error (Figure~\ref{fig:CorandAcfsin}):

<<label=CorandAcfsin, fig=TRUE, include=FALSE, echo=FALSE>>=
res <- data.frame(k = 0, Pearson = 0, rHyndman = 0, Acf = 0)
t <- seq(0, 100)
T <- 20
Orig <- sin(2*pi*t/T)
for (k in (0:25)) {
  res[(k+1),] <- JT.Hyndman(k, Orig)
}
p1 <- ggplot(data = res) +
  geom_segment(aes(x = k - 0.1, y = 0, xend = k - 0.1, yend = Pearson), size = 1, color="red") +
  geom_text(x = 17.4, y = 0.95, color = "red", label = "Pearson") +
  geom_segment(aes(x = k + 0.1, y = 0, xend = k + 0.1, yend = rHyndman), size = 1, color="blue") +
  geom_text(x = 17.7, y = 0.55, color = "blue", label = "rHyndman") +
  ggtitle("Autoregression of sine (T=20)\nwith cor and Afc") +
  xlab("k") +
  geom_hline(yintercept = 0, size = 1, color = "black") +
  scale_y_continuous(limits= c(-1, 1)) +
  JT.theme
res <- data.frame(k = 0, Pearson = 0, rHyndman = 0, Acf = 0)
t <- seq(0, 100)
T <- 20
set.seed(2019)
Orig <- sin(2*pi*t/T) + rnorm(101, 0, 0.5)
for (k in (0:25)) {
  res[(k+1),] <- JT.Hyndman(k, Orig)
}
p2 <- ggplot(data = res) +
  geom_segment(aes(x = k - 0.1, y = 0, xend = k - 0.1, yend = Pearson), size = 1, color="red") +
  geom_text(x = 17.4, y = 0.95, color = "red", label = "Pearson") +
  geom_segment(aes(x = k + 0.1, y = 0, xend = k + 0.1, yend = rHyndman), size = 1, color="blue") +
  geom_text(x = 17.7, y = 0.55, color = "blue", label = "rHyndman") +
  ggtitle("Autoregression of sine (T=20) + error (N(O,0.5))\n with cor and Afc") +
  xlab("k") +
  geom_hline(yintercept = 0, size = 1, color = "black") +
  scale_y_continuous(limits= c(-1, 1)) +
  JT.theme
grid.arrange(p1, p2, nrow = 1)
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-CorandAcfsin}
\caption{Autoregression of sine and sine+error}
\label{fig:CorandAcfsin}
\setfloatalignment{b}
\end{figure}

From these figures you can indeed deduce information about the global behaviour of the original time-series:
\begin{itemize}
	\item a very strong positive lag1 autocorrelation indicates a global linear trend
	\item a very stron negative lag1 autocorrelation indicates a swing pattern
	\item a very strong lagk autocorrelation indicatse a seasonal behaviour, certainly when lagk/2 shows a negative Acf while lagk shows a positive Acf
\end{itemize}

However: you can deduce this as easily from visualising the time-series and the Pearson-correlation for different lags.

\subsection{What is the use of autocorrelation?}\index{autocorrelation!what's the use?}

The strong point of autocorrelation calculation is in checking if a time series is truly \emph{stationary}\index{stationary}\index{time series!stationary}. A \emph{stationary} time-series has no trend and no seasonality. If the gobal model we have constructed for our time-series is a good representation of its behaviour, we expect that the \emph{residuals}\index{residuals} show \textbf{no} autocorrelation. If we do find autocorrelation this suggests that our model is not good enough yet. We could try to improve the global model, or we could ''tweek" it locally using the knowledge that there is autocorrelation within the residuals.

\newthought{Example: Amtrak data}

<<label=Amtrakllinregtrendseasonresid, fig=TRUE, include=FALSE, echo=FALSE>>=
plot(train.trend.seas.lm$residuals,
     ylim = c(-250, 250),
     ylab = "Residuals Ridership",
     xlab = "Time",
     bty = "l",
     xaxt = "n",
     xlim = c(1991, 2002),
     main = "",
     lty = 1,
     col = "red")
axis(1, at = seq(1991, 2002, 1),
     labels = format(seq(1991, 2002, 1)))
@

\begin{marginfigure}[0cm]
\includegraphics[width=0.85\textwidth]{PTSFR-Amtrakllinregtrendseasonresid}
\caption{Residuals of linear regression of Amtrak Ridership: trend = quadratic, season = factor}
\label{fig:Amtrakllinregtrendseasonresid}
\setfloatalignment{b}
\end{marginfigure}

<<>>=
res <- data.frame(k = 0, Pearson = 0, rHyndman = 0, Acf = 0)
for (k in (0:12)) {
  res[(k+1),] <- JT.Hyndman(k, train.trend.seas.lm$residuals)
}
res
@

Pearson and Acf do not differ very much, so we use Acf because it gives us a plot:
<<label=AmtrakAcfresid, fig=TRUE, include=FALSE, echo=FALSE>>=
Acf(train.trend.seas.lm$residuals, lag.max = 12)
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-AmtrakAcfresid}
\caption{Acf of residuals of linear regression of Amtrak Ridership: trend = quadratic, season = factor}
\label{fig:AmtrakAcfresid}
\setfloatalignment{b}
\end{figure}

We can see that the residuals are \textbf{not} stationary: there is substantial autocorrelation at lags 1, 2 and 3. This tells us that usefull information is still available within the residuals which can help to improve out forecast.

\section{Improving Forecasts by capturing autocorrelation: AR and ARIMA models}\index{autocorrelation}\index{AR-models}\index{ARIMA-models}

\subsection{Autoregressive models (AR)}\index{autoregressive model}\index(autoregressive model!AR)

\emph{Autoregressive models} are linear regression models, but the predicting variabel is not the tiem $t$ but past values of the series. An autoregressive model for a time-series $x$ of order\index{autoregressive model!order} 2 is a linear correlation with predictors the lag1 and the lag2 values of the time series:
\begin{equation}
	x_{t} = b_{0} + b_{1}x_{t-1} + b_{2}x_{t-2}
\end{equation}

We can use the traditional \textit{lm}-function but this is not optimal because one of the basic hypotheses of linear regression is the \emph{independence} of the observations. This is clearly not true in the case where we suspect autocorrelation! The \textit{ARIMA}-function takes the interdependency into account.

There are two approaches to take advantage of autocorrelation: AR as a second-layer model and full scale ARIMA models.

\subsection{AR as a second-layer model}\index{autocorrelation model!second-layer model}

The steps to take are:
\begin{enumerate}
	\item generate a k-step ahead forecast of the time-series using a forecasting method. This gives us k values $F_{t+1} \ldots F_{t+k}$
	\item generate a k-step ahead forecast of the time-series of the forecast error (the residuals) using an AR or other model. This gives us k values $e_{t+1} \ldots e_{t+k}$
	\item improve the initial forecasts by adding the forecasted error. This gives us k values $F_{t+1} + e_{t+1} \ldots F_{t+k} + e_{t+k}$
\end{enumerate}

We made a global forecasting model for the Amtrak Ridership data based on a quadratic trend and a seasonal component. We also saw (Figure~\ref{fig:AmtrakAcfresid}) that the residuals are not stationary and hide some information that we can use to improve our forecast. Let's go through the steps:

\begin{enumerate}
	\item we have a global model (quadratic + seasonal) based on the training set
	\item calculate the forecasts for the validation period
	\item calculate the errors for the validation period
	\item construct a linear model regressing the error in the validation period using an AR model
	\item generate a k-step ahead forecast of the time-series of the forecast error (the residuals) using an AR or other model. This gives us k values $e_{t+1} \ldots e_{t+k}$
	\item improve the initial forecasts by adding the forecasted error. This gives us k values $F_{t+1} + e_{t+1} \ldots F_{t+k} + e_{t+k}$
\end{enumerate}

<<>>=
train.JT <- data.frame(t = seq(1, nTrain), number = Amtrak.data$Ridership[1:nTrain], season = rep(c("Jan","Feb","Mar","Apr", "May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"), length.out = nTrain))
train.JT$season <- factor(train.JT$season, levels = c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"))
#
# Construct linear model (quadratic + season) based on the training set
#
linmod <- lm(number ~ t + I(t^2) + season, data = train.JT)
#
train.JT$pred <- predict(linmod)
@

<<label=Amatrakmodel, fig=TRUE, include=FALSE, echo=FALSE>>=
ggplot(data = train.JT, aes(x = t)) +
  geom_line(aes(y = number), lty = 2) +
  geom_line(aes(y = pred), col="red") +
  labs(title = "Training set: values and prediction") +
  JT.theme
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-Amatrakmodel}
\caption{Linear regression of Amtrak Ridership: trend = quadratic, season = factor}
\label{fig:Amatrakmodel}
\setfloatalignment{b}
\end{figure}

The residuals of this model are the differences between the actual and the predicted values. Let's look at the plot of the residuals, the plot of the Pearson and Acf correlations for different lags and the relation between lag1 and lag0 of the residuals (Figure~\ref{fig:Amtrakresid})

<<label=Amtrakresid, fig=TRUE, include=FALSE, echo=FALSE>>=
train.JT$resid <- linmod$residuals
res <- data.frame(k = 0, Pearson = 0, rHyndman = 0, Acf = 0)
for (k in (0:12)) {
  res[(k+1),] <- JT.Hyndman(k, train.JT$resid)
}
p1 <- ggplot(data = train.JT, aes(x = t)) +
  geom_line(aes(y = resid), col = "red") +
  labs(title = "Training set\nresiduals") +
  JT.theme
p2 <- ggplot(data = res, aes(x = k)) +
  geom_segment(aes(x = seq(0,12) - 0.1, y = 0, xend = seq(0, 12) - 0.1, yend = Pearson), col = "red") +
  geom_segment(aes(x = seq(0,12) + 0.1, y = 0, xend = seq(0, 12) + 0.1, yend = Acf), col = "blue") +
  labs(title = "Training set\nautocorrelation", x = "lag", y = "Pearson (red) / Acf (blue)") +
  scale_x_continuous(breaks = c(seq(0, 12, by = 2))) +
  JT.theme
linmod.res.ARx <- linmod$residuals[-nTrain]
linmod.res.ARy <- linmod$residuals[-1]
p3 <- ggplot() +
  geom_point(aes(x = linmod.res.ARx, y = linmod.res.ARy), col="red") +
  labs(title = "Training set\nresiduals lag0 function of lag1", x = "lag1", y = "lag0") +
  JT.theme
grid.arrange(p1, p2, p3, nrow = 1)
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-Amtrakresid}
\caption{Analysis of the residuals of linear regression of Amtrak Ridership}
\label{fig:Amtrakresid}
\setfloatalignment{b}
\end{figure}

A first order (AR1) linear regression seems appropriate:
<<>>=
linmod.res.AR <- lm(linmod.res.ARy ~ linmod.res.ARx)
@

We can now use this extra information on our prediction in the training period. For each moment t we know the actual value (train.JT\$number), the predicted value (train.JT\$pred) and the residual (train.JT\$resid). Using the residual model and the residual at time $t$ we can calculate the ''improvement" (except for the first value because there is no preceding error):

<<>>=
train.JT$improv[1] <- 0
for (i in (2:nTrain)) {
  train.JT$improv[i] <- predict(linmod.res.AR, linmod.res.ARx = c(train.JT$resid[i-1]))[i]
}
train.JT$pred.improv <- train.JT$pred + train.JT$improv
@

<<label=Amatrakimprov, fig=TRUE, include=FALSE, echo=FALSE>>=
ggplot(data = train.JT, aes(x = t)) +
  geom_line(aes(y = number), lty = 2) +
  geom_line(aes(y = pred), col="red") +
  geom_line(aes(y = pred.improv), col = "blue") +
  labs(title = "Training set: improved prediction with AR1") +
  JT.theme
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-Amatrakimprov}
\caption{Improved linear regression of Amtrak Ridership with AR1}
\label{fig:Amatrakimprov}
\setfloatalignment{b}
\end{figure}

\newpage
When we calculate the accuracy metrics for the predictions before and after the AR1-improvement, we can see that it is much better:

<<>>=
accuracy(train.JT$pred, train.JT$number)
accuracy(train.JT$pred.improv, train.JT$number)
@

\printindex

\newpage

\textbf{Thanks} \\
\medskip
R Core Team (2018). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.
\medskip
<<>>=
sessionInfo()
@

\end{document}