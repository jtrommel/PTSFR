\documentclass{tufte-book}
\usepackage{graphicx}  % werken met figuren
\usepackage{gensymb} % werken met wetenschappelijke eenheden\usepackage{geometry}
\usepackage{changepage} % http://ctan.org/pkg/changepage
\usepackage[dutch,british]{babel} % instelling van de taal (woordsplitsing, spellingscontrole)
\usepackage[parfill]{parskip} % Paragrafen gescheiden door witte lijn en geen inspringing
\usepackage[font=small,skip=3pt]{caption} % Minder ruimte tussen figuur/table en ondertitel. Ondertitel klein
\usepackage{capt-of}
\usepackage{indentfirst}
\setlength{\parindent}{0.7cm}
\usepackage{enumitem} % Laat enumerate werken met letters
\usepackage{url}
\usepackage{lipsum}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
% Prints a trailing space in a smart way.
\usepackage{xspace}
\usepackage{hyperref}
\usepackage{amsmath}

\DeclareGraphicsExtensions{.pdf,.png,.jpg}

% Alter some LaTeX defaults for better treatment of figures:
% See p.105 of "TeX Unbound" for suggested values.
% See pp. 199-200 of Lamport's "LaTeX" book for details.
%   General parameters, for ALL pages:
    \renewcommand{\topfraction}{0.9}	% max fraction of floats at top
    \renewcommand{\bottomfraction}{0.9}	% max fraction of floats at bottom
%   Parameters for TEXT pages (not float pages):
    \setcounter{topnumber}{2}
    \setcounter{bottomnumber}{2}
    \setcounter{totalnumber}{4}     % 2 may work better
    \renewcommand{\textfraction}{0.1}	% allow minimal text w. figs
%   Parameters for FLOAT pages (not text pages):
    \renewcommand{\floatpagefraction}{0.8}	% require fuller float pages
% N.B.: floatpagefraction MUST be less than topfraction !!
\setcounter{secnumdepth}{3}

\newcommand{\tthdump}[1]{#1}

\newcommand{\openepigraph}[2]{
  \begin{fullwidth}
  \sffamily\large
    \begin{doublespace}
      \noindent\allcaps{#1}\\ % epigraph
      \noindent\allcaps{#2} % author
    \end{doublespace}
  \end{fullwidth}
}


\usepackage{makeidx}
\makeindex

\title{Practical Time Series Forecasting with R}
\author{Jan Trommelmans}

\begin{document}
\SweaveOpts{concordance=TRUE,prefix.string=PTSFR}
\setkeys{Gin}{width=1.1\marginparwidth} %% Sweave

<<echo=FALSE>>=
library(tidyverse)
library(lubridate)
library(broom)
library(funModeling)
library(forecast)
library(gridExtra)
library(writexl)
library(plotly)
library(ggfortify)
@

% Setting the ggplot theme:
<<echo=FALSE>>=
JT.theme <- theme(panel.border = element_rect(fill = NA, colour = "gray10"),
                  panel.background = element_blank(),
                  panel.grid.major = element_line(colour = "gray85"),
                  panel.grid.minor = element_line(colour = "gray85"),
                  panel.grid.major.x = element_line(colour = "gray85"),
                  axis.text = element_text(size = 8 , face = "bold"),
                  axis.title = element_text(size = 9 , face = "bold"),
                  plot.title = element_text(size = 12 , face = "bold"),
                  strip.text = element_text(size = 8 , face = "bold"),
                  strip.background = element_rect(colour = "black"),
                  legend.text = element_text(size = 8),
                  legend.title = element_text(size = 9 , face = "bold"),
                  legend.background = element_rect(fill = "white"),
                  legend.key = element_rect(fill = "white"))
@

% Functions
<<echo=FALSE>>=
TRJ.FFT <- function(signal.df) {
    # This function calculates the FFT for a time series stored in a data frame with as first 
    # column the time (or order of measurement) and as second column the vector of measurements.
    # The result is a list. 
    # The first element of the list is freqspec: the N frequencies plus for each frequency the 
    # amplitude and phase.
    # The second element of the list is resultaat: a data frame with those frequencies for which 
    # the amplitude  is at least 33% of the maximum amplitude. 
    # The data frame is sorted from highes amplitude to lowest. 
    # This data frame can be seen as containing the most influential frequencies.
    signal <- signal.df
    names(signal) <- c("t","x")
    N <- nrow(signal)
    Ts <- as.numeric(signal$t[2]-signal$t[1])
    Fs <- 1/Ts
    # Calculation of the double sided en single sided spectrum
    z <- fft(signal$x)
    P2 <- Mod(z/N)
    P1 <- P2[1:((N/2)+1)]
    P1[2:(length(P1)-1)] <- 2*P1[2:(length(P1)-1)]
    freq <- seq(0, (Fs/2)-(Fs/N), Fs/N)
    freqspec <- data.frame(freq=freq,amp=P1[1:(N/2)],arg=Arg(z[1:(N/2)]))
    # Finding the most important elements in the frequency spectrum
    grens <- ifelse(freqspec$freq[freqspec$amp==max(freqspec$amp)]==0,max(freqspec$amp[2:nrow(freqspec)])/3,max(freqspec$amp)/3)
    aantal <- length(freqspec$amp[freqspec$amp>grens])
    resultaat <- data.frame(freq=rep(0,aantal), amp=rep(0,aantal), fasehoek=rep(0,aantal))
    resultaat <- data.frame(freq=freqspec$freq[freqspec$amp>grens],
                            amp=freqspec$amp[freqspec$amp>grens],
                            fasehoek_pi=freqspec$arg[freqspec$amp>grens]/pi)
    resultaat <- resultaat[order(-resultaat$amp),]
    return(list("freqspec"=freqspec,"resultaat"=resultaat))
}
@


\frontmatter
\chapter*{Practical Time Series Forecasting with R}

\mainmatter

\chapter{Approaching Forecasting}
\section{Forecasting: Where?}
Everywhere

\section{Basic notation}
\begin{itemize}
 \item t=1, 2, 3 : index of time of measurement of the value
 \item $y_{1}, y_{2}, \ldots y_{i}, \ldots$ : value of the time series for t=i
 \item $F_{t}$ : forecasted value for time index t
 \item $F_{t+k}$ : k-step ahead forecast when the forecasting time is t (don't know what that means)
 \item $e_{t}$ : forecast error at time t =$y_{t}-F_{t}$
\end{itemize}

\section{Forecasting process}

\begin{enumerate}
	\item define goal (chapter 1)
	\item get data (chapter 2)
	\item explore and visualise the time series (chapter 2)
	\item pre-process data (chapter 2)
	\item partition the time series (chapter 4)
	\item apply forecasting methods (chapter 5 - 9)
	\item evaluate, compare performance (chapter 3)
	\item implement forecasting system (chapter 10)
\end{enumerate}

\section{Goal definition}

\begin{itemize}
	\item purpose of generating forecasts? \emph{time series analysis}\index{time series!analysis} is about \emph{descriptive modeling}\index{modeling!descriptive} the time series. \emph{time series forecasting}\index{time series!forecasting} is about \emph{predicting}\index{modeling!predictive} future values. Descriptive methods can use data ''from the future" (e.g. in moving averaging), while predictive methods can only rely on present data and past data.
	\item type of forecasts that are needed? \emph{Forecast horizon}\index{forecast!horizon}: how far in the future should we forecast? A forecast horizon $k$ is the number of time steps ahead we have to forecast $(F_{t+k})$. The value of $k$ depends on the purpose of the forecast. Here the element of data availability is important: if the most recent data are two months old, a forecast for next month requires a forecast horizon $k=3$.
	\item how will the forecasts be used? Do we want a numerical result or a binary one (''yes/no"). Who is the client: technocrats with a deep understanding of forecasting, or generalists who simply want to use the forecast?
	\item costs associated with forecast errors?
	\item data available in the future? \emph{Forecast updating}\index{forecast!updating}: forecasting for one point in time, or ongoing. If it is ongoing our forecasting method should be able to take new information into account. A forecast for december 2019 made in januari 2019 can be refreshed when data for februari, march ... 2019 become available.
	\item the level of automation. The level of automation increases when many time series have to be forecast, when it is an ongoing process, when extra data become available during the forecasting period and when less forecasting expertise is available.
\end{itemize}

\chapter{Time Series Data}

\section{Data Collection}

\subsection{Data quality}

\emph{Data quality}\index{data!quality} refers to:
\begin{itemize}
	\item accuracy of measurement
	\item missing values
	\item corrupted data
	\item data entry errors
\end{itemize}

Check if external data\index{data!external} can be more predictive then solely the historic sequence available in a time series.

\subsection{Temporal frequency}

The \emph{frequency}\index{frequency!of data collection} of data collection is not necessarily the right frequency for forecasting purposes. It depends on the forecasting goal. If we want monthly forecasts it is sensible to reduce daily measurements to monthly values. Fine grained data collection introduces \emph{noise}\index{noise} that is not relevant for longer term forecasts and should be filtered out.

Even if forecasts are wanted on at time base $T$ we can still aggregate the collected data on a time base $n*T$, make the forecast model and de-aggregate the forecast to time base $T$ (by a suitable interpolation method).

\subsection{Granularity of the time series}
\emph{Granularity}\index{granularity} refers to additional information present in the time series data that can be used to make subsets. These can be based on geographical information, sociological strata, age groups etc. A higher levels of detail can lead to subsets where the time series value equals zero. This could lead to changing the forecast method from numerical to binary (''yes/no").

\subsection{Domain expertise}

\index{domain expertise}Do we have the knowledge available to decide on the following topics:
\begin{itemize}
	\item which data are we going to collect?
	\item at what frequency?
	\item can we interpret the patterns in the data?
	\item can we identify extreme values?
	\item do the users of our forecast(s) have the knowledge to interpret the results?
\end{itemize}

\section{Time Series Components}

\emph{Time series}\index{time series} are a specific sort of data. In constrast with \emph{cross-secional data}\index{cross-sectional data}, which are multiple measurements taken at the same time, \emph{time series} consist of one measurement taken at different moments in time.

Time series can have the following characteristics:

\begin{enumerate}
  \item error: random fluctuation of values
  \item level: mean value of the series
  \item trend: gradual evolution without repetition
  \item seasonality: periodic behaviour with characteristic period $T_{i}$. Multiple periodicity is possible
\end{enumerate}

The use of \emph{level}\index{level} and \emph{trend}\index{trend} as separate characteristics is different from normal usage in model building where both are incorporated into the model equation. Thus if we model the general behaviour of $y$ as a function of $t$ by a polynomial:
\begin{equation}
y(t)=b_{0} + b_{1}t + b_{2}t^{2} + \ldots b_{i}t^{i} + \ldots = p(t)
\end{equation}

and we define the level as $\bar{y}$ then the \emph{trend} in the sense as defined above is given by:
\begin{equation}
trend = p(t) - \bar{y} = \left( b_{0} - \bar{y} \right) + b_{1}t + b_{2}t^{2} + \ldots b_{i}t^{i} + \ldots 
\end{equation}

The book\sidenote{Practical Time Series Forecasting with R - Galit Shmueli and Kenneth C. Lichtendahl Jr.} refers to examples of \emph{level} and \emph{trend} by Jim Flower (\url{http://techweb.bsu.edu/jcflowers1/rlo/trends.htm}). However, in his paragraph on ''Some Common Types of Trends" he says:

\textit{Trends are often shown graphically (as line graphs) with the level of a dependent variable on the y-axis and the time period on the x-axis. There are different types of trends, including the following:
\begin{itemize}
	\item constant
	\item linear
	\item exponential
	\item damped
\end{itemize}
}

In the example graphs given by Flower, he only uses the word ''trend", and he says nothing about ''level". This gives the impression that ''trend" here is used in the traditional modeling sense i.e. ''trend"=trend+level. In what follows I will only use the characteristic ''level" where needed and consider it to be equal to the average of the ''trend" in the modeling sense of the word. 

Time series can be constructed by \emph{adding} or \emph{multiplying} or \emph{combinations of adding and multiplying} these basic characteristics. \emph{Multiplication} is used when e.g. the amplitude of an seasonal characteristic is linked to the trend. Multiplying a trend with a sine function (seasonality) will increase the amplitude of the sine function when the trend is rising, and decrease the amplitude when the trend is falling. The same can be said for multiplying the trend and the error term: bigger errors when the trend rises, smaller when it falls.

This gives a lot of possible combinations. If we limit ourselves to a maximum of two seasonal elements, and either a completely additive or a completely multiplicative type we get $2^5 = 32$ combinations:

\begin{tabular}{r | c | c}
characteristic & 0 = not present & 1 = present \\
\hline
error & 0 & 1 \\
trend & 0 & 1 \\
seasonality 1 & 0 & 1 \\
seasonality 2 & 0 & 1 \\
type & A & M
\end{tabular}

However, some of these 32 possibilities are not interesting or self evident:
\begin{itemize}
  \item all multiplicative constructed time series where one of the characteristics is 0, have the same end result i.e. zero everywhere
  \item some models without an error term. Where we have only trend the graph is given by the polynomial $p(t)$ (or another function) and  forecasting is a, tentative, extrapolation
\end{itemize}

The interesting ones are:
\begin{enumerate}
  \item additively created time series:
    \begin{itemize}
      \item without error
        \begin{itemize}
          \item trend + seasonality 1
          \item trend + seasonality 1 + seasonality 2
        \end{itemize}
      \item with error
        \begin{itemize}
          \item error
          \item error + trend
          \item error + trend + seasonality 1
          \item error + trend + seasonality 1 + seasonality 2
        \end{itemize}
      \end{itemize}
  \item multiplicatively created time series:
    \begin{itemize}
      \item without error
        \begin{itemize}
          \item trend*seasonality 1
          \item trend*seasonality 1*seasonality 2
        \end{itemize}
      \item with error
        \begin{itemize}
          \item error*trend
          \item error*trend*seasonality 1
          \item error*trend*seasonality 1*seasonality 2
        \end{itemize}
    \end{itemize}
\end{enumerate}

\section{Constructed data sets}
\label{sec:constructed}
\begin{itemize}
  \item number of elements per time series: N=1000
  \item time step unit: 1
  \item error: random from normal distributiion with $\mu=0$, $\sigma=3$. Random number generator seed: 2019
  \item trend: polynomial. We restrict ourselves to models $y=p(x)$ with second degree polynomials (or first degree when $b_{2}=0$) with co\"{e}ffici\"{e}nts $b_{0}=0$, $b_{1}=0.01$ and $b_{2}=0.00005$. 
  \item seasonality 1: sine with amplitude=0 or $3\sigma$, period=$\frac{N}{9}$, phase=0. N not exact multiple of T1
  \item seasonality 2: sine with amplitude=0 or $\sigma$, period=$\frac{N}{39}$, phase=$\frac{\pi}{3}$. N not exaxt multiple of T2
  \item type: additive (''A") or multiplicative (''M")
\end{itemize}

<<echo=FALSE>>=
# Number of elements in the data set
N <- 1000
# Random number generator seed
set.seed(2019)
# error parameters
mu <- 0
sigma <- 3
# trend model parameters
b0 <- 0
b1 <- 0.01
b2 <- 0.00005
# level
level <- b0 + (b1/2)*N + (b2/3)*N^2
# seasonality 1
amp1 <- 3*sigma
teta1 <- 0
T1 <- N/9
# seasonality 2
amp2 <- sigma
teta2 <- pi/3
T2 <- N/39
# constant for combined additive/multiplicative time series
c <- 0.05
@

\newpage
\subsection{Additive constructed time series}
<<echo=FALSE>>=
#
# creating the additive constructed set
#
construct <- data.frame(t = seq(1:N),
                        error = rnorm(N, mean = mu, sd = sigma))
construct %>% mutate(trend = (b0 + b1*t +b2*t^2)) -> construct
construct %>% mutate(season1 = amp1*sin(2*pi*t/T1 + teta1)) -> construct
construct %>% mutate(season2 = amp2*sin(2*pi*t/T2 + teta2)) -> construct
@

<<label=additive, fig=TRUE, include=FALSE, echo=FALSE>>=
p1 <- ggplot(data = construct, aes(x = t)) +
  geom_line(aes(y = error), size = 1, color="red") +
  geom_hline(yintercept = mu, linetype = 2) +
  scale_y_continuous(limits= c(-25, 75)) +
  JT.theme
p2 <- ggplot(data = construct, aes(x = t)) +
  geom_line(aes(y = error + trend), size = 1, color="red") +
  geom_hline(yintercept = level, linetype = 2) +
  scale_y_continuous(limits= c(-25, 75)) +
  JT.theme
p3 <- ggplot(data = construct, aes(x = t)) +
  geom_line(aes(y = error + trend + season1), size = 1, color="red") +
  geom_hline(yintercept = level, linetype = 2) +
  scale_y_continuous(limits= c(-25, 75)) +
  JT.theme
p4 <- ggplot(data = construct, aes(x = t)) +
  geom_line(aes(y = error + trend + season1 + season2), size = 1, color="red") +
  geom_hline(yintercept = level, linetype = 2) +
  scale_y_continuous(limits= c(-25, 75)) +
  JT.theme
grid.arrange(p1, p2, p3, p4, nrow=2)
construct %>% mutate(addcon = error + trend + season1 + season2) -> construct
write_xlsx(data.frame(t = construct$t,addcon = construct$addcon), "Data/addcon.xlsx")
@

\begin{figure}
\includegraphics[width=0.7\textwidth]{PTSFR-additive}
\caption{Additive constructed time series}
\label{fig:additive}
\setfloatalignment{b}
\end{figure}

This time series is stored in the data frame \textit{construct} in column \textit{addcon}. It is also stored as an Excel-spreadsheet (\textit{addcon.xlsx}).

\subsection{Multiplicative constructed time series}

The definition of a multiplicative time series is:
\begin{equation}
y(t)=error(t)*trend(t)*seasonality(t)
\end{equation}

<<label=trend, fig=TRUE, include=FALSE, echo=FALSE>>=
p1 <- ggplot(data = construct, aes(x = t)) +
  geom_line(aes(y = trend*season1), size=1, color="red") +
  labs(title="") +
  JT.theme
p2 <- ggplot(data = construct, aes(x = t)) +
  geom_line(aes(y = trend*(season1 + season2), size=1, color="red")) +
  labs(title="") +
  JT.theme
grid.arrange(p1, p2, nrow=1)
@

<<label=error, fig=TRUE, include=FALSE, echo=FALSE>>=
p1 <- ggplot(data = construct, aes(x = t)) +
  geom_line(aes(y = error*trend), size=1, color="red") +
  labs(title="") +
  JT.theme
p2 <- ggplot(data = construct, aes(x = t)) +
  geom_line(aes(y = error*trend*season1, size=1, color="red")) +
  labs(title="") +
  JT.theme
grid.arrange(p1, p2, nrow=1)
@

But the multiplication means that $y(t)$ will always be equal to zero whenever one of the factors is zero\sidenote[][-1cm]{\url{https://fs.blog/2016/08/multiplicative-systems/}}. Let's look at some combinations of characteristics:
\begin{itemize}
  \item trend*season: this changes the amplitude of the seasonal component but it loses the overall trend (Figure~\ref{fig:trend})
    \begin{marginfigure}[0cm]
      \includegraphics[width=1\textwidth]{PTSFR-trend}
      \caption{}
      \label{fig:trend}
      \setfloatalignment{b}
    \end{marginfigure}
  \item error*trend and error*trend*season (Figure~\ref{fig:error})
  \begin{marginfigure}[0cm]
      \includegraphics[width=1\textwidth]{PTSFR-error}
      \caption{}
      \label{fig:error}
      \setfloatalignment{b}
    \end{marginfigure}
\end{itemize}

These time series constructions with multiplication do not give the desirede result: the trend only changes the amplitude of the seasonal component and/or the error term, but the trend itself is lost.

\newpage
\subsection{Combined additive and multiplicative constructed time series}
In the end to obtain the goal that both the amplitude of the seasonal terms and the amount of error are influenced by trend we have to combine additive and multiplicative elements in
\begin{equation}
y(t) = trend*(1 + c*(error + season1 + season2)) \quad c=0.05
\end{equation}
resulting in Figure~\ref{fig:multiplicative}.

<<label=multiplicative, fig=TRUE, include=FALSE, echo=FALSE>>=
ggplot(data = construct, aes(x = t)) +
  geom_line(aes(y = trend*(1 + c*(error + season1 + season2))), size=1, color="red") +
  labs(title="") +
  JT.theme
construct %>% mutate(combicon = trend*(1 + c*(error + season1 + season2))) -> construct
write_xlsx(data.frame(t = construct$t,combicon = construct$combicon), "Data/combicon.xlsx")
@

\begin{figure}
\includegraphics[width=0.5\textwidth]{PTSFR-multiplicative}
\caption{multiplicative constructed time series}
\label{fig:multiplicative}
\setfloatalignment{b}
\end{figure}

This time series is stored in the data frame \textit{construct} in column \textit{combicon}.

\section{Visualising Time Series}

A visualisation of the time series will give us a first, visual, indication of the nature of the series. We can look for \emph{missing} or \emph{extreme values}, \emph{unequal spacing} and \emph{patterns}.

A first and obvious plot is a line chart of the series as a function of time. \textit{ggplot} does this very well.

\newthought{Example: Ridership on Amtrak Trains}

<<label=Amdata,fig=TRUE,include=FALSE, echo=FALSE>>=
Amtrak.data <- read.csv("Data/Amtrak data.csv", sep=";", stringsAsFactors = FALSE)
# transforming the ''Month" column into a proper date variable
Amtrak.data %>%  mutate(day = myd(paste0(Month, "-01"))) -> Amtrak.data
Amtrak.data$t <- c(1:nrow(Amtrak.data))
Amtrak.baseplot <- ggplot(data=Amtrak.data) +
                          geom_line(aes(x=t, y=Ridership), size=1, color="red") +
                          labs(title="Amtrak Ridership data") +
                          JT.theme
Amtrak.baseplot
@

\begin{marginfigure}
\includegraphics[width=1\textwidth]{PTSFR-Amdata}
\caption{}
\label{fig:Amdata}
\setfloatalignment{b}
\end{marginfigure}


\newpage
Other operations are:
\subsection{Zooming}\index{visualization!zooming}
\label{subsec:zoomingin}
\newthought{Example} 
We use the time series of the number of vehicles in the Baregg Tunnel.

<<label=Baregg,fig=TRUE,include=FALSE, echo=FALSE>>=
Baregg.data <- read.csv("Data/BareggTunnelTraffic.csv", sep=",", stringsAsFactors = FALSE)
Baregg.data$t <- c(1:nrow(Baregg.data))
Baregg.data$day <- dmy(Baregg.data$day)
Baregg.baseplot <- ggplot(data=Baregg.data) +
  geom_line(aes(x=day, y=number), size=1, color="red") +
  labs(title="Number of vehicles in Baregg Tunnel") +
  JT.theme
p2 <- ggplot(data=Baregg.data) +
  geom_line(aes(x=day, y=number), size=1, color="red") +
  scale_x_date(limits = dmy(c("01-02-2004","31-05-2004")), breaks = dmy(c("01-02-2004", "01-03-2004", "01-04-2004", "01-05-2004", "31-05-2004")))  +
  labs(title="Number of vehicles in Baregg Tunnel\nzooming from 1-Feb-2004 to 31-May-2004") +
  JT.theme
p3 <- ggplot(data=Baregg.data) +
  geom_line(aes(x=day, y=number), size=1, color="red") +
  scale_x_date(limits = dmy(c("01-02-2004","08-02-2004")), breaks = dmy(c("01-02-2004", "02-02-2004", "03-02-2004", "04-02-2004", "05-02-2004", "06-02-2004", "07-02-2004", "08-02-2004")))  +
  labs(title="Number of vehicles in Baregg Tunnel\nzooming week of 1-Feb-2004") +
  JT.theme
grid.arrange(Baregg.baseplot, p2, p3, nrow=3)
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-Baregg}
\caption{}
\label{fig:Baregg}
\setfloatalignment{b}
\end{figure}

\newthought{addcon time series}
<<label=addconviz,fig=TRUE,include=FALSE, echo=FALSE>>=
p1 <- ggplot(data=construct, aes(x=t)) +
  geom_line(aes(y=addcon), size=1, color="red") +
  labs(title="Additive constructed model\nfull range") +
  JT.theme
p2 <- ggplot(data=construct, aes(x=t)) +
  geom_line(aes(y=addcon), size=1, color="red") +
  scale_x_continuous(limits = c(250,500), breaks = seq(250, 500, by=10))  +
  scale_y_continuous(limits = c(-10,40), breaks = seq(-10, 40, by=5))  +
  labs(title="Additive constructed model\nrange = 250 - 500") +
  JT.theme
p3 <- ggplot(data=construct, aes(x=t)) +
  geom_line(aes(y=addcon), size=1, color="red") +
  scale_x_continuous(limits = c(275,285), breaks = seq(275, 285, by=1))  +
  scale_y_continuous(limits = c(-5,15), breaks = seq(-5, 15, by=1))  +
  labs(title="Additive constructed model\nrange = 275 - 285") +
  JT.theme
grid.arrange(p1, p2, p3, nrow=3)
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-addconviz}
\caption{Various levels of detail}
\label{fig:addconviz}
\setfloatalignment{b}
\end{figure}

First we plot the whole range. Then we determine the range where we can see a few periods of a possible seasonal. Then we go in for some more detail(Figure~\ref{fig:addconviz}). Looking at the top graph I can see a seasonal pattern of approximatively 9 periods over a range from 0 to 1000. A first estimate for the seasonal component is that it has a period of 111 (not bad given that it is N/9). The other, more detailed graphs, do not improve on this.

\newthought{combicon time series}
<<label=combiconviz,fig=TRUE,include=FALSE, echo=FALSE>>=
p1 <- ggplot(data=construct, aes(x=t)) +
  geom_line(aes(y=combicon), size=1, color="red") +
  labs(title="Combined constructed model\nfull range") +
  JT.theme
p2 <- ggplot(data=construct, aes(x=t)) +
  geom_line(aes(y=combicon), size=1, color="red") +
  scale_x_continuous(limits = c(250,500), breaks = seq(250, 500, by=10))  +
  scale_y_continuous(limits = c(0,35), breaks = seq(0, 35, by=5))  +
  labs(title="Combined constructed model\nrange = 250 - 500") +
  JT.theme
p3 <- ggplot(data=construct, aes(x=t)) +
  geom_line(aes(y=combicon), size=1, color="red") +
  scale_x_continuous(limits = c(350,375), breaks = seq(350, 375, by=1))  +
  scale_y_continuous(limits = c(7.5,17.5), breaks = seq(7.5, 17.5, by=2.5))  +
  labs(title="Combined constructed model\nrange = 350 - 375") +
  JT.theme
grid.arrange(p1, p2, p3, nrow=3)
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-combiconviz}
\caption{Various levels of detail}
\label{fig:combiconviz}
\setfloatalignment{b}
\end{figure}

First we plot the whole range. Then we determine the range where we can see a few periods of a possible seasonal. Then we go in for some more detail (Figure~\ref{fig:combiconviz}). The top graph shows a seasonal pattern that starts around t=125 and ends at t=1000 after 8 periods. A first estimate for the seasonal component is that it has a period of (1000 - 125)/8=109,4 (again not bad given that it is N/9). The other, more detailed graphs, do not improve on this.

\newthought{Using plotly}

We can turn a \textit{ggplot} into a more flexible \textit{plotly}-object by using the \textbf{plotly}-package and the command \textit{ggplotly(p)} where p is the \textit{ggplot}-object. The full range combined constructed model gets a dynamic zoom window in this way. The result is not a pdf-file, so it cannot be loaded into the Sweave file. But it is a quick tool to do some zooming.

<<echo=FALSE>>=
ggplotly(p1)
@

\newthought{Time Series Objects in R and the parameter frequency}
We can do the visualising perfectly with \textit{ggplot}, which means that az yet we don't need to force the data into a time series object\index{time series object}. Doing so requires you to select a value for the parameter ''frequency"\index{time series!frequency}. First of all: it is \textbf{NOT} a frequency, but a period! Not Hz but sec (or other time scale). While this parameter can be chosen reasonably in certain situations (e.f. the Amtrak data where values are recorded monthly and we can assume a seasonality of 1 year = 12 months), it is (at present) not so clear for the Belragg data or the constructed time series (addcon and combicon). Rob Hyndman\sidenote{\url{https://otexts.org/fpp2/}} gives a few examples:
\begin{itemize}
	\item if you have monthly records, set the frequency to 12. The assumption is that the seasonality has a period of one year
	\item if you have weekly records, set the frequency to 52. Again the assumption is that the seasonality has a period of one year. However: a year is not exactly 52 weeks, but 52,14... However: most time series objects cannot have a non-integer frequency parameter
	\item if you have daily records, set the frequency to 7. Here we assume that many variables will show a weekly pattern (working days on the one hand, weekend on the other). So for the Belragg data is could be sensible to set the parameter frequency to 7.
\end{itemize}

\newthought{Amtrak Riderschip as a time series object}
<<label=Amtrakts, fig=TRUE, include=FALSE, echo=FALSE>>=
ridership.ts <- ts(Amtrak.data$Ridership, start = c(1991,1), end = c(2004,3), frequency = 12)
plot(ridership.ts, xlab = "Time", ylab = "Ridership", ylim = c(1300, 2300), bty = "l")
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-Amtrakts}
\caption{Amtrak Riderschip as a time series object}
\label{fig:Amtrakts}
\setfloatalignment{b}
\end{figure}

\newthought{Example: Baregg Number of Vehicles as a time series object}
The data are collected daily, starting on 2003-11-01. That is the start of week 44 in the year 2003. When we group them  in a weekly pattern (frequency=7) we can create the following time series object:

<<label=Bareggts, fig=TRUE, include=FALSE, echo=FALSE>>=
number.ts <- ts(Baregg.data$number, start = c(44,1), frequency = 7)
plot(number.ts, xlab = "Time", ylab = "Number of vehicles in Baregg tunnel", ylim = c(50000, 150000), bty = "l")
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-Bareggts}
\caption{Baregg Number of Vehicles as a time series object}
\label{fig:Bareggts}
\setfloatalignment{b}
\end{figure}

\newpage
\subsection{Change the scale}
Meant is: try a transformation. If a trend is exponential, a log-transformation\sidenote{all y-values have to be positive for such a transformation!} will lead to a linear relation, which is much easier to spot. Let's try this for the addcon-time series:

<<label=addconscale, fig=TRUE, include=FALSE, echo=FALSE>>=
ggplot(data=construct, aes(x=t)) +
  geom_line(aes(y=log(addcon)), size=1, color="red") +
  labs(title="Log-transformed addcom time series") +
  JT.theme
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-addconscale}
\caption{}
\label{fig:addconscale}
\setfloatalignment{b}
\end{figure}

\subsection{Add a trend line}
The advice here is to do this preliminary work in Excel (or Numbers). The function "add trendline" allows for a quick an dirty check.

In Figure~\ref{fig:addconnumberstrend}) and Figure~\ref{fig:combiconnumberstrend} we can see the trendlines with the highest $R^{2}$ values. For the ''addcon" time series this a second degree polynomial (surprise!), for the ''combicon" time series it is a power law approximation.

\begin{marginfigure}[-2cm]
\includegraphics[width=0.85\textwidth]{PTSFR-addconnumberstrend}
\caption{Trend with Numbers for addcon}
\label{fig:addconnumberstrend}
\setfloatalignment{b}
\end{marginfigure}

\begin{marginfigure}[0cm]
\includegraphics[width=0.85\textwidth]{PTSFR-combiconnumberstrend}
\caption{Trend with Numbers for combicon}
\label{fig:combiconnumberstrend}
\setfloatalignment{b}
\end{marginfigure}

\newpage
\subsection{Suppressing seasonality}

It is often easier to see trends in the data when seasonality is suppressed. We can use:

<<label=Amtrakagg, fig=TRUE, include=FALSE, echo=FALSE>>=
    Amtrak.data %>% group_by(year(day), quarter(day)) %>% summarise(mean(Ridership)) -> Amtrak.year
    names(Amtrak.year) <- c("year","quarter", "mean")
    Amtrak.year$yrqt <- paste(as.character(Amtrak.year$year), "-", Amtrak.year$quarter)
    Amtrak.year <- as.data.frame(Amtrak.year)
    ggplot(data = Amtrak.year) +
      geom_line(aes(x=yrqt, y= mean, group=1), size=1, color="red") +
      scale_y_continuous(limits= c(1000, 2500)) +
      theme(axis.text.x=element_text(angle=-45, hjust=0.001)) +
      labs(title="Quarterly average of riderschip") +
      JT.theme
@

<<label=Bareggagg, fig=TRUE, include=FALSE, echo=FALSE>>=
    Baregg.data %>% group_by(year(day), month(day)) %>% summarise(mean(number)) -> Baregg.month
    names(Baregg.month) <- c("year", "month","mean")
    Baregg.month$yrmt <- paste(as.character(Baregg.month$year), "-", Baregg.month$month)
    Baregg.month <- as.data.frame(Baregg.month)
    ggplot(data = Baregg.month) +
      geom_line(aes(x=yrmt, y= mean, group=1), size=1, color="red") +
      scale_y_continuous(limits= c(0, 125000)) +
      theme(axis.text.x=element_text(angle=-45, hjust=0.001)) +
      labs(title="Monthly average of numbers in Baregg tunnel") +
      JT.theme
@ 

\begin{itemize}
	\item a larger time scale: aggregating daily data into weeks or months, monthly data into years.
	\begin{enumerate}
	  \item \newthought{Amtrak data}: grouped per quarter (a year is too long)
      \begin{marginfigure}[0cm]
        \includegraphics[width=1\textwidth]{PTSFR-Amtrakagg}
        \caption{Amtrak}
        \label{fig:Amtrakagg}
        \setfloatalignment{b}
      \end{marginfigure}
    \item \newthought{Baregg data}
      \begin{marginfigure}
        \includegraphics[width=1\textwidth]{PTSFR-Bareggagg}
        \caption{Baregg tunnel}
        \label{fig:Bareggagg}
        \setfloatalignment{b}
      \end{marginfigure}
    \end{enumerate}
\end{itemize}

Looking at these results, this seems an awkward way of finding the trend. A lot depends on the choice of the level of aggregation. This does not seem to improve on finding a first idea of the trend by using the inbuilt functions of Excel (of Numbers).

\subsection{Interactive visualization}

Specific software (e.g. Tableau) offer the possibility of visualizing\index{visualization} data dynamically so that you can do a number of the above mentioned techniques (zooming, trending, suppressing seasonality) quickly. As mentioned in \ref{subsec:zoomingin} zooming\index{visualization!zooming} can quickly be done on an existing \textit{ggplot}-object by using the \textbf{plotly}-library. It is not ''Tableau" but it's quick and free.

The book offers more examples of visualization packages that allow you to zoom and filter across multiple data sets. Could be done with \textit{ggplot} and \textit{plotly} but with a great deal of effort!

\section{Data pre-processing}\index{data!quality}

\subsection{Exploratory Data Analysis}\index{EDA = Exploratory Data Analysis}
See EDA.pdf

Using the libraries \textit{broom} and \textit{funModelling} we can get a quick overview of the characteristics of the data frame:

\newthought{Number of rows, variables and head of the first row}
<<>>=
glimpse(Amtrak.data)
@

\newpage
\newthought{Metrics on data types, zeros, infinite numbers, missing values}\index{data!zeros}\index{data!missing values}\index{data!infinite numbers}
<<>>=
df_status(Amtrak.data)
@

\newthought{Distribution of the numerical value}
<<label=dist, fig=TRUE, include=FALSE, echo=FALSE>>=
plot_num(data=Amtrak.data)
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-dist}
\caption{distribution of numerical data}
\label{fig:dist}
\setfloatalignment{b}
\end{figure}

\newthought{Checking for extreme values}\index{data!extreme values}
<<>>=
describe(Amtrak.data$Ridership)
@

\subsection{Missing values}\index{data!missing values}

Missing values should turn up in the Exploratory Data Analysis and/or the visualization process. But they can be subtle. So we can detect them by finding the number of values there should be between the start and the end of the time series, and the number of values in the data:

<<echo=TRUE>>=
(interval(ymd(Amtrak.data$day[1]), ymd(Amtrak.data$day[nrow(Amtrak.data)])) %/% months(1) + 1) -
      nrow(Amtrak.data)
(interval(ymd(Baregg.data$day[1]),ymd(Baregg.data$day[nrow(Baregg.data)])) %/% days(1) + 1) -
      nrow(Baregg.data)
@

To check: let's construct a faulty time series: two weeks of data with the weekends missing:
<<>>=
faulty <- data.frame(day = seq(ymd('2012-04-09'), ymd('2012-04-22'), by = '1 day'),
                     y = seq(0:13)) 
faulty$weekday <- wday(faulty$day, label=TRUE)
faulty %>% dplyr::filter(as.character(weekday) != "Sun" &
                         as.character(weekday) !="Sat") -> faulty
(interval(ymd(faulty$day[1]),ymd(faulty$day[nrow(faulty)])) %/% days(1) + 1) - nrow(faulty)
@

Many time-series forecasting methods (such as ARIMA) cannot stand time-series with ''holes". Usually they are filled by \emph{imputation}\index{data!imputing missing values}

Other methods (e.g. linear or logistic regression models) are not influenced when there are missing values.

\subsection{Unequally spaced series}

Spacing can be checked with the \textit{diff}-command:

<<>>=
diff(Amtrak.data$day)
diff(Baregg.data$day)
@

\subsection{Extreme values}\index{data!extreme values}
Extreme values can be discarded if they are unique (e.g. the result of an earthquake) and unlikely to repeat itself in the forecasting period. When in doubt: do two forecasts: one with and one without the extreme values.

\subsection{Choice of time span}\index{forecast!horizon}
How far back in the past should we consider data to be relevant? Older data may have resulted from a different context and environment which are not relevant to the situation in the forecasting period. The rule is the \emph{we extend our time series backwards for only those periods where the environment is assumed to be similar to the forecast horizon}.

\chapter{Performance evaluation}

Using the same data to develop the forecasting model \textbf{and} to assess iets performance, we introduce \emph{bias}\index{bias}\index{performance!bias}. Our model will be fine tuned to this particular set of data. It will not only be adjusted to the systematic component of the data, but also to the noise. The metaphor of the tailor making a perfectly fitting suit for a customer is apt. The suit will not be useful when the customer gains or loses some weight. From this observation follows the idea of partitioning of the data set.

\section{Data partitioning}\index{partition}\index{data!partition}

\emph{Partitioning} is the splitting of the data set into two parts: forecasting models are built using the data in part 1. Using these models we make a forecast of part 2. The differences between the forecasted values and the true values of part 2 are used to measure the performance of different forecasting models.

\subsection{Partitioning of cross-sectional data}

Cross-sectional data\index{data!cross sectional} have values for different variables at one specific moment in time. Then we create three partitions: the \emph{training set}\index{training set}, a \emph{validation set}\index{validation set} and a \emph{test set}\index{test set}. The partitioning is done \emph{at random}: we (the computer) picks at random which data elements go into each set.

\subsection{Temporal partitioning}
When our goal is forecasting, we want to predict the future. Therefore there is no \emph{test set} available. Partitioning is \emph{random} because this would create \emph{missing values}\index{missing values} into both the \emph{training set} and the \emph{validation set}, and it would miss the logical temporal sequence of past and future. Visualization of actual an predicted series for both sets is interesting, because it could indicate overfitting when the predicted and actual series are close to each other in the training period, but not in the validation period.

\subsection{Joining partitions for forecasting}
Before attempting to predict the, unknown, future values we will use our forecasting methods on the whole time series: the joined training and validation sets. This has the following advantages:
\begin{itemize}
	\item the validation period is the most recent and therefore more in tune with the future we wish to forecast
	\item more data can lead to better estimation of model parameters
	\item using only the training set will require that the forecasting must be done further in the future
\end{itemize}

\subsection{Choosing the validation period}
The length of the validation period is determined by the forecast horizon\index{forecast!horizon}. A shorter validation period does not give good information on the quality of our forecasting model at the end of the forecasting period. A longer validation period reduces the amount of data in the training set, and specifically the most recent data.

If we were to make a forecast on the Amtrak data for the next 3 years (=36 months) we would set the validation period nValid to 36. Consequently the training set (nTrain) would be limited to the whole range of Amtrak.data (=nrow(Amtrak.data)) minus nValid. We will be using the times series object that was created before. We include a polynomial regression for the trend. This gives us the following code (you need the library \textbf{forecast}):

<<label=Validationperiod, fig=TRUE, include=FALSE, echo=FALSE>>=
nValid <- 36
nTrain <- length(ridership.ts) - nValid
train.ts <- window(ridership.ts, 
                   start = c(1991, 1), 
                   end = c(1991, nTrain))
valid.ts <- window(ridership.ts, 
                   start = c(1991, nTrain + 1), 
                   end = c(1991, nTrain + nValid))
ridership.lm <- tslm(train.ts ~ trend + I(trend^2))
ridership.lm.pred <- forecast(ridership.lm, h = nValid, level = 0)
plot(ridership.lm.pred, 
     ylim = c(1300, 2600), 
     ylab = "Ridership", 
     xlab = "Time", 
     bty = "l",
     xaxt = "n",
     xlim = c(1991, 2006.25),
     main = "",
     lty = 2)
axis(1, at = seq(1991, 2006, 1), labels = format(seq(1991, 2006,1)))
lines(ridership.lm$fitted, lwd = 2)
lines(valid.ts)
abline(v = 1991 , col="blue")
abline(v = 1991 + (nTrain)/12 , col="blue")
abline(v = 1991 + (nTrain + nValid)/12, col="blue")
text(x = 1996, y = 2400, labels="Training")
arrows(x0 = 1991, y0 = 2300, 
       x1 = 1991 + nTrain/12, y1 = 2300, 
       code = 3, length = 0.1)
text(x = 2002.6, y = 2400, labels="Validation")
arrows(x0 = 1991 + (nTrain)/12, y0 = 2300, 
       x1 = 1991 + (nTrain + nValid)/12, y1 = 2300, 
       code = 3, length = 0.1)
text(x = 2005.5, y = 2400, labels="Future")
arrows(x0 = 1991 + (nTrain + nValid + 1)/12, y0 = 2300, 
       x1 = 1991 + (nTrain + nValid + 36)/12, y1 = 2300, 
       code = 2, length = 0.1)
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-Validationperiod}
\caption{Amtrak Riderschip validation period}
\label{fig:Validationperiod}
\setfloatalignment{b}
\end{figure}

\section{Naive forecasts}\index{forecast!naive}

\emph{Naive forecasts} have the merit that they are simple, easy to understand and, in some cases, are sufficient to achieve the forecasting goal. If not, their performance can by used as a benchmark for more complicated model.

\begin{itemize}
	\item For a time series that does not show substantial seasonality, or if the seasonality has a period that is much greater than the forecast horizon, the \emph{naive} forecast is \emph{the most recent value} of the time series. The k-step-ahead forecast is:
    \begin{equation}
    	F_{t_{latest}+k} = y_{t_{latest}} = constant
    \end{equation}
        Graphically it means that the forecasted values are on a horizontal line for the whole forecasting horizon.
  \item For a time series with seasonality with known period T that extends into the forecasting horizon the \emph{naive} forecast is \emph{the value - 1 period} of the time series. The k-step-ahead forecast is:
    \begin{equation}
    	F_{t_{latest} + 1} = y_{t_{latest} + 1 - T}
    \end{equation}
        Graphically it means that the forecasted values are a repeat of the last known period
\end{itemize}

It is the equivalent of the principle of \emph{persistency} that is used as a naive forecast in weather forecasting.

\section{Measuring predictive accuray}\index{accuracy}\index{forecast!accuracy}

\emph{Predictive accuracy} is not the same as \emph{Goodness of Fit}. The latter measure how well the model fits the data. In forecasting however we are interested in the question of how wel the model, based on the \emph{training set}, predicts the values in the \emph{validation set}.

\subsection{Metrics for prediction accurary}\index{forecast!accuracy metrics}

The \emph{forecast error}\index{forecast!error}\index{forecast!accuracy} or \emph{residual}\index{residual} is the difference between the actual value $y_{t}$ and the forecast value $F_{t}$:
\begin{equation}
	e_{t}=y_{t} - F_{t}
\end{equation}

The validation set has time indexes $t_{start + n_{Train}} + i$, with $i=1 \ldots n_{Valid}$, than we can define the following metrics:
\begin{itemize}
	\item Mean Absolute Error (or Deviation) (MAE of MAD)
	  \begin{equation}
	    MAE = \frac{1}{n_{Valid}}\sum_{1}^{n_{Valid}} |e_{i}|
    \end{equation}
  \item Average Error: idem but without the absolute values. Gives an idea of under- or overprediction
     \begin{equation}
	    MAE = \frac{1}{n_{Valid}}\sum_{1}^{n_{Valid}} e_{i}
    \end{equation}
  \item Mean Absolute Percentage Error: relative absolute error. Used when comparing performance across time series with different scales
     \begin{equation}
	    MAPE = \frac{1}{n_{Valid}}\sum_{1}^{n_{Valid}}| \frac{e_{i}}{y_{i}}*100 | 
    \end{equation}
  \item Root Mean Square Error
    \begin{equation}
	    RMSE = \sqrt{ \frac{1}{n_{Valid}}\sum_{1}^{n_{Valid}} e_{i}^{2} }
    \end{equation}
\end{itemize}

Applying these principles and definitons to the Amtrak.data time series we get:

<<>>=
forecast.error <- valid.ts - ridership.lm.pred$mean
MAE <- (1/nValid)*sum(abs(forecast.error))
MAE
AvEr <- (1/nValid)*sum(forecast.error)
AvEr
MAPE <- (1/nValid)*sum(abs(forecast.error*100/valid.ts))
MAPE
RMSE <- sqrt((1/nValid)*sum(forecast.error^2))
RMSE
@

or we immediately use the \textit{accuracy}-function\index{accuracy}\index{forecast!accuracy}:
<<>>=
accuracy(ridership.lm.pred$mean, valid.ts)
@

\subsection{Zero counts}
Calculating MAPE is impossible when $y_{i}=0$. When you cannot reasonably exclude these values, another metric is used: the MASE = Mean Absolute Scaled Error. It devides the model MAE by the MAE of the naive forecast \emph{on the training set}:

\begin{equation}
	MASE = \frac{validation MAE}{training MAE of naive forecast} = \frac{\frac{1}{n_{Valid}}\sum_{1}^{n_{Valid}} |e_{i}|}{\frac{1}{n_{Train}-1}\sum_{1}^{n_{Train}} |y_{t-1} - y_{t}|}
\end{equation}

Values of MASE lower higher than 1 indicate a poorer performance than the naive forecast.

\subsection{Forecast accuracy vs. profitability}\index{forecast!accuracy}\index{forecast!cost}
Some metrics (e.g. MAPE and RMSE) inflate the effect of large errors. But the effects, or costs, of the error should also come into consideration. Sometimes the cost of a forecast error is big for positive errors, but not for negative ones. Sometimes the cost is proportional to the error, but the relation can be non-linear: it can be that the cost is the same once a certain error treshold is crossed.A measure of profitability could be very usefull when choosing the forecast method.

\section{Evaluating forecast uncertainty}

\subsection{Distribution of forecast errors}

We should go beyond the clustered metrics like MAPE or RMSE. It is interesting to examine the distribution of forecasting errors: are there extremely low or high errors. For example: we have constructed a second degree polynomial trend model (ridership.lm) based on the training set. Using this model we can predict the ridership data both for the training set and for the validation set (ridership.lm.pred).

<<label=Residuals, fig=TRUE, include=FALSE, echo=FALSE>>=
par(mfrow = c(2,1))
plot(ridership.lm.pred$residuals, 
     ylim = c(-400, 400), 
     ylab = "Residuals", 
     xlab = "Time", 
     bty = "l",
     xaxt = "n",
     xlim = c(1991, 2006.25),
     main = "",
     lty = 1)
axis(1, at = seq(1991, 2006, 1), labels = format(seq(1991, 2006,1)))
lines(valid.ts - ridership.lm.pred$mean)
abline(v = 1991 , col="blue")
abline(v = 1991 + (nTrain)/12 , col="blue")
abline(v = 1991 + (nTrain + nValid)/12, col="blue")
text(x = 1996, y = 350, labels="Training")
arrows(x0 = 1991, y0 = 300, 
       x1 = 1991 + nTrain/12, y1 = 300, 
       code = 3, length = 0.1)
text(x = 2002.6, y = 350, labels="Validation")
arrows(x0 = 1991 + (nTrain)/12, y0 = 300, 
       x1 = 1991 + (nTrain + nValid)/12, y1 = 300, 
       code = 3, length = 0.1)
text(x = 2005.5, y = 350, labels="Future")
arrows(x0 = 1991 + (nTrain + nValid + 1)/12, y0 = 300, 
       x1 = 1991 + (nTrain + nValid + 36)/12, y1 = 300, 
       code = 2, length = 0.1)
hist(ridership.lm.pred$residuals, 
     ylab = "Frequency", 
     xlab = "Residuals", 
     bty = "l", 
     main = "")
par(mfrow=c(1,1))
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-Residuals}
\caption{Amtrak Riderschip Residuals}
\label{fig:Residuals}
\setfloatalignment{b}
\end{figure}

\subsection{Prediction intervals}
A \emph{prediction intervel}\index{prediction interval} gives a confidence interval\index{confidence interval} of the forecast. If the forecast error is normally distributed we can use standard techniques to calculate the confidence intervals. For forecast models where the error does not have a normal distribution (e.g. the Amtrak second order polynomial model with residuals given in Figure\ref{fig:Residuals}) we can use the 5th and 95th quantile from the residuals to construct a confidence interval.

\subsection{Prediction cones}
Certain forecast models will have confidence intervals that become larger when our forecasts are made for points more and more into the future.

\section{Advanced Data Partitioning: roll-forward validation}\index{roll-forward validation}
When we partition the time series into one, fixed, training set and one, fixed, validation set then our forecasting method will give us only one prediction trajectory into the future.

When using \emph{roll-forward validation} we have a number of partitions where we start from one training/validation combination, and make new training/validation sets by extending the training set with one period, and consequently reduce the validation set with the same period. For example: with the Amtrak time series we started with a validation set that had 36 months. With \emph{roll-forward validation} we create 36 partitionings where the validation set has 36, 35, 34 ... 1 months of data, and the training set grows each time with one month. When we keep the forecast horizon equal to the length of the validation set, we will get forecasts for 36 months from the first partition, 35 months from the second, ending with 1 month forecast from the last partition. This means that in the end we will have 36 forecasts for month 1 into the future, 35 forecasts for mont 2 into the future and 1 forecast for month 36 into the future. Roll-forward partitioning is also a good choice when our time series is dynamic and we get new information on a steady basis. Our time series grows in time, and we can enlarge the training set and reduce the validation set if the forecast date(s) remain the same.

For example: if we use a very simple forecast model (naive, only trend), then for the fixed partition our model for the next 36 months is a constant equal to the last value of $y_{i}$ in the training set.

<<>>=
forecast.fixed <- rep(last(train.ts), nValid)
forecast.fixed.ts <- ts(forecast.fixed, 
                        start = c(2001,4), 
                        end = c(2004, 3), 
                        freq = 12)
@

In a \emph{roll-forward validation} we 

\begin{itemize}
	\item start with a 36 period validation set, which gives 36 forecasts for the future equal to the last value in the training set
	\item continue with a 35 period validation set, resulting in 35 forecast equal to the last value of the training set \textbf{+ 1}. This is the first value in the fixed validation set
	\item a 34 period validation set, with 34 forecasts equal to the second value in the fixed validation set
	\item ...
	\item a 1 period validation set, with 1 forecast equal to the last value \textbf{but one} in the fixed validation set
\end{itemize}

The forecasts are thus given by:
<<>>=
forecast.roll.fwd <- append(last(train.ts), valid.ts[1:(length(valid.ts)-1)])
forecast.roll.fwd.ts <- ts(forecast.roll.fwd, 
                           start = c(2001,4), 
                           end = c(2004, 3), 
                           freq = 12)
@

<<label=rollforward, fig=TRUE, include=FALSE, echo=FALSE>>=
plot(ridership.lm.pred, 
     ylim = c(1300, 2600), 
     ylab = "Ridership", 
     xlab = "Time", 
     bty = "l",
     xaxt = "n",
     xlim = c(1991, 2006.25),
     main = "",
     lty = 1)
axis(1, at = seq(1991, 2006, 1), labels = format(seq(1991, 2006,1)))
lines(ridership.lm$fitted, lwd = 2)
lines(valid.ts, lty = 1)
lines(forecast.fixed.ts, lty = 3, col="red")
lines(forecast.roll.fwd.ts, lty = 2, col="red")
abline(v = 1991 , col="blue")
abline(v = 1991 + (nTrain)/12 , col="blue")
abline(v = 1991 + (nTrain + nValid)/12, col="blue")
text(x = 1996, y = 2400, labels="Training")
arrows(x0 = 1991, y0 = 2300, 
       x1 = 1991 + nTrain/12, y1 = 2300, 
       code = 3, length = 0.1)
text(x = 2002.6, y = 2400, labels="Validation")
arrows(x0 = 1991 + (nTrain)/12, y0 = 2300, 
       x1 = 1991 + (nTrain + nValid)/12, y1 = 2300, 
       code = 3, length = 0.1)
text(x = 2005.5, y = 2400, labels="Future")
arrows(x0 = 1991 + (nTrain + nValid + 1)/12, y0 = 2300, 
       x1 = 1991 + (nTrain + nValid + 36)/12, y1 = 2300, 
       code = 2, length = 0.1)
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-Rollforward}
\caption{Amtrak Riderschip roll-forward validation}
\label{fig:Rollforward}
\setfloatalignment{b}
\end{figure}

However, this is not a good example! Of course our forecast in the validation period is better because we use the actual data from that period! We could have made it a perfect fit by choosing the actual last data point instead of the data point minus 1. A better example would be to determine the second degree polynomial model based on the dynamic training set and calculate the forecasting metrics using the dynamic validation set.

\section{Example: comparing two models}

Using the Amtrak ridership data we compare two forecast methods: naive and seasonal naive. For both we use fixed partitioning and roll-forward partitioning. \textsf{R} packages like \textit{forecast} have functions \textit{naive} and \textit{snaive} that generate the forecast models.

\subsection{Fixed partitioning}

<<>>=
fixed.nValid <- 36
fixed.nTrain <- length(ridership.ts) - fixed.nValid
train.ts <- window(ridership.ts, 
                   start = c(1991, 1),
                   end = c(1991, fixed.nTrain))
valid.ts <- window(ridership.ts, 
                   start = c(1991, fixed.nTrain + 1), 
                   end = c(1991, fixed.nTrain + fixed.nValid))
naive.pred <- naive(train.ts, h = fixed.nValid)
snaive.pred <- snaive(train.ts, h = fixed.nValid)
accuracy(naive.pred, valid.ts)
accuracy(snaive.pred, valid.ts)
@

\subsection{Roll-forward partitioning}

<<>>=
fixed.nValid <- 36
fixed.nTrain <- length(ridership.ts) - fixed.nValid
stepsAhead <- 1
error.naive <- rep(0, fixed.nValid - stepsAhead + 1)
percent.error.naive <- rep(0, fixed.nValid - stepsAhead + 1)
error.snaive <- rep(0, fixed.nValid - stepsAhead + 1)
percent.error.snaive <- rep(0, fixed.nValid - stepsAhead + 1)
for  (j in fixed.nTrain:(fixed.nTrain + fixed.nValid - stepsAhead)) {
  train.ts <- window(ridership.ts, 
                    start = c(1991, 1),
                    end = c(1991, j))
  valid.ts <- window(ridership.ts, 
                   start = c(1991, j + stepsAhead), 
                   end = c(1991, fixed.nTrain + fixed.nValid))
  naive.pred <- naive(train.ts, h = stepsAhead)
  snaive.pred <- snaive(train.ts, h = stepsAhead)
  error.naive[j - fixed.nTrain + 1] <- valid.ts - naive.pred$mean[stepsAhead]
  percent.error.naive[j - fixed.nTrain + 1] <- error.naive[j - fixed.nTrain + 1]/valid.ts
  error.snaive[j - fixed.nTrain + 1] <- valid.ts - snaive.pred$mean[stepsAhead]
  percent.error.snaive[j - fixed.nTrain + 1] <- error.snaive[j - fixed.nTrain + 1]/valid.ts
}
print("naive")
mean(abs(error.naive))
sqrt(mean(error.naive^2))
mean(abs(percent.error.naive))
print("snaive")
mean(abs(error.snaive))
sqrt(mean(error.snaive^2))
mean(abs(percent.error.snaive))
@

\chapter{Forecasting methods: overview}

\section{Model-based vs. Data-driven methods}\index{forecast!model-based methods}\index{forecast!data-driven methods}\index{model-based methods}\index{data-driven methods}\index{methods!model-based}\index{methods!data-driven}

\emph{Model-based methods} use statistical, mathematical or other scientific models to approximate a time series. E.g. based on some prior knowledge of the subject we can reasonably assume a linear relation in time: $y(t)=b_{0} + b_{1}t$. The training data and a statistical method (linear regression) are used to determine the parameters $b_{0}$ and $b_{1}$ and the confidence interval on the predicted (forecasted) values. Model-based methods are very usefull when the data set is small: we do not need a lot of data points when we ''know" the type of relationship. These methods are better when the pattern is \emph{global}.

\emph{Data-driven methods} find a pattern within the data, without presuming the know the relation of the data with time. A typical example is finding the trend using a \emph{moving average} or other smoothing technique. Naive methods are also only based on the data. Data-driven methods usually need more data points, but they have the advantage of being less user dependent and they adapt automatically when additional data become available. These methods adapt themselves to \emph{local} patterns.

\section{Extrapolation methods, Econometric methods, External information}
\emph{Extrapolation methods}\index{extrapolation methods} such as forecasting learn from history and forecast the future. Even when the forecast is done for multiple time series, each serie is forecast based on its own history.

\emph{Econometric methods}\index{econometric methods} take into account the cross-correlation between series from a causal standpoint. Information from one or more other series are used as input for the forecast of the time series under study.\emph{Multivariate time series}\index{multivariate time series}\index{time series!multivariate} directly model the cross-correlations between sets of series.

Using \emph{external information} to improve a forecast is a third method. The golder rule in this case is to \emph{only use values that are available at the time of prediction}.

\section{Manual vs. Automated Forecasting}\index{forecast!manual}\index{forecast!automatic}

\emph{Model-based methods}\index{methods!model-based} are usually built on assumptions and therefore they require constant attention to make sure that the assumptions are still valid. They fall within the domain of \emph{manual forecasting}. \emph{Data-driven methods}\index{methods!data-driven} learn from the data, and are more easily automated.

\section{Combining methods and Ensembles}

\emph{Combining}\index{methods!combining} the results of different forecasting methods often lead to better results than relying on only one method. A two step action where method one makes a forecast based on the original time series, and method two uses the forecast errors to predict future forecast errors and in this way ''corrects" the first method.

\emph{Ensembles}\index{ensemble}\index{methods!ensembles} is the name for multiple forecasts generated by different methods starting from the same original time series. A weighed average of the forecast results is the final forecast. Different forecasting methods capture different aspects of the time series, and a combination of the results is often better than relying on only one method.

\emph{Using different time series} that measure the variable of interest can also lead to better forecasts.

\emph{Combining methods} and \emph{Ensembles} have a number of disadvantages:
\begin{itemize}
	\item costly
	\item require knowledge of the methods used
	\item good procedures! Agree beforehand how the different elements will be weighed.
\end{itemize}

\chapter{Smoothing methods}

\emph{Smoothing methods}\index{smoothing}\index{methods!smoothing} are \emph{data-driven} and therefore mostly \emph{automated}. However, care must be taken in the choice of the \emph{smoothing constants}, and this is \emph{manual} work.

\section{Introducton}
\emph{Smoothing methods}\index{methods!smoothing} estimate time series components directly from the data, \emph{without predetermined structure} e.g. a linear evolution in time. When the time series components (e.g. the trend) changes in time, a smoothing method will adapt to this change. \emph{Smoothing methods} filter out the noise\index{noise}, and doing so can reveal the underlying pattern (trend and/or seasonality).

\section{Moving Average}

The \emph{moving average smoother}\index{moving average} comes in two flavors: \emph{centered}\index{smoothing!moving average!centered}\index{CMA} and \emph{trailing}\index{smoothing!moving average!trailing}\index{TMA}. In both cases we look at the time series through a \emph{window} with width $w$ = the number of consecutive values we take into account.

\subsection{Centered moving average}\index{moving average!centered}

\begin{equation}
	MA_{t} = \left( y_{t-(w-1)/2} + \ldots + y_{t-1} + y_{t} + y_{t+1} + \ldots  + y_{t+(w-1)/2} \right)/w
\end{equation}

\textbf{The width of the window $w$ is chosen equal to the period of the time series}. However: in engineering and science this period (and not the frequency!!!) is usually not known. And idealy $w$ should be uneven, which is not always the case! The \textit{filter}-function is useful. \sidenote{If the window $w$ is even, the function description says: 'more of the filter is forward in time than backward". This means that for $w=5$ the filter will give ''NA" for the first two calculations. For the third calculation it will use values 1, 2, ... 5, which are two values backward (1 and 2), two values forward (4 and 5) and the value at the data point itself (3). For $w=6$ it will give ''NA" for the first two calculations. For the third calculation it will use values 1, 2, ... 6, which are two values backward (1 and 2), three values forward (4, 5 and 6) and the value at the data point itself (3).} The function $f_{w}$ is a weighing function. For a uniform distribution $f_{w}=\frac{1}{w}$ and the result will be the average of the $w$ data points. Experimenting with the value of $w$ can get us close to eliminating the seasonal effect.

<<label=centeredsmooth, fig=TRUE, include=FALSE, echo=FALSE>>=
w <- 12
fw <- rep(1/w, w)
p1 <- ggplot(data=Amtrak.data) +
      geom_line(aes(x = t, y = Ridership), size = 0.5, color ="red") +
      geom_line(aes(x = t, y = stats::filter(Ridership, fw, sides=2)), color= "blue", size = 1) +
      labs(title="Amtrak Ridership data\ncentered smooth") +
      JT.theme
w1 <- 10
fw1 <- rep(1/w1, w1)
w2 <- 50
fw2 <- rep(1/w2, w2)
w3 <- 100
fw3 <- rep(1/w3, w3)
p2 <- ggplot(data=construct) +
      geom_line(aes(x = t, y = addcon), size = 0.5, color ="red") +
      geom_line(aes(x = t, y = stats::filter(addcon, fw1, sides=2)), color= "lightblue", alpha = 0.5, size = 1) +
      annotate("text", x= 10, y = 75, label = "w=10", color = "lightblue", hjust = 0) +
      geom_line(aes(x = t, y = stats::filter(addcon, fw2, sides=2)), color= "blue", alpha = 0.5, size = 1) +
      annotate("text", x= 10, y = 65, label = "w=50", color = "blue", hjust = 0) +
      geom_line(aes(x = t, y = stats::filter(addcon, fw3, sides=2)), color= "darkblue", alpha = 0.5, size = 1) +
      annotate("text", x= 10, y = 55, label = "w=100", color = "darkblue", hjust = 0) +
      labs(title="addcon\ncentered smooth") +
      JT.theme
p3 <- ggplot(data=construct) +
      geom_line(aes(x = t, y = combicon), size = 0.5, color ="red") +
      geom_line(aes(x = t, y = stats::filter(combicon, fw1, sides=2)), color= "lightblue", alpha = 0.5, size = 1) +
      annotate("text", x= 10, y = 75, label = "w=10", color = "lightblue", hjust = 0) +
      geom_line(aes(x = t, y = stats::filter(combicon, fw2, sides=2)), color= "blue", alpha = 0.5, size = 1) +
      annotate("text", x= 10, y = 65, label = "w=50", color = "blue", hjust = 0) +
      geom_line(aes(x = t, y = stats::filter(combicon, fw3, sides=2)), color= "darkblue", alpha = 0.5, size = 1) +
      annotate("text", x= 10, y = 55, label = "w=100", color = "darkblue", hjust = 0) +
      labs(title="combicon\ncentered smooth") +
      JT.theme
grid.arrange(p1, p2, p3, nrow=1)
@

\begin{figure*}
\includegraphics[width=0.85\textwidth]{PTSFR-centeredsmooth}
\caption{Centered moving average}
\label{fig:centeredsmooth}
\setfloatalignment{b}
\end{figure*}

The graphs obtained by Centered Moving Average (Figure~\ref{fig:centeredsmooth}) give a good idea of the trend. However they lose information at the start, but more crucially for forecasting, also at the end.

\subsection{Trailing Moving Average}
When wanting to forecast we cannot use ''future" values, because they to not exist. In that case we use \emph{Trailing Moving Average}\index(moving average!trailing):
\begin{equation}
	F_{t+k} = (y_{t} + y_{t-1} + \ldots + y_{t-w+1})/w
\end{equation}

In the \textit{filter}-function we only have to set the parameter \emph{sides} equal to 1.

<<label=trailingsmooth, fig=TRUE, include=FALSE, echo=FALSE>>=
w <- 12
fw <- rep(1/w, w)
p1 <- ggplot(data=Amtrak.data) +
      geom_line(aes(x = t, y = Ridership), size = 0.5, color ="red") +
      geom_line(aes(x = t, y = stats::filter(Ridership, fw, sides=1)), color= "blue", size = 1) +
      labs(title="Amtrak Ridership data\ntrailing smooth") +
      JT.theme
w1 <- 10
fw1 <- rep(1/w1, w1)
w2 <- 50
fw2 <- rep(1/w2, w2)
w3 <- 100
fw3 <- rep(1/w3, w3)
p2 <- ggplot(data=construct) +
      geom_line(aes(x = t, y = addcon), size = 0.5, color ="red") +
      geom_line(aes(x = t, y = stats::filter(addcon, fw1, sides=1)), color= "lightblue", alpha = 0.5, size = 1) +
      annotate("text", x= 10, y = 75, label = "w=10", color = "lightblue", hjust = 0) +
      geom_line(aes(x = t, y = stats::filter(addcon, fw2, sides=1)), color= "blue", alpha = 0.5, size = 1) +
      annotate("text", x= 10, y = 65, label = "w=50", color = "blue", hjust = 0) +
      geom_line(aes(x = t, y = stats::filter(addcon, fw3, sides=1)), color= "darkblue", alpha = 0.5, size = 1) +
      annotate("text", x= 10, y = 55, label = "w=100", color = "darkblue", hjust = 0) +
      labs(title="addcon\ntrailing smooth") +
      JT.theme
p3 <- ggplot(data=construct) +
      geom_line(aes(x = t, y = combicon), size = 0.5, color ="red") +
      geom_line(aes(x = t, y = stats::filter(combicon, fw1, sides=1)), color= "lightblue", alpha = 0.5, size = 1) +
      annotate("text", x= 10, y = 75, label = "w=10", color = "lightblue", hjust = 0) +
      geom_line(aes(x = t, y = stats::filter(combicon, fw2, sides=1)), color= "blue", alpha = 0.5, size = 1) +
      annotate("text", x= 10, y = 65, label = "w=50", color = "blue", hjust = 0) +
      geom_line(aes(x = t, y = stats::filter(combicon, fw3, sides=1)), color= "darkblue", alpha = 0.5, size = 1) +
      annotate("text", x= 10, y = 55, label = "w=100", color = "darkblue", hjust = 0) +
      labs(title="combicon\ntrailing smooth") +
      JT.theme
grid.arrange(p1, p2, p3, nrow=1)
@

\begin{figure*}
\includegraphics[width=1\textwidth]{PTSFR-trailingsmooth}
\caption{}
\label{fig:trailingsmooth}
\setfloatalignment{b}
\end{figure*}

The accuracy of a naive forecast of the Amtrak ridership data based on the smoothed data in the training set is given by
<<>>=
accuracy(naive(stats::filter(train.ts, fw, sides=1), h = nValid), valid.ts)
@

<<label=naivetrailingsmooth, fig=TRUE, include=FALSE, echo=FALSE>>=
nValid <- 36
nTrain <- length(ridership.ts) - nValid
train.ts <- window(ridership.ts, 
                   start = c(1991, 1), 
                   end = c(1991, nTrain))
valid.ts <- window(ridership.ts, 
                   start = c(1991, nTrain + 1), 
                   end = c(1991, nTrain + nValid))
w <- 12
fw <- rep(1/w, w)
ridership.trailing <- stats::filter(train.ts, fw, sides=1)
naive.pred.trailing <- naive(ridership.trailing, h = nValid)
plot(train.ts, 
     ylim = c(1300, 2200), 
     ylab = "Ridership", 
     xlab = "Time", 
     bty = "l",
     xaxt = "n",
     xlim = c(1991, 2006.25),
     main = "",
     lty = 1)
axis(1, at = seq(1991, 2006, 1), labels = format(seq(1991, 2006,1)))
lines(ridership.trailing, col = "red", lty = 2, lwd = 2)
lines(valid.ts, lty = 1)
lines(naive.pred.trailing$mean, col = "red", lty = 2, lwd = 2)
abline(v = 1991 , col="blue")
abline(v = 1991 + (nTrain)/12 , col="blue")
abline(v = 1991 + (nTrain + nValid)/12, col="blue")
text(x = 1996, y = 2190, labels="Training")
arrows(x0 = 1991, y0 = 2170, 
       x1 = 1991 + nTrain/12, y1 = 2170, 
       code = 3, length = 0.1)
text(x = 2002.6, y = 2190, labels="Validation")
arrows(x0 = 1991 + (nTrain)/12, y0 = 2170, 
       x1 = 1991 + (nTrain + nValid)/12, y1 = 2170, 
       code = 3, length = 0.1)
text(x = 2005.5, y = 2190, labels="Future")
arrows(x0 = 1991 + (nTrain + nValid + 1)/12, y0 = 2170, 
       x1 = 1991 + (nTrain + nValid + 36)/12, y1 = 2170, 
       code = 2, length = 0.1)

@

\begin{figure}
\includegraphics[width=1\textwidth]{PTSFR-naivetrailingsmooth}
\caption{Amtrak Ridership with naive forecast based on trailing moving average}
\label{fig:naivetrailingsmooth}
\setfloatalignment{b}
\end{figure}

From Figure~\ref{fig:naivetrailingsmooth} we can see that this model does not predict the actual values very well in the validation period. The accuracy of this naive forecast of the Amtrak ridership data based on the smoothed data in the training set is given by
<<>>=
accuracy(naive(stats::filter(train.ts, fw, sides=1), h = nValid), valid.ts)
@

\subsection{Choosing window width w}

The choice of the window width $w$ is a balancing act between \emph{under-smoothing}\index{smoothing!under-smoothing} and \emph{over-smoothing}\index{smoothing!over-smoothing}. Narrow windows will show local trends, broader windows will show the general trend. Domain knowledge should give some indication.

\subsection{What does averaging do to trend, seasonal and error components?}

\newthought{Trend}\index{averaging!effect on trend}

The average of a linear trend, is the trend itself. Centered averaging will give you the same line, trailing averaging results in a time shift.

<<label=influenceaverageontrend, fig=TRUE, include=FALSE, echo=FALSE>>=
inf.trend <- data.frame(t = seq(0, 100, by = 1), trend = 0, smoothcent = 0, smoothtrail = 0)
w <- 10
fw <- rep(1/w, w)
inf.trend$trend <- 1 + 5*inf.trend$t
inf.trend$smoothcent <- stats::filter(inf.trend$trend, fw, sides = 2)
inf.trend$smoothtrail <- stats::filter(inf.trend$trend, fw, sides = 1)
ggplot(data=inf.trend, aes(x=t)) +
  geom_line(aes(y = inf.trend$trend)) +
  geom_line(aes(y = inf.trend$smoothcent), color = "red", lty = 2) +
  geom_line(aes(y = inf.trend$smoothtrail), color = "blue", lty = 2) +
  JT.theme
@

\begin{marginfigure}
\includegraphics[width=1\textwidth]{PTSFR-influenceaverageontrend}
\caption{Averaging a linear trend. Centered (no influence), Trailing (time shift)}
\label{fig:influenceaverageontrend}
\setfloatalignment{b}
\end{marginfigure}

The average of a polynomial trend follows the polynomial trend, with an extra constant (Figure~\ref{fig:influenceaverageontrendpoly}). Centered averaging will give you approximatively the same line, trailing averaging results in a time shift equal to $w/2$.

If
\begin{equation}
	y = b_{0} + b_{1}t + b_{2}t^{2}
\end{equation}

then
\begin{equation}
	\bar{y} = \frac{1}{w} \int_{t-\frac{w}{2}}^{t + \frac{w}{2}} (b_{0} + b_{1}t + b_{2}t^2) =b_{0} + b_{1}t + b_{2}t^{2} + \frac{b_{2}w^{2}}{12} = y(t) + const
\end{equation}

<<label=influenceaverageontrendpoly, fig=TRUE, include=FALSE, echo=FALSE>>=
inf.trend <- data.frame(t = seq(0, 100, by = 1), trend = 0, smoothcent = 0, smoothtrail = 0)
w <- 10
fw <- rep(1/w, w)
inf.trend$trend <- 1 + 5*inf.trend$t + 0.1*inf.trend$t^2
inf.trend$smoothcent <- stats::filter(inf.trend$trend, fw, sides = 2)
inf.trend$smoothtrail <- stats::filter(inf.trend$trend, fw, sides = 1)
ggplot(data=inf.trend, aes(x=t)) +
  geom_line(aes(y = inf.trend$trend)) +
  geom_line(aes(y = inf.trend$smoothcent), color = "red", lty = 2) +
  geom_line(aes(y = inf.trend$smoothtrail), color = "blue", lty = 2) +
  JT.theme
@

\begin{marginfigure}
\includegraphics[width=1\textwidth]{PTSFR-influenceaverageontrendpoly}
\caption{Averaging a polynomial trend. Centered (no influence), Trailing (time shift)}
\label{fig:influenceaverageontrendpoly}
\setfloatalignment{b}
\end{marginfigure}

\newthought{Seasonal}\index{averaging!effect on seasonal component}

The moving average of a seasonal component (if it is a pure sine), is a sine with the same frequency but a lower amplitude. If the width equals the period of the sine, the amplitude is zero. Centered averaging will give a sine in phase, trailing average will give a phase shift. 

<<label=influenceaverageonseason, fig=TRUE, include=FALSE, echo=FALSE>>=
inf.season <- data.frame(t = seq(0, 100, by = 1), sine = 0, smoothcent = 0, smoothtrail = 0)
T <- 25
w <- 7
fw <- rep(1/w, w)
inf.season$sine <- sin(2*pi*inf.season$t/T)
inf.season$smoothcent <- stats::filter(inf.season$sine, fw, sides = 2)
inf.season$smoothtrail <- stats::filter(inf.season$sine, fw, sides = 1)
ggplot(data=inf.season, aes(x=t)) +
  geom_line(aes(y = inf.season$sine)) +
  geom_line(aes(y = inf.season$smoothcent), color = "red", lty = 2) +
  geom_line(aes(y = inf.season$smoothtrail), color = "blue", lty = 2) +
  JT.theme
@

\begin{marginfigure}
\includegraphics[width=1\textwidth]{PTSFR-influenceaverageonseason}
\caption{Averaging a seasonal component. Centered (no influence), Trailing (time shift)}
\label{fig:influenceaverageonseason}
\setfloatalignment{b}
\end{marginfigure}

\newthought{The error term}\index{averaging!effect on error}

If the error (noise) can be considered as coming from a normal distribution with mean=0 and a variance =$\sigma^{2}$, then averaging of width $w$ will lead to a new error distribution which will be normal, with mean=0 but with a reduced variance=$var=\frac{\sigma^{2}}{w}$.
\newpage
\section{Differencing}

Removing a trend and/or a seasonal pattern can be done by \emph{differencing}\index(differencing)\index{trend!removing of}\index{seasonality!removing of}. A \emph{lag-1} difference\index{lag}\index{differencing!lag} takes the difference between every two consecutive values in the series. In general, a \emph{lag-k} difference substracts the value from k time units back:
\begin{equation}
	lag_{k}(t) = y_{t} - y_{t-k}
\end{equation}

In R the \textit{diff}-function provides the differenced series.

\subsection{Removing a trend}\index{trend:removing}

A \emph{lag-1} difference can remove a trend. While the ridership time series (upper section in Figure~\ref{fig:Amtraklag1}) shows a U-shaped trend, no trend is visible in the lag1-series. For highter order trends, another round of lag-1 differencing might by necessary. In this case it seems that one lag-1 difference is sufficient to remove the trend. Look at the difference in the scale of the y-axis! The differenced time series should hover around the horizontal line when the trend is removed. In the detrended time series the seasonal component should now be detectable.

\newthought{linear trend}\index{differencing!linear trend}

If
\begin{equation}
	y_{t} = b_{0} + b_{1}t
\end{equation}

then
\begin{equation}
	lagk_{t} = y_{t} - y_{t-k}=\left( b_{0} + b_{1}t \right) - \left( b_{0} + b_{1}(t-k) \right) = b_{1}k
\end{equation}

The linear trend is removed and we only have a level equal to $b_{1}k$. For $k=1$ this reduces to $lag1_{t}=b_{1}$.

\newthought{quadratic trend}\index{differencing!quadratic trend}

If
\begin{equation}
	y_{t} = b_{0} + b_{1}t + b_{2}t^{2}
\end{equation}

then
\begin{equation}
	lagk_{t} = y_{t} - y_{t-1}=\left( b_{0} + b_{1}t + b_{2}t^{2}\right) - \left( b_{0} + b_{1}(t-k) + b_{2}(t-k)^{2} \right) = (b_{1} - b_{2}k^{2}) + 2b_{2}kt
\end{equation}

The quadratic trend is reduced to a linear trend. A second lag-1 differencing will reduce this to a constant. For $k=1$ this reduces to $lag1_{t}=(b_{1} - b_{2}) + 2b_{2}t$.

\newthought{polynomial trend}\index{differencing!polynomial trend}

If
\begin{equation}
	y_{t} = p(t)
\end{equation}

A lag-1 differencing will reduce the degree of the polynomial trend by 1.

<<label=Amtraklag1, fig=TRUE, include=FALSE, echo=FALSE>>=
p1 <- autoplot(ridership.ts) + 
  labs(title="ridership.ts") +
  xlab("time") +
  ylab("ridership") +
  JT.theme
p2 <- autoplot(diff(ridership.ts, lag = 1)) + 
  labs(title="ridership.lag1.ts") +
  xlab("time") +
  ylab("ridership.lag1.ts") +
  JT.theme
p3 <- autoplot(diff(diff(ridership.ts, lag = 1),1)) + 
  labs(title="ridership.double.lag1.ts") +
  xlab("time") +
  ylab("ridership.double.lag1.ts") +
  JT.theme
grid.arrange(p1, p2, p3, nrow=3)
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-Amtraklag1}
\caption{Amtrak Ridership original series and lag1-series}
\label{fig:Amtraklag1}
\setfloatalignment{b}
\end{figure}

Let's do this for the addcon- and combicon-series (Figure~\ref{fig:Constructlag1}). In both cases the trend is removed after a lag-1 differencing.

<<label=Constructlag1, fig=TRUE, include=FALSE, echo=FALSE>>=
addcon.ts <- ts(construct$addcon, start = 1)
combicon.ts <- ts(construct$combicon, start = 1)
p1 <- autoplot(addcon.ts) + 
  labs(title="addcon.ts") +
  xlab("time") +
  ylab("addcon") +
  JT.theme
p2 <- autoplot(combicon.ts) + 
  labs(title="combicon.ts") +
  xlab("time") +
  ylab("combicon") +
  JT.theme
p3 <- autoplot(diff(addcon.ts, lag = 1)) + 
  labs(title="addcon.lag1.ts") +
  xlab("time") +
  ylab("addcon.lag1.ts") +
  JT.theme
p4 <- autoplot(diff(combicon.ts, lag = 1)) + 
  labs(title="combicon.lag1.ts") +
  xlab("time") +
  ylab("combicon.lag1.ts") +
  JT.theme
grid.arrange(p1, p2, p3, p4,  nrow=2)
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-Constructlag1}
\caption{addcon and combicon series: original and lag1}
\label{fig:Constructlag1}
\setfloatalignment{b}
\end{figure}

\subsection{Removing Seasonality}\index{seasonality!removing}\index{deseasonalizing}\index{seasonal adjustment}

\newthought{Seasonality as a sine-function}\index{differencing!seasonality!sine function}

If
\begin{equation}
	y_{t} = \hat{a}sin(\frac{2\pi t}{T} + \alpha)
\end{equation}

then

\begin{equation}
	lag_k(t)=y_{t}-y_{t-k}=\hat{a}sin(\frac{2\pi t}{T} + \alpha) - \hat{a}sin(\frac{2\pi (t-k)}{T} + \alpha) = \hat{a}sin(\frac{2\pi t}{T} + \alpha) - \hat{a}sin(\frac{2\pi t}{T} + \alpha - \frac{2\pi k}{T})
	\label{eq:seasonlag}
\end{equation}

The difference between two sine-functions with the same period T (and thus the same frequency f) is again a sine function with the same period T. \emph{Differencing} seasonal elements within the signal will therefore not change the frequency of the intial seasonal component. It will change the amplitude because the phase-angle of the second term in equation \ref{eq:seasonlag} has changed by $\frac{2\pi k}{T}$. In the special case where $k=T$ the change in phase-angle will be $2\pi$ and the difference between the two sine-functions will be zero. This means that choosing the lag equal to the period of a seasonal component will make this component disappear: a \emph{lag-T}-differencing will completely remove (a true sine) seasonality of the same period.

Do \emph{lag-T}-differencing, where $T$ is the period (or suspected period) of the season. For the Amtrak time series $T=12$. 

<<label=Amtraklag12, fig=TRUE, include=FALSE, echo=FALSE>>=
p1 <- autoplot(ridership.ts) + 
  labs(title="ridership.ts") +
  xlab("time") +
  ylab("ridership") +
  JT.theme
p2 <- autoplot(diff(ridership.ts, lag = 12)) + 
  labs(title="ridership.lag12.ts") +
  xlab("time") +
  ylab("ridership.lag12.ts") +
  JT.theme
grid.arrange(p1, p2, nrow=2)
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-Amtraklag12}
\caption{Amtrak Ridership original series and lag12-series}
\label{fig:Amtraklag12}
\setfloatalignment{b}
\end{figure}

Figure~\ref{fig:Amtraklag12}: the monthly pattern has gone, but there is a hint of a 6 to 8 month pattern.

For the addcon time series we have guessed before that it is around $T=1000/9=111$.

<<label=Constructlag111, fig=TRUE, include=FALSE, echo=FALSE>>=
p1 <- autoplot(addcon.ts) + 
  labs(title="addcon.ts") +
  xlab("time") +
  ylab("addcon") +
  JT.theme
p2 <- autoplot(combicon.ts) + 
  labs(title="combicon.ts") +
  xlab("time") +
  ylab("combicon") +
  JT.theme
p3 <- autoplot(diff(addcon.ts, lag = 111)) + 
  labs(title="addcon.lag111.ts") +
  xlab("time") +
  ylab("addcon.lag111.ts") +
  JT.theme
p4 <- autoplot(diff(combicon.ts, lag = 111)) + 
  labs(title="combicon.lag111.ts") +
  xlab("time") +
  ylab("combicon.lag111.ts") +
  JT.theme
grid.arrange(p1, p2, p3, p4,  nrow=2)
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-Constructlag111}
\caption{addcon and combicon series: original and lag111}
\label{fig:Constructlag111}
\setfloatalignment{b}
\end{figure}

Figure~\ref{fig:Constructlag111}: the most obvious pattern has gone, but some trend has remained and there is a sign of a shorter periodicity which we can determine by doing a \textit{ggplotly} on graphs p3 and p4. Some measurements on these plots suggest a shorter period of about 27. Doing a second differencing with the shorter period give Figure~\ref{fig:Constructdoublelag}.

<<label=Constructdoublelag, fig=TRUE, include=FALSE, echo=FALSE>>=
p1 <- autoplot(addcon.ts) + 
  labs(title="addcon.ts") +
  xlab("time") +
  ylab("addcon") +
  JT.theme
p2 <- autoplot(combicon.ts) + 
  labs(title="combicon.ts") +
  xlab("time") +
  ylab("combicon") +
  JT.theme
p3 <- autoplot(diff(diff(addcon.ts, lag = 111),27)) + 
  labs(title="addcon.lag111/27.ts") +
  xlab("time") +
  ylab("addcon.lag111/27.ts") +
  JT.theme
p4 <- autoplot(diff(diff(combicon.ts, lag = 111),27)) + 
  labs(title="combicon.lag111/27.ts") +
  xlab("time") +
  ylab("combicon.lag111/27.ts") +
  JT.theme
grid.arrange(p1, p2, p3, p4,  nrow=2)
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-Constructdoublelag}
\caption{addcon and combicon series: double lag: first T=111, then T=27}
\label{fig:Constructdoublelag}
\setfloatalignment{b}
\end{figure}

\subsection{Removing trend and seasonality}
Do a double lag: first for seasonality (lag-T) and on the lagged time series a (lag-1).

<<label=Amtrakdouble, fig=TRUE, include=FALSE, echo=FALSE>>=
p1 <- autoplot(ridership.ts) + 
  labs(title="ridership.ts") +
  xlab("time") +
  ylab("ridership") +
  JT.theme
p2 <- autoplot(diff(diff(ridership.ts, lag = 12),1)) + 
  labs(title="ridership.lag12/1.ts") +
  xlab("time") +
  ylab("ridership.lag12/1.ts") +
  JT.theme
grid.arrange(p1, p2, nrow=2)
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-Amtrakdouble}
\caption{Amtrak Ridership original series and lag12-lag1 series}
\label{fig:Amtrakdouble}
\setfloatalignment{b}
\end{figure}

<<label=Constructdouble, fig=TRUE, include=FALSE, echo=FALSE>>=
p1 <- autoplot(addcon.ts) + 
  labs(title="addcon.ts") +
  xlab("time") +
  ylab("addcon") +
  JT.theme
p2 <- autoplot(combicon.ts) + 
  labs(title="combicon.ts") +
  xlab("time") +
  ylab("combicon") +
  JT.theme
p3 <- autoplot(diff(diff(addcon.ts, lag = 111),1)) + 
  labs(title="addcon.lag111/1.ts") +
  xlab("time") +
  ylab("addcon.lag111/1.ts") +
  JT.theme
p4 <- autoplot(diff(diff(combicon.ts, lag = 111),1)) + 
  labs(title="combicon.lag111/1.ts") +
  xlab("time") +
  ylab("combicon.lag111/1.ts") +
  JT.theme
grid.arrange(p1, p2, p3, p4,  nrow=2)
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-Constructdouble}
\caption{addcon and combicon series: double lag: first T=111, then T=1}
\label{fig:Constructdouble}
\setfloatalignment{b}
\end{figure}

\subsection{Effect of differencing on the error term}\index{differencing!effect on error}

If the error (noise) can be considered as coming from a normal distribution with mean=0 and a variance =$\sigma^{2}$, then a differencing operation (of any lag) will lead to a new error distribution which will be normal, with mean=0 but with variance=$var=\sigma^{2} + \sigma^{2}=2\sigma^{2}$. Further lag-differencing operations will steadily increase the variance of the error term.

\section{Simple exponential smoothing}\index{simple exponential smoothing}\index{SEM}\index{smoothing!simple exponential}

It is a trailing moving average method where we take into account \emph{all} previous values, but they are not weighted uniformly: the weighing factor decreases exponentially. This give more weight to recent information. The sum of the weighing factors must of course be equal to 1. The choice for the weighing factor is $\alpha (1 - \alpha)^{i}$, because
\begin{equation}
	\sum_{i=0}^{i=+\infty} \alpha (1 -\alpha)^{i} = 1 \quad for \quad 0 \leq \alpha \leq 1
\label{eq:smoothingconstant}
\end{equation}

This generates a forecast at $t=t+1$:
\begin{equation}
	F_{t+1}= \alpha y_{t} + \alpha (1-\alpha) y_{t-1} + \ldots + \alpha (1-\alpha)^{i}y_{t-i} + \ldots
\label{eq:forecastexpsmooth}
\end{equation}

Because
\begin{equation}
	F_{t}= \alpha y_{t-1} + \alpha (1-\alpha) y_{t-2} + \ldots + \alpha (1-\alpha)^{i}y_{t-i-1} + \ldots
\end{equation}

it follows that:
\begin{equation}
	F_{t+1}  = \alpha y_{t} + (1-\alpha) F_{t} = F_{t} + \alpha (y_{t} - F_{t}) = F_{t} + \alpha e_{t}
\end{equation}

This formulation shows that \emph{simple exponential smoothing} is a \emph{learning} process: it takes the former forecast and adjusts it based on the difference with the actual value. If the former forecast was too high ($e_{t} <0$) then it will adjust the next forecast downward. If the former was too low ($e_{t}>0$) it will adjust upwards. The ''learning" is exemplified by the \emph{smoothing constant}\index{smoothing constant}\index{smoothing!constant} $\alpha$.

From the formula we can see that \emph{simple exponential smoothing} will only generate one prediction for the future. The second prediction cannot be calculated as we do not know the acutal value of $y_{t+1}$. Usually this means that for forecasts from $F_{t+2}$ and higher we keep the forecast constant and equal to $F_{t+1}$. This is a small improvement on a naive forecast, but not much. As such, \emph{simple exponential smoothing} is not a usefull forecasting method for time series that have trend and/or seasonality.

\subsection{Choosing the smoothing constant $\alpha$}\index{smoothing constant}\index{simple exponential smoothing!smoothing constant}

The value of the smoothing constant has to be between 0 and 1 (included) (see equation~\ref{eq:smoothingconstant}). $\alpha=1$ results in $F_{t+1}=y_{t}$, which is the naive forecast. $\alpha$ values close to 0 will take into account a lot of the former data values. A rule of thumb for $\alpha$ is the range $[0.1 - 0.2]$, but we can optimize $\alpha$ by looking at the predictive quality metrics (MAPE, RMSE ...) of the validation period for different choices of $\alpha$.

\newthought{Example: Amtrak data, twice differenced}

The example given in the book is the time series that we get from the Amtrak time series after double differencing: a lag12 differencing to remove the seasonality, and on the result a lag1 differencing to remove the trend. The result of these operations should be the error, but not the original noise on the signal but an ''enhanced" version because the differencing operations each time double the variance. The goal is, according to the book, to work with a time series that has no trend and no seasonality. I think that means a random signal with average 0. Forecasting such a signal seems to be easy: it is zero for the whole validation period and for the future. I see no reason to make this very complicated. Working with the assumption that this is a ''deformed" error signal with 4 times (due to two differencing operations) the variance of the original signal, we could even say more: the forecast for the error signal is a random value taken from a normal distribution with mean=0 and standard deviation equal to $\frac{\sqrt{var signal}}{2}$. From this we could calculate a 95\% confidence interval.

However, the book uses the \textit{ets}-function from the \textbf{forecast}-package. The letters ''ets" stand for: \emph{error}, \emph{trend} and \emph{seasonality}. The simple exponential smoothing model is called using the letter combination ''ANN" where ''A" stands for \emph{additive error}, ''N" for \emph{no trend} en the second ''N" for \emph{no seasonality}. The results of the model with $\alpha=0.2$ are given in Figure~\ref{fig:Amtraksimpleexpo} (the blue line). You can let the \textit{ets}-function determine the optimum value of $\alpha$ by calling the \textit{ets}-function without specifying the value of $\alpha$ (the red line).

<<label=Amtraksimpleexpo, fig=TRUE, include=FALSE, echo=FALSE>>=
diff.twice.ts <- diff(diff(ridership.ts, lag = 12), lag = 1)
train.ts <- window(diff.twice.ts, start = c(1992, 2), end = c(1992, nTrain + 1))
valid.ts <- window(diff.twice.ts, start = c(1992, nTrain + 2), end = c(1992, nTrain + 1 + nValid))
ses <- ets(train.ts, model = "ANN", alpha = 0.2)
ses.opt <- ets(train.ts, model = "ANN")
ses.pred <- forecast(ses, h = nValid, level = 0) # level=0 means no confidence interval calculation
ses.opt.pred <- forecast(ses.opt, h = nValid, level = 0) # level=0 means no confidence interval calculation
plot(ses.pred,
     ylim = c(-250, 300),
     ylab = "Ridership (Twice-Differenced)",
     xlab = "Time",
     bty = "l",
     xaxt = "n",
     xlim = c(1991, 2006.25),
     main = "",
     lty = 2)
axis(1 , at = seq(1991, 2006, 1), labels = format(seq(1991, 2006, 1)))
lines(ses.pred$fitted, lwd = 2, col = "blue")
lines(ses.opt.pred$fitted, lwd = 2, col = "red")
lines(ses.opt.pred$mean, lwd = 2, col = "red")
lines(valid.ts)
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-Amtraksimpleexpo}
\caption{Simple exponential smoothing on Amtrak data twice differenced lag12 and lag1}
\label{fig:Amtraksimpleexpo}
\setfloatalignment{b}
\end{figure}

The model forecasts a constant value equal to \Sexpr{round(tail(ses.pred$fitted, n = 1),3)} when we set $\alpha=0.2$. For the optimum model the constant is \Sexpr{round(tail(ses.opt.pred$fitted, n = 1),5)}. When we look at the details of the optimal model, we find that the optimum value of $\alpha=$ \Sexpr{round(ses.opt$par[1],4)}. Because every term in equation~\ref{eq:forecastexpsmooth} is multiplied with $\alpha$ this comes down to a value of about zero.

\newthought{Example: addcon data, twice differenced T=111 and T=1}

<<label=addconsimpleexpo, fig=TRUE, include=FALSE, echo=FALSE>>=
diff.twice.ts <- diff(diff(addcon.ts, lag = 111), lag = 1)
nValid <- 100
nTrain <- length(addcon.ts) - nValid
train.ts <- window(diff.twice.ts, start = 1, end = nTrain)
valid.ts <- window(diff.twice.ts, start = nTrain + 1, end = nTrain + nValid)
ses <- ets(train.ts, model = "ANN", alpha = 0.2)
ses.opt <- ets(train.ts, model = "ANN")
ses.pred <- forecast(ses, h = nValid, level = 0) # level=0 means no confidence interval calculation
ses.opt.pred <- forecast(ses.opt, h = nValid, level = 0) # level=0 means no confidence interval calculation
plot(ses.pred,
     ylim = c(-25, 25),
     ylab = "addcon (Twice-Differenced)",
     xlab = "t",
     bty = "l",
     xaxt = "n",
     xlim = c(0, 1000),
     main = "",
     lty = 2)
axis(1 , at = seq(0, 1000, 100))
lines(ses.pred$fitted, lwd = 2, col = "blue")
lines(ses.opt.pred$fitted, lwd = 2, col = "red")
lines(ses.opt.pred$mean, lwd = 2, col = "red")
lines(valid.ts)
@

\begin{figure}
\includegraphics[width=0.85\textwidth]{PTSFR-addconsimpleexpo}
\caption{Simple exponential smoothing on addcon data twice differenced lag111 and lag1}
\label{fig:addconsimpleexpo}
\setfloatalignment{b}
\end{figure}

The model forecasts a constant value equal to \Sexpr{round(tail(ses.pred$fitted, n = 1),3)} when we set $\alpha=0.2$. For the optimum model the constant is \Sexpr{round(tail(ses.opt.pred$fitted, n = 1),5)}. When we look at the details of the optimal model, we find that the optimum value of $\alpha=$ \Sexpr{round(ses.opt$par[1],4)}. Because every term in equation~\ref{eq:forecastexpsmooth} is multiplied with $\alpha$ this comes down to a value of about zero.

What I expected was that this represents the error term in the construction of the addcon time-series. The error was created by choosing random from a N(0,$\sigma=3$) distribution. However, as noted, the differencing will increase the variance. Twice differencing will create a variance that is 4 times the initial variance. Our time-series in the training set has the following characteristics:

<<>>=
mean(train.ts)
var(train.ts)
sqrt(var(train.ts))/2 - sigma
# looks good
@

Is addcon twice differenced normally distributed?

<<label=histaddcon2diff, fig=TRUE, include=FALSE, echo=FALSE>>=
ggplot(data = as.data.frame(diff.twice.ts)) + 
  geom_histogram(aes(x), bins = 50) +
  labs(title="histogram of addcon twice differenced") +
  xlab("time") +
  ylab("addcon lag111 and lag1") +
  JT.theme
@

\begin{figure}
\includegraphics[width=0.6\textwidth]{PTSFR-histaddcon2diff}
\caption{Histogram of addcon data twice differenced lag111 and lag1}
\label{fig:histaddcon2diff}
\setfloatalignment{b}
\end{figure}

\newthought{!!! Important point !!!}

My irritation with the seemingly pointless exercise of modelling a time series without a trend or seasonality is the consequence of ''automatice engineering thinking". The automatic assumption is that there is an underlying trend which can be found as a polynomial (or other function) model. Seasonality can be reduced to finding the Fourier components of the periodic element within the time series. The rest is noise: an error component presumed to be modelled by a normal distribution N(0,$\sigma$). This is clearly a \emph{model based} forecasting method. The forecasting element resides in extrapolating the trend (=assuming that the function holds outside the data range), assuming that the seasonality holds, and that we can estimate the error.

Methods like \emph{simple exponential smoothing} and more sofisticated versions (see \ref{sec:advanced exponential smoothing} \emph{advanced exponential smooting}) are \emph{data driven} methods. For example: in the modelling of a time series without trend or seasonality, it does not assume that the error is N(0,$\sigma$)-distributed. It prefers agnosticism and bases itself on the data. The \emph{simple exponential smoothing} ''learns" from all previous data points. The \emph{simple exponential smoothing} will only give you one forecast $F_{t+1}$ based on all available data points $y_{i} \quad for \quad i=1 \ldots t$.  

\subsection{Relation between $w$ (moving average smoothing) and $\alpha$ (simple exponential smoothing)}
\begin{equation}
	w = \frac{2}{\alpha}-1
\end{equation}

\newpage
\section{Advanced Exponential Smoothing}\index{advanced exponential smoothing}\index{smoothing!advanced exponential}
\label{sec:advanced exponential smoothing}

We can eliminate trend and seasonality by differencing. But these actions also have an impact on each other and on the error component. An alternative is the use of special functions that allow for trend and/or seasonality.

\subsection{Time series with a trend: additive or multiplicative}

\newthought{Additive trend}

The main idea is that we ''rebuild" the time series learning from previous data points. This is another way of looking at \emph{trend}\index{trend}. From a modelling perspective we can propose that the trend is given by (e.g.) a linear equation:

\begin{equation}
	y_{t} = b_{0} + b_{1}t
\end{equation}

But we could also define it by specifying \emph{locally} how the next data point can be found from the last:

\begin{eqnarray}
  y_{0} &=& b_{0}\\
  y_{t+1} &=& y_{t} + b_{1}
\end{eqnarray}

This is a \emph{local} generating formula, because it is based on the last local position and it adds to that a, local, change. The \emph{trend}\index{trend} here is interpreted as an added (positive or negative) \emph{local} term. In this case this trend is a constant ($b_{1}$), but this is not necessary. In general, when there is a trend, we can write that the next element can be found from two terms: the (local) \emph{level} $L_{t}$ and the (local) \emph{trend} $T_{t}$. For an \emph{additive trend}\index{trend!additive} the forecast one time step ahead will be:

\begin{equation}
	F_{t+1} = L_{t} + T_{t}
\end{equation}

It is also the beginning of a forecast further into the future. Assuming the trend holds we can write:

\begin{equation}
	F_{t+k} = L_{t} + k.T_{t}
	\label{eq:AESadded}
\end{equation}

The values for the local level $L_{t}$ and the local trend $T_{t}$ are calculated by using a form of \emph{simple exponential smoothing}:

\begin{eqnarray}
  L_{t} &=& \alpha y_{t} + (1-\alpha)(L_{t-1} + T_{t-1}) = \alpha y_{t} + (1-\alpha)F_{t}  \\
  T_{t} &=& \beta (L_{t} - L_{t-1}) + (1-\beta)T_{t-1}
\end{eqnarray}

The smoothing constants $\alpha$ and $\beta$ are constants between 0 and 1. Once we have calculated the values of $L_{t}$ and $T_{t}$ the forecast with an additive trend, given by \ref{eq:AESadded} is \emph{linear}. It is clear that the level $L_{t}$ and the trend $T_{t}$ have the same units as the time-series value $y_{t}$.

There is of course also an error term present. It can be additive
\begin{equation}
	y_{t+1} = L_{t} + T_{t} + e_{t+1} = F_{t+1}  + e_{t+1}
\end{equation}

or multiplicative
\begin{equation}
	y_{t+1} = (L_{t} + T_{t}).(1 + e_{t+1}) = F_{t+1}.(1 + e_{t+1})
\end{equation}

\newthought{Multiplicative trend}

The disadvantage of a model with \emph{added trend} is that the forecast will be linear in time. When we have the impression (from visualizing the data, or from other information) that the relation between the time-series value $y_{t}$ and time $t$ is non-linear we can incorporate this into the forecasting model by making the term we add to the level \emph{dependent} upon the present value of the time-series. If the difference between the present value and the next value depends upon the present value we can write this locally as:
\begin{equation}
	y_{t+1} - y_{t} = y_{t}.p
\end{equation}

From this it follows that
\begin{equation}
	y_{t+1} = y_{t} + y_{t}.p = y_{t}(1 + p)
\end{equation}

and, assuming that $p$ remains the same,
\begin{equation}
	y_{t+2} = y_{t+1}(1 + p) = y_{t}(1+p)^{2}
\end{equation}

and in general
\begin{equation}
	y_{t+k} = y_{t}(1+p)^{k} = y_{t}(1 + kp + \frac{1}{2}k(k-1)p^{2} + \ldots + \frac{1}{i!}k(k-1)..(k-i+1)p^{i} + \ldots)
\end{equation}

which makes $y_{t+k}$ non-linear in the time step $k$ (Figure~\ref{fig:nonlinearp}).

<<label=nonlinearp, fig=TRUE, include=FALSE, echo=FALSE>>=
nonlin <- data.frame(k = seq(0, 5, by=0.1), pos = 0, neg = 0)
ppos <- 0.8
pneg <- -0.8
nonlin$pos <- 5*(1+ppos)^(nonlin$k)
nonlin$neg <- 100*(1+pneg)^(nonlin$k)
p1 <- ggplot(data = nonlin, aes(x = k)) +
  geom_line(aes(y = pos), color= "red") +
  ggtitle("positive percentage growth") +
  ylab("p = +0.8") +
  ylim (low = 0, high = 100) +
  JT.theme
p2 <- ggplot(data = nonlin, aes(x = k)) +
  geom_line(aes(y = neg), color = "red") +
  ggtitle("negative percentage growth") +
  ylab("p = -0.8") +
  JT.theme
grid.arrange(p1, p2, nrow = 1)
@

\begin{marginfigure}
\includegraphics[width=0.85\textwidth]{PTSFR-nonlinearp}
\caption{Non-linear aspect of multiplicative model}
\label{fig:nonlinearp}
\setfloatalignment{b}
\end{marginfigure}

Our forecast for $t+k$ will now be:
\begin{equation}
	F_{t+k} = L_{t}.T_{t}^{k} \quad with \quad T_{t}=1+p
\end{equation}

From this equation it follows that the level $L_{t}$ has the same units as the time-series value $y_{t}$, but that the trend $T_{t}$ is \emph{dimensionless} and depends on the fraction $\frac{L_{t}}{L_{t-1}}$.

The values for the local level $L_{t}$ and the local trend $T_{t}$ are again calculated using a form of \emph{simple exponential smoothing}. For the trend we must use the fraction $\frac{L_{t}}{L_{t-1}}$ :
\begin{eqnarray}
  L_{t} &=& \alpha y_{t} + (1-\alpha)(L_{t-1} . T_{t-1}) = \alpha y_{t} + (1-\alpha)F_{t}  \\
  T_{t} &=& \beta (\frac{L_{t}}{L_{t-1}}) + (1-\beta)T_{t-1}
\end{eqnarray}

The error term can be additive
\begin{equation}
	y_{t+1} = L_{t} . T_{t} + e_{t+1} = F_{t+1} + e_{t+1}
\end{equation}

or multiplicative
\begin{equation}
	y_{t+1} = (L_{t} . T_{t}).(1 + e_{t+1}) = F_{t+1}.(1 + e_{t+1})
\end{equation}

\subsection{Time series with a trend and seasonality: additive, multiplicative or combinations}

\newthought{Additive model}

An extra term $S$ is added to capture the seasonality. Assuming all components are additive, the forecast $k$ time steps ahead is now:
\begin{equation}
	F_{t+k} = L_{t} + k.T{t} + S_{t+k-M} \quad with \quad M=period
\end{equation}

We add \emph{level}, \emph{trend} and the value of the seasonal component one period (M) ago. A measure for the seasonal component is the difference between the actual value $y_{t}$ and the level $L_{t}$. For the calculation of the new level, we have to substract the seasonal value from the measured value $y_{t}$. This gives the followign exponential smoothing equations:
\begin{eqnarray}
  L_{t} &=& \alpha (y_{t} - S_{t-M}) + (1-\alpha)(L_{t-1} + T_{t-1})  \\
  T_{t} &=& \beta (L_{t} - L_{t-1}) + (1-\beta)T_{t-1} \\
  S_{t} &=& \gamma (y_{t} - L_{t}) + (1 - \gamma)S_{t-M}
\end{eqnarray}

\printindex

\newpage

\textbf{Thanks} \\
\medskip
R Core Team (2018). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.
\medskip
<<>>=
sessionInfo()
@

\end{document}